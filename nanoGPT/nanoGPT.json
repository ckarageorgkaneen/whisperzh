{"text": " Hi everyone. So by now you have probably heard of ChatGPT. It has taken the world and the AI community by storm, and it is a system that allows you to interact with an AI and give it text-based tasks. So for example, we can ask ChatGPT to write us a small haiku about how important it is that people understand AI, and then they can use it to improve the world and make it more prosperous. So when we run this, AI knowledge brings prosperity for all to see, embrace its power. Okay, not bad. And so you could see that ChatGPT went from left to right and generated all these words sort of sequentially. Now, I asked it already the exact same prompt a little bit earlier, and it generated a slightly different outcome. AI's power to grow, ignorance holds us back, learn, prosperity waits. So pretty good in both cases and slightly different. So you can see that ChatGPT is a probabilistic system, and for any one prompt, it can give us multiple answers, sort of replying. Now, this is just one example of a prompt. People have come up with many, many examples, and there are entire websites that index interactions with ChatGPT. And so many of them are quite humorous. Explain HTML to me like I'm a dog, write release notes for chess too, write a note about Elon Musk buying a Twitter, and so on. So as an example, please write a breaking news article about a leaf falling from a tree, and a shocking turn of events. The leaf falling from a tree in the local park. Witnesses report that the leaf, which was previously attached to a branch of a tree, detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty remarkable system, and it is what we call a language model, because it models the sequence of words or characters or tokens more generally, and it knows how certain words follow each other in English language. And so from its perspective, what it is doing is it is completing the sequence. So I give it the start of a sequence, and it completes the sequence with the outcome. And so it's a language model in that sense. Now, I would like to focus on the under the hood of under the hood components of what makes ChatGPT work. So what is the neural network under the hood that models the sequence of these words? And that comes from this paper called Attention is All You Need. In 2017, a landmark paper, a landmark paper in AI that produced and proposed the Transformer and the Translator. And this paper is called the Transformer and the Translator. And this paper is called the Transformer and the Translator. And this paper is called the Transformer Architecture. So GPT is short for generatively, generatively pre trained transformer. So transformer is the neural net that actually does all the heavy lifting under the hood. It comes from this paper in 2017. Now, if you read this paper, this reads like a pretty random machine translation paper. And that's because I think the authors didn't fully anticipate the impact that the transformer would have on the field. And this architecture that they produced in the context of machine translation, in their case, actually ended up taking over the rest of AI in the next five years after. And so this architecture with minor changes was copy pasted into a huge amount of applications in AI in more recent years. And that includes at the core of ChatGPT. Now, we are not going to, what I'd like to do now is I'd like to build out something like ChatGPT. But we're not going to be able to, of course, reproduce ChatGPT. This is a very serious production grade system. It is trained on a good chunk of internet. And then there's a lot of pre training and fine tuning stages to it. And so it's very complicated. What I'd like to focus on is just to train a transformer based language model. And in our case, it's going to be a character level language model. I still think that is a very educational with respect to how these systems work. So I don't want to train on the chunk of internet, we need a smaller data set. In this case, I propose that we work with my favorite toy data set. It's called ChatGPT. And I'm going to show you how that works. I'm going to show you what it looks like. So first, I'm going to create a little tiny Shakespeare. And what it is is basically it's a concatenation of all of the works of Shakespeare in my understanding. And so this is all of Shakespeare in a single file. This file is about one megabyte. And it's just all of Shakespeare. And what we are going to do now is we're going to basically model how these characters follow each other. So for example, given a chunk of these characters like this, given some context of characters in the past, the transformer neural network will look at the model of the character. characters that i've highlighted and it's going to predict that g is likely to come next in the sequence and it's going to do that because we're going to train that transformer on shakespeare and it's just going to try to produce character sequences that look like this and in that process is going to model all the patterns inside this data so once we've trained the system i just like to give you a preview we can generate infinite shakespeare and of course it's a fake thing that looks kind of like shakespeare um apologies for there's some jank that i'm not able to resolve in in here but um you can see how this is going character by character and it's kind of like predicting shakespeare-like language so verily my lord the sights have left the again the king coming with my curses with precious pale and then tronio says something else etc and this is just coming out of the transformer in a very similar manner as it would come out in chat gpt in our case character by character in chat gpt it's coming out on the token by token level and tokens are these sort of like little subword pieces so they're not word level they're kind of like word chunk level um and now i've already written this entire code uh to train these transformers um and it is in a github repository that you can find and it's called nano gpt so nano gpt is a repository that you can find on my github and it's a repository for training transformers um on any given text and what i think is interesting about it because there's many ways to train transformers but this is a very simple implementation so it's just two files of 300 lines of code each one file defines the gpt model the transformer and one file trains it on some given text dataset and here i'm showing that if you train it on a open web text dataset web pages then i reproduce the the performance of gpt2 so gpt2 is an early version of openai's gpt from 2017 if i recall correctly and i've only so far reproduced the the smallest 124 million parameter model but basically this is just proving that the code base is correctly arranged and i'm able to load the neural network weights that openai has released later so you can take a look at the finished code here in nano gpt what i would like to do in this lecture is i would like to basically write this repository from scratch so we're going to begin with an empty file and we're going to define a transformer piece by piece we're going to train it on the tiny shakespeare dataset and we'll see how we can then generate infinite shakespeare and of course this can copy paste to any arbitrary text dataset that you like but my goal really here is to just make you understand and appreciate how under the hood chat gpt works and really all that's required is a proficiency in python and some basic understanding of calculus and statistics and it would help if you also see my previous videos on the same youtube channel in particular my make more series where i define smaller and simpler neural network language models so multi-layered perceptrons and so on it really introduces the language modeling framework and then here in this video we're going to focus on the transformer so let's look at the general structure of the neural network itself okay so i created a new google collab jupiter notebook here and this will allow me to later easily share this code that we're going to develop together with you so you can follow along so this will be in a video description later now here i've just done some preliminaries i downloaded the dataset the tiny shakespeare dataset at this url and you can see that it's about a one megabyte file then here i open the input.txt file and just read in all the text of the string and you can see that we are working with one million characters roughly and the first 1000 characters if we just print them out are basically what you would expect this is the first 1000 characters of the tiny shakespeare dataset roughly up to here so so far so good next we're going to take this text and the text is a sequence of characters in python so when i call the set constructor on it i'm just going to get the set of all the characters that occur in this text and then i'm just going to set the set of all the characters that occur in this text and then i'm going to I'm going to sort that to create a list of those characters instead of just a set so that i have an ordering an arbitrary ordering and then i sort that so basically we get just all the characters that occur in the entire data set and they're sorted now the number of them is going to be our vocabulary size these are the possible elements of our sequences and we see that when i print here the characters there's 65 of them in total there's a space character and then all kinds of special characters lowercase letters so that's our vocabulary and that's the sort of like possible characters that the model can see or emit okay so next we would like to develop some strategy to tokenize the input text now when people say tokenize they mean convert the raw text as a string to some sequence of integers according to some notebook according to some vocabulary of possible elements so as an example here we are going to be building a character level language model so we're simply going to be translating individual characters into integers so let me show you a chunk of code that sort of does that for us so we're building both the encoder and the decoder and let me just talk through what's happening here when we encode an arbitrary text like hi there we're going to receive a list of integers that represents that string so for example 46 47 etc and then we also have the reverse mapping so we can take this list and decode it into a string so we can take this list and decode it into a string so we can take this list and decode it to get back the exact same string so it's really just like a translation to integers and back for arbitrary string and for us it is done on a character level now the way this was achieved is we just iterate over all the characters here and create a lookup table from the character to the integer and vice versa and then to encode some string we simply translate all the characters individually and to decode it back we use the reverse mapping concatenate all of it now this is only one of many possible encodings or many possible tokenizers and it's a very simple one but there's many other schemas that people have come up with in practice so for example Google uses SENTENCEPIECE so SENTENCEPIECE will also encode text into integers but in a different schema and using a different vocabulary and SENTENCEPIECE is a sub-word sort of tokenizer and what that means is that you're not encoding entire words but you're not also encoding individual characters it's a subword unit level and that's usually what's adopted in practice. For example also OpenAI has this library called tiktoken that uses a byte pair encoding tokenizer and that's what GPT uses and you can also just encode words into like hello world into lists of integers. So as an example I'm using the tiktoken library here I'm getting the encoding for GPT-2 or that was used for GPT-2. Instead of just having 65 possible characters or tokens they have 50 000 tokens and so when they encode the exact same string high there we only get a list of three integers but those integers are not between 0 and 64 they are between 0 and 50 256. So basically you can trade off the codebook size and the sequence lengths so you can have a very long string and you can have a very long string and you can have a very long string and you can have a very long string and you can have a very long string and you can have a very long string and you can have a very long string and you can have a very long string and you can have a very long sequences of integers with very small vocabularies or you can have short sequences of integers with very large vocabularies and so typically people use in practice these subword encodings but I'd like to keep our tokenizer very simple so we're using character level tokenizer and that means that we have very small codebooks we have very simple encode and decode functions but we do get very long sequences as a result but that's the level at which we're going to stick with this lecture because it's the simplest thing okay so now that we have an encoder and a decoder effectively a tokenizer we can tokenize the entire training set of Shakespeare so here's a chunk of code that does that and I'm going to start to use the pytorch library and specifically the torch.tensor from the pytorch library so we're going to take all of the text in tiny Shakespeare encode it and then wrap it into a torch.tensor to get the data tensor so here's what the data tensor looks like when I look at just the first one thousand character or the one thousand elements of it so we see that we have a massive sequence of integers and this sequence of integers here is basically an identical translation of the first 1000 characters here so I believe for example that zero is a new line character and maybe one is a space I'm not 100 sure but from now on the entire data set of text is re-represented as just it's just stretched out as a single very large sequence of integers let me do one more thing before we move on here we're going to separate out our data set into a train and a validation split so in particular we're going to take the first 90 of the data set and consider that to be the training data for the transformer and we're going to withhold the last 10 at the end of it to be the validation data and this will help us understand to what extent our model is overfitting so we're going to basically hide and keep the validation data on the side because we don't want just a perfect memorization of this exact Shakespeare we want a neural network that sort of creates Shakespeare's like text and so it should be fairly likely for it to produce the actual like stowed away true Shakespeare text and so we're going to use this to get a sense of the overfitting okay so now we would like to start plugging these text sequences or integer sequences into the transformer so that it can train and learn those patterns now the important thing to realize is we're never going to actually feed entire text into transformer all at once that would be very expensive and prohibitive so when we actually train a transformer on a lot of these data sets we only work with chunks of the data set and when we train the transformer we basically sample random little chunks out of the training set and train them just chunks at a time and these chunks have basically some kind of a length and some maximum length now the maximum length typically at least in the code i usually write is called block size you can you can find it under different names like context length or something like that let's start with the block size of just eight and let me look at the first train data characters the first block size plus one characters i'll explain why plus one in a second so this is the first nine characters in the sequence in the training set now what i'd like to point out is that when you sample a chunk of data like this so say these nine characters out of the training set this actually has multiple examples packed into it and that's because all of these characters follow each other and so what this thing is going to say when we plug it into a transformer is we're going to actually simultaneously train it to make a prediction at every one of these positions now in the in a chunk of nine characters there's actually eight individual examples packed in there so there's the example that when 18 when in the context of 18 47 likely comes next in a context of 18 and 47 56 comes next in the context of 1847 47, 56, 57 can come next, and so on. So that's the eight individual examples. Let me actually spell it out with code. So here's a chunk of code to illustrate. X are the inputs to the transformer. It will just be the first block size characters. Y will be the next block size characters. So it's offset by one. And that's because Y are the targets for each position in the input. And then here I'm iterating over all the block size of eight. And the context is always all the characters in X up to T and including T. And the target is always the T character, but in the targets array Y. So let me just run this. And basically it spells out what I said in words. These are the eight examples hidden in a chunk of nine characters that we sampled from the training set. I want to make sure that I'm not missing anything. Let me just mention one more thing. We train on all the eight examples here with context between one all the way up to context of block size. And we train on that not just for computational reasons because we happen to have the sequence already or something like that. It's not just done for efficiency. It's also done to make the transformer network be used to seeing contexts all the way from as little as one all the way to block size. And we'd like the transformer to be used to seeing everything in between. And that's going to be useful. Later during inference, because while we're sampling, we can start to set a sampling generation with as little as one character of context. And the transformer knows how to predict the next character with all the way up to just context of one. And so then it can predict everything up to block size. And after block size, we have to start truncating because the transformer will never receive more than block size inputs when it's predicting the next character. Okay, so we've looked at the time dimension of the tensors that are going to be feeding into the transformer. There's one more dimension to care about, and that is the batch dimension. And so as we're sampling these chunks of text, we're going to be actually every time we're going to feed them into a transformer, we're going to have many batches of multiple chunks of text that are all like stacked up in a single tensor. And that's just done for efficiency just so that we can keep the GPUs busy because they are very good at parallel processing of data. And so we just want to process multiple chunks all at the same time. But those chunks are processed completely independently. They don't take up too much space. They don't talk to each other and so on. So let me basically just generalize this and introduce a batch dimension. Here's a chunk of code. Let me just run it and then I'm going to explain what it does. So here, because we're going to start sampling random locations in the data sets to pull chunks from, I am setting the seed so that in the random number generator, so that the numbers I see here are going to be the same numbers you see later if you try to reproduce this. Now, the batch size here is how many independent sequences we are producing. We're processing every forward backward pass of the transformer. The block size, as I explained, is the maximum context length to make those predictions. So let's say batch size four, block size eight. And then here's how we get batch for any arbitrary split. If the split is a training split, then we're going to look at train data, otherwise at val data. That gives us the data array. And then when I generate random positions to grab a chunk out of, I actually grab, I actually generate random data. I actually generate random positions to grab a chunk out of. I actually generate random positions to grab a chunk out of. I actually generate random positions to grab a chunk out of. assumeonline, will generate batch size number of random offsets. So because this is four, we are, i, x is going to be a four numbers that are randomly generated between 0 and len of data minus block size. are randomly generated between 0 and len of data minus block size. So it's just random offsets into the training set. And then x' as I explained are the first block size characters, starting at i. The y' are the offset by 1 of that. So just add plus 1. And then we're going to get roughly how many random fields are generated. So just add plus 1. get those chunks for every one of integers i in ix and use a torch.stack to take all those one-dimensional tensors as we saw here and we're going to stack them up as rows and so they all become a row in a four by eight tensor so here's where i'm printing them when i sample a batch xb and yb the inputs the transformer now are the input x is the four by eight tensor four rows of eight columns and each one of these is a chunk of the training set and then the targets here are in the associated array y and they will come in to the transformer all the way at the end to create the loss function so they will give us the correct answer for every single position inside x and then these are the four independent rows so spelled out as we did before this 4x8 array contains a total of 32 examples and they're completely independent as far as the transformer is concerned so when the input is 24 the target is 43 or rather 43 here in the y array when the input is 2443 the target is 58. when the input is 2443 58 the target is 5 etc or like when it is a 52581 the target is 58 right so you can sort of see this spelled out these are the 32 independent examples packed in to a single batch of the input x and then the desired targets are in y and so now this integer tensor of x is going to feed into the transformer and that transformer is going to simultaneously process all these examples and then look up the correct integers to predict in every one of these positions in the tensor y okay so now that we have our batch of input that we'd like to feed into a transformer let's start basically feeding this into neural networks now we're going to start off with the simplest possible neural network which in the case of language modeling in my opinion is the bigram language model and we've covered the bigram language model in my make more series in a lot of depth and so here i'm going to sort of go faster and let's just implement the pytorch module directly that implements the bigram language model so i'm importing the pytorch nn module for reproducibility and then here i'm constructing a bigram language model which is a subclass of nn module and then i'm calling it and i'm passing in the inputs and the targets and i'm just printing now when the inputs and targets come here you see that i'm just taking the index the inputs and targets and then i'm just printing the inputs and targets and then i'm just printing the inputs x here which i rename to idx and i'm just passing them into this token embedding table so what's going on here is that here in the constructor we are creating a token embedding table and it is of size vocab size by vocab size and we're using an endot embedding which is a very thin wrapper around basically a tensor of shape vocab size by vocab size and what's happening here is that when we pass idx here every single integer in our input is going to refer to this embedding table and is going to pluck out a row of that embedding table corresponding to its index so 24 here will go to the embedding table and we'll pluck out the 24th row and then 43 will go here and pluck out the 43rd row etc and then pytorch is going to arrange all of this into a batch by time by channel tensor in this case batch is 4 time is 8 and c which is the channels and so we're just going to pluck out all those rows arrange them in a b by t by c and now we're going to interpret this as the logits which are basically the scores for the next character in a sequence and so what's happening here is we are predicting what comes next based on just the individual identity of a single token and you can do that because um i mean currently the tokens are not talking to each other and they're not seeing any context except for they're just seeing themselves so i'm a i'm a token number five and then i can actually make pretty decent predictions about what comes next just by knowing that i'm token 5 because some characters know um follow other characters in typical scenarios so we saw a lot of this in a lot more depth in the make more series and here if i just run this then we currently get the predictions the scores the logits for every one of the four by eight positions now that we've made predictions about what comes next we'd like to evaluate the loss function and so in this case we want to make predictions about how would the loss function Te intentions are impact the sc noises you would get this this existence and then we oriented out we would get the loss function and so at make more series we saw that a good way to measure a loss or like a quality of the predictions is to use the negative log likelihood loss which is also implemented in PyTorch under the name cross entropy. So what we'd like to do here is loss is the cross entropy on the predictions and the targets and so this measures the quality of the logits with respect to the targets. In other words we have the identity of the next character so how well are we predicting the next character based on the logits and intuitively the correct dimension of logits depending on whatever the target is should have a very high number and all the other dimensions should be very low number right. Now the issue is that this won't actually this is what we want we want to basically output the logits and the loss this is what we want but unfortunately this won't actually run we get an error message but intuitively we want to you measure this. Now when we go to the PyTorch cross entropy documentation here we're trying to call the cross entropy in its functional form so that means we don't have to create like a module for it but here when we go to the documentation you have to look into the details of how PyTorch expects these inputs and basically the issue here is PyTorch expects if you have multi-dimensional input which we do because we have a b by t by c tensor then it actually expects a multi-dimensional input which we do because we have a b by t by c tensor then it actually really wants the channels to be the second dimension here so if you so basically it wants a b by c by t instead of a b by t by c and so just the details of how PyTorch treats these kinds of inputs and so we don't actually want to deal with that so what we're going to do instead is we need to basically reshape our logits. So here's what I like to do I like to take basically give names to the dimensions So logits.shape is B by T by C and unpack those numbers. And then let's say that logits equals logits.view. And we want it to be a B times C, B times T by C. So just a two-dimensional array, right? So we're going to take all the, we're going to take all of these positions here and we're going to stretch them out in a one-dimensional sequence and preserve the channel dimension as the second dimension. So we're just kind of like stretching out the array so it's two-dimensional. And in that case, it's going to better conform to what PyTorch sort of expects in its dimensions. Now we have to do the same to targets because currently targets are of shape B by T and we want it to be just B times T. So one-dimensional. Now, alternatively, you could always still just do minus one because PyTorch will guess what this should be if you want to lay it out. But let me just be explicit and say if you can see it. If you can see it, it's going to be B times T. Once we reshape this, it will match the cross-entropy case and then we should be able to evaluate our loss. Okay, so that right now, and we can do loss. And so currently we see that the loss is 4.87. Now, because we have 65 possible vocabulary elements, we can actually guess at what the loss should be. And in particular, we covered negative log likelihood in a lot of detail. We are expecting, we're expecting log or lon of one over 65 and negative of that. So we're expecting the loss to be about 4.17, but we're getting 4.87. And so that's telling us that the initial predictions are not super diffuse. They've got a little bit of entropy. And so we're guessing wrong. So yes, but actually we are able to evaluate the loss. Okay, so now that we can evaluate the quality of the model on some data, we'd like to also be able to generate from the model. So let's do the generation. Now I'm going to go again a little bit faster here because I covered all this already in the previous videos. So here's a generate function for the model. So we take some, we take the same kind of input IDX here. And basically this is the current context of some characters in a batch, in some batch. So it's also B by T. And the job of generate is to basically take this B by T and extend it to be B by T plus one, plus two, plus three. And so it's just basically, it continues the generation in all the batch dimensions in the time dimension. So that's its job. And it will do that for max new tokens. So you can see here on the bottom, there's going to be some stuff here, but on the bottom, whatever is predicted is concatenated on top of the previous IDX along the first dimension, which is the time dimension to create a B by T plus one. So that becomes a new IDX. So the job of generate is to take a B by T and make it a B by T plus one, plus two, plus three, as many as we want max new tokens. So this is the generation from the model. Now inside the generation, what are we doing? We're taking the current indices. We're getting the predictions. So we get those are in the logits. And then the loss here is going to be ignored because we're not using that. And we have no targets that are sort of ground truth targets that we're going to be comparing with. Then once we get the logits, we are only focusing on the last step. So instead of a B by T by C, we're going to pluck out the negative one, the last element in the time dimension, because those are the predictions for what comes next. So that gives us the logits, which we then convert to probabilities via softmax. And then we use torch.multinomial to sample from those probabilities. And we ask PyTorch to give us one sample. And so IDX next, we'll become a B by one, because in each one of the batch dimensions, we're going to have a single prediction for what comes next. So this numSamples equals one, will make this be a one. And then we're going to take those integers that come from the sampling process according to the probability distribution given here. And those integers got just concatenated on top of the current sort of like running stream of integers. And this gives us a B by T plus one. And then we can return that. Now, one thing here is, here is you see how i'm calling self of idx which will end up going to the forward function i'm not providing any targets so currently this would give an error because targets is uh is uh sort of like not given so target has to be optional so targets is none by default and then if targets is none then there's no loss to create so it's just loss is none but else all of this happens and we can create a loss so this will make it so um if we have the targets we provide them and get a loss if we have no targets we'll just get the logits so this here will generate from the model and let's take that for a ride now oops so i have another code chunk here which will generate for the model from the model and okay this is kind of crazy so maybe let me let me break this down so these are the idx right i'm creating a batch will be just one time will be just one so i'm creating a little one by one tensor and it's holding a zero and the d type the data type is uh integer so zero is going to be how we kick off the generation and remember that zero is uh is the element standing for a new line character so it's kind of like a reasonable thing to to feed in as the very first character sequence to be the new line um so it's going to be idx which we're going to feed in here then we're going to ask for 100 tokens and then end that generate will continue that now because uh generate works on the level of batches we then have to index into the zero throw to basically unplug the um the single batch dimension that exists and then that gives us a um time steps is just a one-dimensional array of all the indices which we will convert to simple python list from pytorch tensor so that that can feed into our decode function and convert those integers into text so let me bring this back and we're generating 100 tokens let's run and uh here's the generation that we achieved so obviously it's garbage and the reason it's garbage is because this is a totally random model so next up we're going to want to do is we're going to want to do a we're going to want to train this model now one more thing i wanted to point out here is this function is written to be general but it's kind of like ridiculous right now because we're feeding in all this we're building out this context and we're concatenating it all and we're always feeding it all into the model but that's kind of ridiculous because this is just a simple bigram model so to make for example this prediction about k we only needed this w but actually what we fed into the model is we fed the entire sequence and then we only looked at the very last piece and predicted k so the only reason i'm writing it in this way is because right now this is a bigram model but i'd like to keep this function fixed and i'd like it to work later when our characters actually basically look further in the history and so right now the history is not used so this looks silly but eventually the history will be used and so that's why we want to do it this way so just a quick comment on that so now we see that this is random so let's train the model so it becomes a bit less random okay let's now train the model so first what i'm going to do is i'm going to create a pytorch optimization object so here we are using the optimizer adam w now in the make more series we've only ever used stochastic gradient descent the simplest possible optimizer which you can get using the sgd instead but i want to use adam which is a much more advanced and popular optimizer and it works extremely well for a lot of other optimizers but i want to use adam which is a much more advanced and popular optimizer and it works extremely well typical good setting for the learning rate is roughly 3e negative 4 but for very very small networks like it's the case here you can get away with much much higher learning rates 1-3 or even higher probably but let me create the optimizer object which will basically take the gradients and update the parameters using the gradients and then here our batch size up above was only 4 so let me actually use something bigger let's say 32 and then for some number of steps we're sampling a new batch of data we're evaluating the loss we're zeroing out all the gradients from the previous step getting the gradients for all the parameters and then using those gradients to update our parameters so typical training loop as we saw in the make more series so let me now run this for say 100 iterations and let's see what kind of loss is we're going to get so we started around 4.7 and now we're getting down to like 4.6 so the optimization is definitely happening but let's sort of try to increase the number of iterations and only print at the end because we probably will not train for longer okay so we're down to 3.6 roughly roughly down to three this is the most janky optimization if we do that and clean those up we get six hours of telly in in mobile okay it's working let's just do 10 000 and then from here we want to copy this and hopefully we're going to get something reasonable and of course it's not going to be shakespeare from a bigger model but at least we see that the loss is improving and hopefully we're expecting something a bit more reasonable so we're down in about 2.5-ish let's see what we get okay Let me just increase the number of tokens. Okay, so we see that we're starting to get something at least like reasonable-ish. Certainly not Shakespeare, but the model is making progress. So that is the simplest possible model. So now what I'd like to do is, obviously, this is a very simple model because the tokens are not talking to each other. So given the previous context of whatever was generated, we're only looking at the very last character to make the predictions about what comes next. So now these tokens have to start talking to each other and figuring out what is in the context so that they can make better predictions for what comes next. And this is how we're going to kick off the transformer. Okay, so next, I took the code that we developed in this Jupyter notebook and I converted it to be a script. And I'm doing this because I just want to simplify our intermediate work, which is just the final product that we have. At this point, so in the top here, I put all the hyperparameters that we've defined. I introduced a few and I'm going to speak to that in a little bit. Otherwise, a lot of this should be recognizable, reproducibility, read data, get the encoder and decoder, create the train and test splits, use the kind of like data loader that gets a batch of the inputs and targets. This is new, and I'll talk about it in a second. Now, this is the bigram language model that we developed. And it can forward and give us a logits and loss and it can generate. And then here we are creating the optimizer and this is the training loop. So everything here should look pretty familiar. Now, some of the small things that I added. Number one, I added the ability to run on a GPU if you have it. So if you have a GPU, then you can, this will use CUDA instead of just CPU and everything will be a lot more faster. Now, when device becomes CUDA, then we need to make sure that when we load the data. We move it to device. When we create the model, we want to move the model parameters to device. So as an example, here we have the in an embedding table and it's got a dot weight inside it, which stores the sort of lookup table. So that would be moved to the GPU so that all the calculations here happen on the GPU and they can be a lot faster. And then finally here, when I'm creating the context that feeds into generate, I have to make sure that I create on the device. Number two, when I enter. Introduced is the fact that here in the training loop. Here, I was just printing the loss dot item inside the training loop. But this is a very noisy measurement of the current loss because every batch will be more or less lucky. And so what I want to do usually is I have an estimate loss function and the estimate loss basically then goes up here and it averages up. The loss over multiple batches. So in particular, we're going to iterate eval, either times and we're going to basically get our loss and then we're going to get the average loss for both splits. And so this will be a lot less noisy. So here when we call the estimate loss, we're going to report the pretty accurate train and validation loss. Now when we come back up, you'll notice a few things here. I'm setting the model to evaluation phase and down here. I'm resetting it back to training phase. Now right now for our model as is, this doesn't actually do anything because the only thing inside this model is this nn.embedding and this network would behave the same in both evaluation mode and training mode. We have no dropout layers. We have no batch drum layers, etc. But it is a good practice to think through what mode your neural network is in because some layers will have different behavior at inference time or training time. And there's also this context manager, torch.nograd, and this is just telling PyTorch that everything that happens inside this function, we will not call .backward on. And so PyTorch can be a lot more efficient with its memory use because it doesn't have to store all the intermediate variables because we're never going to call backward. And so it can be a lot more efficient in that way. So also a good practice to tell PyTorch when we don't intend to do backpropagation. So, right now, this script is about 120 lines of code of and that's kind of our starter code. I'm calling it bigram.py and I'm going to release it later. Now running this script gives us output in the terminal and it looks something like this. It basically, as I ran this code, it was giving me the train loss and val loss. And we see that we convert to somewhere around 2.5 with the bigram model. And then here's the sample that we produced at the end. And so we have everything packaged up in the script and we're in a good position now to iterate on this. Okay, so we are almost ready to start writing our very first self-attention block for processing these tokens. Now, before we actually get there, I want to get you used to a mathematical trick that is used in the self-attention inside a transformer and is really just like at the heart of an efficient implementation of self-attention. And so I want to work with this toy example to just get you used to this operation. And then it's going to make it much more clear once we actually get to it in the script again. So let's create a B by T by C where B, T and C are just 4, 8 and 2 in this toy example. And these are basically channels and we have batches and we have the time component and we have some information at each point in the sequence. So C. Now what we would like to do is we would like these tokens. So we have up to eight tokens here in a batch. And these eight tokens are currently not talking to each other and we would like them to talk to each other. We'd like to couple them. And in particular, we don't we want to couple them in this very specific way. So the token, for example, at the fifth location, it should not communicate with tokens in the sixth seventh and eighth location because those are future tokens in the sequence. The token on the fifth location should only talk to the one in the fourth third second and first. So it's only so information only flows. From previous context to the current time step and we cannot get any information from the future because we are about to try to predict the future. So what is the easiest way for tokens to communicate? Okay, the easiest way I would say is okay. If we are up to if we're a fifth token and I'd like to communicate with my past the simplest way we can do that is to just do a weight is to just do an average of all the of all the preceding elements. So for example, if I'm the fifth token, I would like to take the channels that make up that are information at my step, but then also the channels from the fourth step third step second step in the first step. I'd like to average those up and then that would become sort of like a feature vector that summarizes me in the context of my history. Now, of course, just doing a sum or like an average is an extremely weak form of interaction. Like this communication is extremely lossy. We've lost a ton of information about spatial arrangements of all those tokens, but that's okay. For now, we'll see how we can bring that information back later. For now, what we would like to do is for every single batch element independently for every teeth token in that sequence. We'd like to now calculate the average of all the vectors in all the previous tokens and also at this token. So let's write that out. I have a small snippet here and instead of just fumbling around, let me just copy paste it and talk to it. So in other words, we're going to create X and B O W is short for bag of words because bag of words is is kind of like a term that people use when you are just averaging up things. So this is just a bag of words. Basically, there's a word stored on every one of these eight locations and we're doing a bag of words for just averaging. So in the beginning, we're going to say that it's just initialized at zero and then I'm doing a for loop here. So we're not being efficient yet. That's coming. But for now, we're just iterating over all the batch dimensions independently. Iterating over time and then the previous tokens are at this batch dimension and then everything up to and including the teeth token. Okay. So when we slice out X in this way, Xprev becomes of shape, how many T elements there were in the past and then of course C. So all the two-dimensional information from these little tokens. So that's the previous sort of chunk of tokens from my current sequence. And then I'm just doing the average or the mean over the zero dimension. So I'm averaging out the time here and I'm just going to get a little C one-dimensional vector, which I'm going to store in X bag of words. So I can run this and this is not going to be very informative because let's see. So this is X of zero. So this is the zeroth batch element and then expo at zero. Now, you see how the at the first location here, you see that the two are equal and that's because it's we're just doing an average of this one token. But here this one is now an average of these two. And now this one is an average of these three. And so on. So and this last one is the average of all of these elements. So vertical average just averaging up all the tokens now gives this outcome. Here. So this is all well and good, but this is very inefficient. Now. The trick is that we can be very very efficient about doing this using matrix multiplication. So that's the mathematical trick. And let me show you what I mean. Let's work with the toy example here. You run it and I'll explain. I have a simple matrix here. That is three by three of all ones a matrix B of just random numbers and it's a three by two and a matrix C, which will be three by three multiply three by two. Which will give out a three by two. So here we're just using matrix multiplication. So a multiply B gives us C. Okay, so how are these numbers in C achieved? Right? So this number in the top left is the first row of a dot product with the first column of B. And since all the row of a right now is all just once then the dot product here with with this column of B. Is just going to do a sum of these of this column. So two plus six plus six is 14. The element here in the output of C is also the first column here. The first row of a multiplied now with the second column of B. So seven plus four plus plus five is 16. Now you see that there's repeating elements here. So this 14 again is because this row is again all once and it's multiplying the first column of B. So we get 14 and this one is and so on. So this last number here is the. The last row dot product last column. Now the trick here is the following. This is just a boring number of is just a boring array of all once but torch has this function called trill which is short for a triangular. Something like that and you can wrap it in torch that once and it will just return the lower triangular portion of this. Okay. So now it will basically zero out. Of these guys here. So we just get the lower triangular part. Well, what happens if we do that? So now we'll have a like this and be like this. And now what are we getting here and see? Well, what is this number? Well, this is the first row times the first column and because this is zeros. These elements here are now ignored. So we just get a two and then this number here is the first row times the second column. And because these are zeros they get ignored and it's just seven. The seven multiplies this one. But look what happened here because this is one and then zeros. We what ended up happening is we're just plucking out the row of this row of B and that's what we got. Now here we have one one zero. So here one one zero dot product with these two columns will now give us two plus six which is eight and seven plus four which is 11. And because this is one one one we ended up with. The addition of all of them. And so basically depending on how many ones and zeros we have here. We are basically doing a sum currently of the variable number of these rows and that gets deposited into C. So currently we're doing sums because these are ones but we can also do average right and you can start to see how we could do average of the rows of B sort of an incremental fashion. Because we don't have to we can basically normalize. These rows so that they sum to one and then we're going to get an average. So if we took a and then we did a equals a divide a torch dot sum in the. Of a in the. One. Dimension and then let's keep them is true. So therefore the broadcasting will work out. So if I rerun this you see now that these rows now sum to one. So this row is one this row is point five point five zero. And here we get one thirds. And now when we do a multiply be what are we getting. Here we are just getting the first row first row. Here now we are getting the average of the first two rows. Okay so two and six average is four and four and seven averages five point five. And on the bottom here we are now getting the average of these three rows. So the average of all of elements of B are now deposited here. And so you can see that by manipulating these elements of this multiplying matrix and then multiplying it with any given matrix. We can do these averages in this incremental fashion because we just get. And we can manipulate that based on the elements of a. Okay so that's very convenient so let's swing back up here and see how we can vectorize this and make it much more efficient using what we've learned. So in particular. We are going to produce an array. But here I'm going to call it way short for weights. But this is our a. And this is how much of every row we want to average up and it's going to be an average because you can see that these rows sum to one. So this is our a and then our B in this example of course is. X. So it's going to happen here now is that we are going to have an expo to. And this expo to is going to be way. Multiplying. Rx. So let's think this through way is T by T and this is matrix multiplying in PyTorch a B by T by C. And it's giving us. What shape. So PyTorch will come here and it will see that these shapes are not the same so it will create a bash dimension here and this is a batch matrix multiply. And so it will apply this matrix multiplication in all the batch elements. In parallel. And individually and then for each batch element there will be a T by T multiplying T by C exactly as we had below. So this will now create. B by T by C. And expo to will now become identical to expo. So. We can see that torched out all close. Of expo and expo to should be true now. So this kind of like misses us that these are in fact the same. So expo and expo to if I just print them. Okay, we're not going to be able to. Okay, we're not going to be able to just stare it down but. Well, let me try expo basically just at the 0th element and expo to at the 0th element. So just the first batch and we should see that this and that should be identical which they are. Right. So what happened here. The trick is we were able to use batch matrix multiply to do this aggregation really and it's a weighted aggregation and the weights are specified in this T by T array. And we're basically doing weighted sums and these weighted sums are according to the weights inside here that take on sort of this triangular form. And so that means that a token at the teeth dimension will only get. Sort of information from the tokens preceding it. So that's exactly what we want. And finally, I would like to rewrite it in one more way. And we're going to see why that's useful. So this is the third version and it's also identical to the first and second, but let me talk through it. It uses softmax. So trill here is this Matrix lower triangular once way begins as all zero. Okay, so if I just print way in the beginning, it's all zero then I used masked fill. So what this is doing is wait that masked fill it's all zeros. And I'm saying for all the elements where trill is equals equals zero make them be negative Infinity. So all the elements where trill is zero will become negative Infinity now. So this is what we get. And then the final line here is softmax. So if I take a softmax along every single so dim is negative one so long every single row if I do a softmax, what is that going to do? Well softmax is is also like a normalization operation, right? And so spoiler alert you get the exact same Matrix. Let me bring back the softmax and recall that in softmax. We're going to exponentiate every single one of these. And then we're going to divide by the sum. And so if we exponentiate every single element here, we're going to get a one and here we're going to get basically zero zero zero zero everywhere else. And then when we normalize we just get one here. We're going to get one one and then zeros and then softmax will again divide and this will give us 0.5 0.5 and so on. And so this is also the same way to produce this mask. Now the reason that this is a bit more interesting. And the reason we're going to end up using it in self-attention is that these weights here begin with zero and you can think of this as like an interaction strength or like an affinity. So basically it's telling us how much of each token from the past do we want to aggregate and average up and then this line is saying tokens from the past cannot communicate by setting them to negative Infinity. We're saying that we will not. Aggregate anything from those tokens. And so basically this then goes through softmax and through the weighted and this is the aggregation through matrix multiplication. And so what this is now is you can think of these as the zeros are currently just set by us to be zero but quick preview is that these affinities between the tokens are not going to be just constant at zero. They're going to be data dependent. These tokens are going to start looking at each other and some tokens will find other tokens. More or less interesting and depending on what their values are, they're going to find each other interesting to different amounts and I'm going to call those affinities. I think and then here we are saying the future cannot communicate with the past. We're going to clamp them. And then when we normalize and some we're going to aggregate sort of their values depending on how interesting they find each other. And so that's the preview for self-attention and basically long story short from this entire section is that. You can do weighted aggregations of your past elements by having by using matrix multiplication of a lower triangular fashion. And then the elements here in the lower triangular part are telling you how much of each element fuses into this position. So we're going to use this trick now to develop the self-attention block. So first let's get some quick preliminaries out of the way. First the thing I'm kind of bothered by is that you see how we're passing in vocab size into the constructor. You don't need to do that because vocab size is already defined up top as a global variable. So there's no need to pass this stuff around. Next what I want to do is I don't want to actually create. I want to create like a level of indirection here where we don't directly go to the embedding for the logits. But instead we go through this intermediate phase because we're going to start making that bigger. So let me introduce a new variable and embed it short for number of embedding dimensions. So an embed. Here will be say 32. That was a suggestion from GitHub copilot by the way. It also suggested 32 which is a good number. So this is an embedding table and only 32 dimensional embeddings. So then here this is not going to give us logits directly. Instead this is going to give us token embeddings. That's what I'm going to call it. And then to go from the token embeddings to the logits we're going to need a linear layer. So self.lmhead let's call it short for language modeling head. Is an in linear from an embed up to vocab size. And then we swing over here. We're actually going to get the logits by exactly what the copilot says. Now we have to be careful here because this C and this C are not equal. This is an embed C and this is vocab size. So let's just say that an embed is equal to C. And then this just creates one spurious layer of indirection through a linear layer. But this should basically run. So we see that this runs and this currently looks kind of spurious. But we're going to build on top of this. Now next up so far. We've taken these indices and we've encoded them based on the identity of the tokens inside IDX. The next thing that people very often do is that we're not just encoding the identity of these tokens, but also their position. So we're going to have a second position embedding table here. So solve that position embedding table is an embedding of block size by an embed. And so each position from zero to block size minus one will also get its own embedding vector. And then here first, let me decode B by T from IDX dot shape. And then here we're also going to have a plus embedding, which is the positional embedding. And these are this is torr dash arrange. So this will be basically just integers from zero to zero. To T minus one. And all of those integers from zero to T minus one get embedded through the table to create a T by C. And then here this gets renamed to just say X and X will be the addition of the token embeddings with the positional embeddings. And here the broadcasting note will work out. So B by T by C plus T by C. This gets right aligned and new dimension of one gets added and it gets broadcasted across batch. So at this point X. Holds not just the token identities, but the positions at which these tokens occur. And this is currently not that useful because of course, we just have a simple migraine model. So it doesn't matter if you're in the fifth position, the second position or wherever it's all translation invariant at this stage. So this information currently wouldn't help. But as we work on the self-attention block, we'll see that this starts to matter. Okay, so now we get the crux of self-attention. So this is probably the most important part of this video. To understand. We're going to implement a small self-attention for a single individual head as they're called. So we start off with where we were. So all of this code is familiar. So right now I'm working with an example where I change the number of channels from 2 to 32. So we have a 4 by 8 arrangement of tokens and each token and the information at each token is currently 32 dimensional. But we just are working with random numbers. Now we saw here that The code as we had it before does a simple weight simple average of all the past tokens and the current token. So it's just the previous information and current information is just being mixed together in an average. And that's what this code currently achieves. And it does so by creating this lower triangular structure, which allows us to mask out this weight matrix that we create. So we mask it out and then we normalize it and currently When we initialize the affinities between all the different sort of tokens or nodes, I'm going to use those terms interchangeably. So when we initialize the affinities between all the different tokens to be zero, then we see that way gives us this structure where every single row has these Uniform numbers. And so that's what that's what then in this matrix multiply makes it so that we're doing a simple average. Now, We don't actually want. This to be All uniform Because different tokens will find different other tokens more or less interesting and we want that to be data dependent. So for example, if I'm a vowel then maybe I'm looking for consonants in my past and maybe I want to know what those consonants are and I want that information to flow to me. And so I want to now gather information from the past, but I want to do it in a data dependent way. And this is the problem that self-attention solves. Now the way self-attention solves this Is the following. Every single node or every single token at each position will emit two vectors. It will emit a query and it will emit a key. Now the query vector roughly speaking is what am I looking for? And the key vector roughly speaking is what do I contain? And then the way we get affinities between these tokens now in a sequence is we basically just do a dot product between the keys and the query. So my query dot products with all the keys of all the other tokens and that dot product now becomes way. And so if the key and the query are sort of aligned, they will interact to a very high amount and then I will get to learn more about that specific token as opposed to any other token in the sequence. So let's implement this now. We're going to implement a single what's called head of self-attention. So this is just one head. There's a hyper parameter involved with these heads, which is the head size. And then here I'm initializing linear modules and I'm using bias equals false. So these are just going to apply a matrix multiply with some fixed weights. And now let me produce a key and Q K and Q by forwarding these modules on X. So the size of this. This will not become B by T by 16 because that is the head size and the same here B by T by 16. So this being the head size. So you see here that when I forward this linear on top of my X all the tokens in all the positions in the B by T arrangement all of them in parallel and independently produce a key and a query. So no communication has happened yet. But the communication comes now. All the queries will dot product with all the keys. So basically what we want is we want way now or the affinities between these to be query multiplying key, but we have to be careful with we can't matrix multiply this. We actually need to transpose K but we have to be also careful because these are when you have the batch dimension. So in particular we want to transpose the last two dimensions. Dimension. Negative one and dimension negative two. So negative two negative one. And so this matrix multiply now will basically do the following B by T by 16. Matrix multiplies B by 16 by T to give us B by T by T. Right? So for every row of B, we're not going to have a T square matrix giving us the affinity. We're going to have a T square matrix giving us the affinities and these are now the way so they're not zeros. They are now coming from this dot product between the keys in the queries. So this can now run I can I can run this and the weighted aggregation now is a function in a data abandoned manner between the keys and queries of these notes. So just inspecting what happened here the way takes on this form. And you see that before way was just a constant. There's no way to all the batch elements. But now every single batch elements will have different sort of way because every single batch element contains different tokens at different positions. And so this is not data dependent. So when we look at just the zero row, for example in the input, these are the weights that came out. And so you can see now that they're not just exactly uniform. And in particular as an example here for the last row, this was the eighth token and the eighth token knows what content. It has and it knows at what position it's in. And now the eight token based on that creates a query. Hey, I'm looking for this kind of stuff. I'm a vowel. I'm on the eighth position. I'm looking for any consonants at positions up to four. And then all the nodes get to emit keys and maybe one of the channels could be I am a I am a consonant and I am in a position up to four. And that key would have a high number in that specific channel. And that's how the query and the key when they dark product, they can find each other and create a high affinity. And when they have a high affinity, like say this token was pretty interesting to to this eighth token. When they have a high affinity, then through the softmax, I will end up aggregating a lot of its information into my position. And so I'll get to learn a lot about it. Now just this was looking at way after this has already happened. Let me erase this operation as well. So let me erase the masking and the softmax just to show you the under the hood internals and how that works. So without the masking and the softmax way comes out like this, right? This is the outputs of the dark products. And these are the raw outputs and they take on values from negative, you know, two to positive two Etc. So that's the raw interactions and raw Affinities between all the nodes. But now if I'm a if I'm a fifth node, I will not want to aggregate anything from the sixth node seventh node and the eighth node. So actually we use the upper triangular masking. So those are not allowed to communicate. And now we actually want to have a nice distribution. So we don't want to aggregate negative point one one of this note. That's crazy. So instead we exponentiate and normalize. And now we get a nice distribution that sums to one. And this is telling us now in the data dependent manner, how much of information to aggregate from any of these tokens in the past. So that's way. And it's not zeros anymore, but but it's calculated in this way. Now, there's one more part to a single self-attention head. And that is that when we do the aggregation, we don't actually aggregate the tokens. Exactly. We aggregate, we produce one more value here and we call that the value. So in the same way that we produced key and query, we're also going to create a value. And then here we don't aggregate. X we calculate a V, which is just achieved by propagating this linear on top of X again. And then we output way multiplied by V. So V is the elements that we aggregate or the vector that we aggregate instead of the raw X. And now of course, this will make it so that the output here of the single head will be 16 dimensional because that is the head size. So you can think of X as kind of like private information. So you can think of X as kind of like private information. To this token, if you if you think about it that way. So X is kind of private to this token. So I'm a fifth token at some and I have some identity and my information is kept in vector X. And now for the purposes of the single head, here's what I'm interested in. Here's what I have. And if you find me interesting, here's what I will communicate to you. And that's stored in V. And so V is the thing that gets aggregated for the purposes of this single head between the different nodes. And that's basically the self-attention mechanism. This is this is what it does. There are a few notes that I would make like to make about attention. Number one attention is a communication mechanism. You can really think about it as a communication mechanism where you have a number of nodes in a directed graph where basically you have edges pointing between nodes like this. And what happens is every node has some vector of information and it gets to aggregate information via a weighted sum from all of the nodes that point to it. And this is done in a data dependent manner. So depending on whatever data is actually stored at each node at any point in time. Now, our graph doesn't look like this. Our graph has a different structure. We have eight nodes because the block size is eight and there's always eight tokens. And the first node is only pointed to by itself. The second node is pointed to by the first node and itself all the way up to the eighth node, which is pointed to by itself. Pointed to by all the previous nodes and itself. And so that's the structure that are directed graph has or happens happens to have an autoregressive sort of scenario like language modeling. But in principle attention can be applied to any arbitrary directed graph and it's just a communication mechanism between the nodes. The second note is that notice that there is no notion of space. So attention simply acts over like a set of vectors in this graph. And so by default these nodes have no idea where they are positioned in the space. And that's why we need to encode them positionally and sort of give them some information that is anchors to a specific position so that they sort of know where they are. And this is different than for example from convolution because if you run for example, a convolution operation over some input there is a very specific sort of layout of the information in space and the convolutional filters sort of act in space. And so it's it's not like an attention and attention is just a set of vectors out there in space. They communicate and if you want them to have a notion of space you need to specifically add it which is what we've done when we calculated the relative the positional encode encodings and added that information to the vectors. The next thing that I hope is very clear is that the elements across the batch dimension which are independent examples never talk to each other. They're always processed independently and this is a batch matrix multiply that applies basically a matrix multiplication kind of in parallel across the batch dimension. So maybe it would be more accurate to say that in this analogy of a directed graph. We really have because the batch size is for we really have four separate pools of eight nodes and those eight nodes only talk to each other but in total there's like 32 nodes that are being processed but there's sort of four separate pools of eight you can look at it that way. The next note is that here in the case of language modeling we have this specific structure of directed graph where the future tokens will not communicate to the past tokens but this doesn't necessarily have to be the constraint in the general case. And in fact in many cases you may want to have all of the notes talk to each other fully. So as an example if you're doing sentiment analysis or something like that with a transformer you might have a number of tokens and you may want to have them all talk to each other fully because later you are predicting for example the sentiment of the sentence and so it's okay for these notes to talk to each other and so in those cases you will use an encoder block of self-attention and all it means that it's an encoder block. Is that you will delete this line of code allowing all the notes to completely talk to each other. What we're implementing here is sometimes called a decoder block and it's called a decoder because it is sort of like decoding language and it's got this autoregressive format where you have to mask with the triangular matrix so that notes from the future never talk to the past because they would give away the answer. And so basically in encoder blocks you would delete this allow all the notes to talk to each other. In decoder blocks this will always be present so that you have this triangular structure but both are allowed and attention doesn't care. Attention supports arbitrary connectivity between notes. The next thing I wanted to comment on is you keep me you keep hearing me say attention self-attention etc. There's actually also something called cross attention. What is the difference? So basically the reason this attention is self-attention is because the keys queries and the values are all coming from the same source. From X so the same source X produces keys queries and values. So these nodes are self-attending but in principle attention is much more general than that. So for example in encoder decoder transformers you can have a case where the queries are produced from X but the keys and the values come from a whole separate external source and sometimes from encoder blocks that encode some context that we'd like to condition on and so the keys and the values will actually come from a whole separate source. Those are nodes on the side and here. We're just producing queries and we're reading off information from the side. So cross attention is used when there's a separate source of nodes. We'd like to pull information from into our notes and it's self-attention. If we just have nodes that would like to look at each other and talk to each other. So this attention here happens to be self-attention. But in principle attention is a lot more general. Okay in the last note at this stage is if we come to the attention is all you need paper here. We've already implemented attention. So given query key and value we've multiplied the query on the key. We've soft maxed it and then we are aggregating the values. There's one more thing that we're missing here, which is the dividing by 1 over square root of the head size. The DK here is the head size. Why are they doing this? Why is this important? So they call it a scaled attention and it's kind of like an important normalization to basically have. The problem is. If you have unit Gaussian inputs, so 0 mean unit variance, K and Q are unit Gaussian. And if you just do way naively, then you see that your way actually will be the variance will be on the order of head size, which in our case is 16. But if you multiply by 1 over head size square root, so this is square root and this is 1 over then the variance of way will be 1. So it will be preserved. Now, why is this important? You'll notice that way here will feed into softmax. And so it's really important, especially at initialization that way be fairly diffuse. So in our case here, we sort of lucked out here and way had a fairly diffuse numbers here. So like this. Now, the problem is that because of softmax, if weight takes on very positive and very negative numbers inside it, softmax will actually converge towards one hot vectors. And so I can illustrate that here. Say, we are applying softmax to a tensor of values that are very close to zero. Then we're going to get a diffuse thing out of softmax. But the moment I take the exact same thing and I start sharpening it, making it bigger by multiplying these numbers by 8, for example, you'll see that the softmax will start to sharpen. And in fact, it will sharpen towards the max. So it will sharpen towards whatever number here is the highest. And so basically we don't want these values to be too extreme, especially the initialization. Otherwise softmax will be way too peaky and um, you're basically aggregating information from like a single node. Every node just aggregates information from a single other node. That's not what we want, especially at initialization. And so the scaling is used just to control the variance at initialization. Okay. So having said all that, let's now take our self-attention knowledge and let's take it for a spin. So here in the code, I've created this head module and implements a single head of self-attention. So you give it a head size and then here it creates the key query and evaluate. Linear layers. Typically people don't use biases in these. So those are the linear projections that we're going to apply to all of our nodes. Now here, I'm creating this trill variable. Trill is not a parameter of the module. So in sort of pytorch naming conventions, this is called a buffer. It's not a parameter and you have to call it. You have to assign it to the module using a register buffer. So that creates the trill, the lower triangular matrix. And when we're given the input X, this should look very familiar now. We calculate the keys, the queries, we calculate the attention scores inside way. We normalize it. So we're using scaled attention here. Then we make sure that sure doesn't communicate with the past. So this makes it a decoder block and then softmax and then aggregate the value and output. Then here in the language model, I'm creating a head in the constructor and I'm calling it self-attention head and the head size. I'm going to keep as the same and embed just for now. And then here once we've encoded the information with the token embeddings and the position embeddings, we're simply going to feed it into the self-attention head and then the output of that is going to go into the decoder language modeling head and create the logits. So this is sort of the simplest way to plug in a self-attention component into our network right now. I had to make one more change, which is that here in the generate, we have to make sure that our IDX that we feed into the model because now we're using positional embeddings, we can never have more than block size coming in because if IDX is more than block size, then our position embedding table is going to run out of scope because it only has embeddings for up to block size. And so therefore I added some code here to crop the context that we're going to feed into self so that we never pass in more than block size elements. So those are the changes and let's now train the network. Okay, so I also came up to the script here and I decreased the learning rate because the self-attention can't tolerate very very high learning rates. And then I also increased the number of iterations because the learning rate is lower and then I trained it and previously we were only able to get to up to 2.5 and now we are down to 2.4. So we definitely see a little bit of improvement from 2.5 to 2.4 roughly, but the text is still not amazing. So clearly the self-attention head is doing some useful communication, but we still have a long way to go. Okay. So now we've implemented the scale dot product attention. Now next up in the attention is all you need paper. There's something called multi-head attention. And what is multi-head attention? It's just applying multiple attentions in parallel and concatenating the results. So they have a little bit of diagram here. I don't know if this is super clear. It's really just multiple attentions in parallel. So let's implement that fairly straightforward. If we want a multi-head attention, then we want multiple heads of self-attention running in parallel. So in PyTorch we can do this by simply creating multiple heads. So however many heads you want and then what is the head size of each and then we run all of them in parallel into a list and simply concatenate all of the outputs and we're concatenating over the channel dimension. So the way this looks now is we don't have just a single attention that has a head size of 32 because remember an embed is 32. Instead of having one communication channel, we now have four communication channels in parallel and each one of these communication channels typically will be smaller correspondingly. So because we have four communication channels, we want eight-dimensional self-attention. And so from each communication channel, we're getting together eight-dimensional vectors and then we have four of them. And that concatenates to give us 32, which is the original and embed. And so this is kind of similar to if you're familiar with convolutions, this is kind of like a group convolution because basically instead of having one large convolution, we do convolution in groups and that's multi-headed self-attention. And so then here we just use essay heads, self-attention heads instead. Now, I actually ran it and scrolling down, I ran the same thing and then we now get down to 2.28 roughly and the output is still, the generation is still not amazing, but clearly the validation loss is improving because we were at 2.4 just now. And so it helps to have multiple communication channels because obviously these tokens have a lot to talk about. They want to find the consonants, the vowels, they want to find the vowels just from certain positions, they want to find any kinds of different things. And so it helps to create multiple independent channels of communication, gather lots of different types of data and then decode the output. Now going back to the paper for a second, of course, I didn't explain this figure in full detail, but we are starting to see some components of what we've already implemented. We have the positional encodings, the token encodings that add, we have the masked multi-headed attention implemented. Now, here's another multi-headed attention, which is a cross attention to an encoder, which we haven't, we're not going to implement in this case. I'm going to come back to that later. But I want you to notice that there's a feed forward part here and then this is grouped into a block that gets repeated. And again, now the feed forward part here is just a simple multi-layer perceptron. So the multi-headed, so here position wise feed forward networks is just a simple little MLP. So I want to start basically in a similar fashion. Also adding computation into the network and this computation is on the per node level. So I've already implemented it and you can see the diff highlighted on the left here when I've added or changed things. Now before we had the multi-headed self-attention that did the communication, but we went way too fast to calculate the logits. So the tokens looked at each other, but didn't really have a lot of time to think on what they found from the other tokens. And so what I've implemented here is a little feed forward single layer and this little layer is just a linear followed by a relu non-linearity and that's it. So it's just a little layer and then I call it feed forward and embed. And then this feed forward is just called sequentially right after the self-attention. So we self-attend then we feed forward and you'll notice that the feed forward here when it's applying linear. This is on a per token level. All the tokens do this independently. So the self-attention is the communication and then once they've gathered all the data now they need to think on that data individually. And so that's what feed forward is doing and that's why I've added it here. Now when I train this the validation laws actually continues to go down now to 2.24. Which is down from 2.28. The output still look kind of terrible, but at least we've improved the situation. And so as a preview we're going to now start to intersperse the communication with the computation and that's also what the transformer does when it has blocks that communicate and then compute and it groups them and replicates them. Okay, so let me show you what we'd like to do. We'd like to do something like this. We have a block and this block is basically this part here except for the cross attention. Now the block basically intersperses communication and then computation. The computation is done using multi-headed self-attention and then the computation is done using a feed forward network on all the tokens independently. Now what I've added here also is you'll notice this takes the number of embeddings in the embedding dimension and number of heads that we would like which is kind of like group size in group convolution. And I'm saying that number of heads we'd like is four and so because this is 32 we calculate that because this is 32 the number of heads should be four the head size should be eight so that everything sort of works out channel wise. So this is how the transformer structures sort of the sizes typically. So the head size will become eight and then this is how we want to intersperse them. And then here I'm trying to create blocks which is just a sequential application of block block block. So that we're interspersing communication feed forward many many times and then finally we decode. Now actually try to run this and the problem is this doesn't actually give a very good answer and very good result. And the reason for that is we're starting to actually get like a pretty deep neural net and deep neural nets suffer from optimization issues. And I think that's what we're kind of like slightly starting to run into. So we need one more idea that we can borrow from the transformer paper to resolve those difficulties. Now there are two optimizations that dramatically help with the depth of these networks and make sure that the networks remain optimizable. Let's talk about the first one. The first one in this diagram is you see this arrow here and then this arrow and this arrow. Those are skip connections or sometimes called residual connections. They come from this paper the procedural learning for image recognition from about 2015 that introduced the concept. Now these are basically what it means is you transform the data, but then you have a skip connection with addition from the previous features. Now the way I like to visualize it that I prefer is the following. Here the computation happens from the top to bottom and basically you have this residual pathway and you are free to fork off from the residual pathway, perform some computation and then project back to the residual pathway via addition. And so you go from the the inputs to the targets only via plus and plus and plus. And the reason this is useful is because during dot propagation remember from our micrograd video earlier addition distributes gradients equally to both of its branches that fed as the input. And so the supervision or the gradients from the loss basically hop through every addition node all the way to the input and then also fork off into the residual blocks. But basically you have this gradient superhighway that goes directly from the supervision all the way to the input unimpeded. And then these residual blocks are usually initialized in the beginning. So they contribute very very little if anything to the residual pathway. They are initialized that way. So in the beginning they are sort of almost kind of like not there. But then during the optimization they come online over time and they start to contribute but at least at the initialization you can go from directly supervision to the input gradient is unimpeded and just flows. And then the blocks over time kick in. And so that dramatically helps with the optimization. So let's implement this. So coming back to our block here. Basically what we want to do is we want to do x equals x plus self-attention and x equals x plus self.feedforward. So this is x and then we fork off and do some communication and come back and we fork off and we do some computation and come back. So those are residual connections and then swinging back up here. We also have to introduce this projection. So nn.linear and this is going to be from after we concatenate this. This is the size and embed. So this is the output of the self-attention itself. But then we actually want the to apply the projection and that's the result. So the projection is just a linear transformation of the outcome of this layer. So that's the projection back into the residual pathway. And then here in a feedforward, it's going to be the same thing. I could have a self.projection here as well. But let me just simplify it and let me couple it inside the same sequential container. And so this is the projection layer going back into the residual pathway. And so that's well, that's it. So now we can train this. So I implemented one more small change. When you look into the paper again, you see that the dimensionality of input and output is 512 for them. And they're saying that the inner layer here in the feedforward has dimensionality of 2048. So there's a multiplier of 4. And so the inner layer of the feedforward network should be multiplied by 4 in terms of channel sizes. So I came here and I multiplied 4 times embed here for the feedforward and then from 4 times an embed coming back down to an embed when we go back to the projection. So adding a bit of computation here and growing that layer that is in the residual block on the side of the residual pathway. And then I train this and we actually get down all the way to 2.08 validation loss. And we also see that network is starting to get big enough that our train loss is getting ahead of validation loss. So we started to see like a little bit of overfitting and our our generations here are still not amazing. But at least you see that we can see like is here this now grief sync like this starts to almost look like English. So yeah, we're starting to really get there. Okay. And the second innovation that is very helpful for optimizing very deep neural networks is right here. So we have this addition now that's the residual part. But this norm is referring to something called layer norm. So layer norm is implemented in pytorch. It's a paper that came out a while back here. And layer norm is very very similar to bash norm. So remember back to our make more series part three. We implemented bash normalization and bash normalization basically just made sure that across the batch dimension. Any individual neuron had unit Gaussian distribution. So it was zero mean and unit standard deviation one standard deviation output. So what I did here is I'm copy pasting the bathroom 1D that we developed in our make more series and see here we can initialize for example this module and we can have a batch of 32 100 dimensional vectors feeding through the bathroom layer. So what this does is it guarantees that when we look at just the 0th column, it's a zero mean one standard deviation. So it's normalizing every single column of this input. Now the rows are not going to be normalized by default because we're just normalizing columns. So let's not implement layer norm. It's very complicated. Look we come here. We change this from 0 to 1 so we don't normalize the columns. We normalize. The rows and now we've implemented layer norm. So now the columns are not going to be normalized. But the rows are going to be normalized for every individual example. It's 100 dimensional vector is normalized in this way and because our computation now does not span across examples, we can delete all of this buffers stuff because we can always apply this operation and don't need to maintain any running buffers. So we don't need the buffers. We don't there's no distinction between training and test time. And we don't need these running buffers. We do keep gamma and beta. We don't need the momentum. We don't care if it's training or not. And this is now a layer norm and it normalizes the ropes instead of the columns and this here is identical to basically this here. So let's. Now implement layer norm in our transformer before I incorporate the layer norm. I just wanted to note that as I said very few details about the transformer have changed in the last five years, but this is actually something that's likely departs from the original paper. You see that the ad and norm is applied after the transformation. But now it is a bit more basically common to apply the layer norm before the transformation. So there's a reshuffling of the layer norms. So this is called the pre norm formulation and that the one that we're going to implement as well. So slight deviation from the original paper. Basically, we need to layer norms layer norm. One is an end dot layer norm and we tell it how many was the embedding dimension and we need the second layer norm. And then here the layer norms are applied immediately on X. So self-taught layer norm one in applied on X and self-taught layer norm two applied on X before it goes into self-attention and feed forward and the size of the layer norm here is an embed so 32. So when the layer norm is normalizing our features it is the normalization here happens the mean and the variance are taken over 32 numbers. So the batch and the time act as batch dimensions both of them. So this is kind of like a per token transformation that just normalizes the features and makes them a unit mean unit Gaussian at initialization. But of course because these layer norms inside it have these gamma and beta trainable parameters the layer normal eventually create outputs that might not be unit Gaussian but the optimization will determine that so for now, this is the this is incorporating the layer norms and let's train them up. Okay, so I let it run and we see that we get down to 2.06 which is better than the previous 2.08. So a slight improvement by adding the layer norms and I'd expect that they help. Even more if we have bigger and deeper network. One more thing. I forgot to add is that there should be a layer norm here. Also typically as at the end of the transformer and right before the final linear layer that decodes into vocabulary. So I added that as well. So at this stage, we actually have a pretty complete transformer coming to the original paper and it's a decoder only transformer. I'll I'll talk about that in a second but at this stage the major pieces are in place so we can try to scale this up and see how well we can push this number now in order to scale up the model. I had to perform some cosmetic changes here to make it nicer. So I introduced this variable called in layer which just specifies how many layers of the blocks. We're going to have I create a bunch of blocks and we have a new variable number of heads as well. I pulled out the layer norm here. And so this is identical. Now one thing that I did briefly change is I added dropout. So dropout is something that you can add right before the residual connection. Back right before the connection back into the residual pathway. So we can drop out that as the last layer here. We can drop out here at the end of the multi-headed extension as well. And we can also drop out here when we calculate the basically affinities and after the softmax we can drop out some of those so we can randomly prevent some of the notes from communicating. And so dropout comes from this paper from 2014 or so. And basically it takes your neural net and it randomly every forward backward pass shuts off some subset of neurons so randomly drops them to zero and trains without them and what this does effectively is because the mask of what being dropped out has changed every single forward backward pass it ends up kind of training an ensemble of sub networks and then at test time everything is fully enabled and kind of all those sub networks. Are merged into a single ensemble. If you can if you want to think about it that way. So I would read the paper to get the full detail for now. We're just going to stay on the level of this is a regularization technique and I added it because I'm about to scale up the model quite a bit and I was concerned about overfitting. So now when we scroll up to the top we'll see that I changed a number of hyper parameters here about our neural net. So I made the batch size be much larger now 64. I changed the block size to be 256. So previously was just eight. Eight characters of context. Now it is 256 characters of context to predict the 257th. I brought down the learning rate a little bit because the neural net is now much bigger. So I brought down the learning rate. The embedding dimension is not 384 and there are six heads. So 384 divide 6 means that every head is 64 dimensional as it as a standard and then there was going to be six layers of that and the dropout will be a point to so every forward backward pass. 20% of all these intermediate calculations are disabled and dropped to zero and then I already trained this and I ran it. So drumroll how does it perform? So let me just scroll up here. We get a validation loss of 1.48 which is actually quite a bit of an improvement on what we had before which I think was 2.07. So we went from 2.07 all the way down to 1.48 just by scaling up this neural net with the code that we have. And this of course ran for a lot longer. This may be trained for I want to say about 15 minutes on my A100 GPU. So that's a pretty good GPU and if you don't have a GPU you're not going to be able to reproduce this on a CPU. This would be I would not run this on the CPU or MacBook or something like that. You'll have to break down the number of layers and the embedding dimension and so on. But in about 15 minutes we can get this kind of a result and I'm printing some of the Shakespeare here. But what I did also is I printed 10,000 characters. So a lot more and I wrote them to a file. And so here we see some of the outputs. So it's a lot more recognizable as the input text file. So the input text file just for reference look like this. So there's always like someone speaking in this matter and our predictions now take on that form except of course they're nonsensical when you actually read them. So it is every crimp to be a house. Oh those probation we give heed. You know. Oh ho sent me you mighty Lord. Anyway, so you can read through this. It's nonsensical of course, but this is just a transformer trained on the character level for 1 million characters that come from Shakespeare. So there's sort of like blabbers on in Shakespeare like math. Banner, but it doesn't of course make sense at this scale. But I think I think still a pretty good demonstration of what's possible. So now I think that kind of like concludes the programming section of this video. We basically kind of did a pretty good job and of implementing this transformer, but the picture doesn't exactly match up to what we've done. So what's going on with all these additional parts here? So let me finish explaining this architecture and why it looks so funky. Basically, what's happening here is what we implemented here is a decoder only transformer. So there's no component here. This part is called the encoder and there's no cross attention block here. Our block only has a self attention and the feed forward. So it is missing this third in between piece here. This piece does cross attention. So we don't have it and we don't have the encoder. We just have the decoder and the reason we have a decoder only is because we are just generating text. And it's unconditioned on anything or just we're just blabbering on according to a given data set. What makes it a decoder is that we are using the triangular mask in our transformer. So it has this autoregressive property where we can just go and sample from it. So the fact that it's using the triangulate triangular mask to mask out the attention makes it a decoder and it can be used for language modeling. Now, the reason that the original paper had an encoder decoder architecture is because it is a machine translation paper. So it is concerned with a different setting in particular. It expects some tokens that encode say for example French and then it is expected to decode the translation in English. So so you typically these here are special tokens. So you are expected to read in this and condition on it and then you start off the generation with a special token called start. So this is a special new token that you introduce and always place in the beginning and then the. Network is expected to output neural networks are awesome and then a special end token to finish the generation. So this part here will be decoded exactly as we have we've done it neural networks are awesome will be identical to what we did but unlike what we did they want to condition the generation on some additional information. And in that case this additional information is the French sentence that they should be translating. So what they do now is they. Bring the encoder now the encoder reads this part here. So we're all going to take the part of French and we're going to create tokens from it exactly as we've seen in our video and we're going to put a transformer on it, but there's going to be no triangular mask. And so all the tokens are allowed to talk to each other as much as they want and they're just encoding whatever the content of this French sentence once they've encoded it they've they basically come out in the top here. And then what happens here is in our decoder which does the language modeling. There's an additional connection here to the outputs of the encoder and that is brought in through a cross attention. So the queries are still generated from X but now the keys and the values are coming from the side the keys and the values are coming from the top generated by the nodes that came outside of the decode the encoder and those tops the keys and the values there the top of it. Feed in on the side into every single block of the decoder and so that's why there's an additional cross attention and really what is doing is it's conditioning the decoding not just on the past of this current decoding but also on having seen the full fully encoded French prompt sort of and so it's an encoder decoder model, which is why we have those two transformers and additional block and so on. So we did not do this because we have no we have nothing to do. Nothing to encode. There's no conditioning. We just have a text file and we just want to imitate it and that's why we are using a decoder only transformer exactly as done in GPT. Okay. So now I wanted to do a very brief walkthrough of nano GPT, which you can find in my GitHub and nano GPT is basically two files of interest. There's train.pi and model.pi train.pi is all the boilerplate code for training the network. It is basically all the stuff that we had here is the training loop. It's just that. It's a lot more complicated because we're saving and loading checkpoints and pre-trained weights and we are decaying the learning rate and compiling the model and using distributed training across multiple nodes or GPUs. So the training that Pi gets a little bit more hairy, complicated. There's more options Etc, but the model that I should look very very similar to what we've done here. In fact, the model is almost identical. So first here we have the causal self-attention block and all of this should look very very very similar. recognizable to you we're producing queries keys values we're doing dot products we're masking applying softmax optionally dropping out and here we are pooling the values what is different here is that in our code i have separated out the multi-headed attention into just a single individual head and then here i have multiple heads and i explicitly concatenate them whereas here all of it is implemented in a batched manner inside a single causal self-attention and so we don't just have a b and a t and a c dimension we also end up with a fourth dimension which is the heads and so it just gets a lot more sort of hairy because we have four-dimensional array tensors now but it is equivalent mathematically so the exact same thing is happening as what we have it's just it's a bit more efficient because all the heads are now treated as a batch dimension as well then we have the multi-layered perceptron and we have the multi-layered perceptron and we have the multi-layered perceptron it's using the gelu non-linearity which is defined here except instead of relu and this is done just because openly i used it and i want to be able to load their checkpoints the blocks of the transformer are identical the communicate and the compute phase as we saw and then the gpt will be identical we have the position encodings token encodings the blocks the layer norm at the end the final linear layer and this should look all very recognizable and there's a bit more here because i'm loading checkpoints and stuff like that i'm separating out the parameters into those that should be weight decayed and those that shouldn't but the generate function should also be very very similar so a few details are different but you should definitely be able to look at this file and be able to understand a lot of the pieces now so let's now bring things back to chat gpt what would it look like if we wanted to train chat gpt ourselves and how does it relate to what we learned today well to train the chat gpt there are roughly two stages first is the pre-training stage and then the fine-tuning stage and then the pre-training stage in the pre-training stage we are training on a large chunk of internet and just trying to get a first decoder only transformer to babble text so it's very very similar to what we've done ourselves except we've done like a tiny little baby pre-training step and so in our case this is how you print a number of parameters i printed it and it's about 10 million so this transformer that i created here to create a little shakespeare transformer was about 10 million parameters our data set is roughly 1 million characters so roughly 1 million tokens but you have to remember that openai uses different vocabulary they're not on the character level they use these subword chunks of words and so they have a vocabulary of 50 000 roughly elements and so their sequences are a bit more condensed so our data set the shakespeare data set would be probably around 300 000 tokens in the openai vocabulary roughly so we trained about 10 million parameter model on roughly 300 000 tokens now when you go to the gpt3 paper and you look at the transformers that they trained they trained a number of transformers of different sizes but the biggest transformer here has 175 billion parameters uh so ours is again 10 million they used this number of layers in the transformer this is the n embed this is the number of heads and this is the head size and then this is the batch size so ours was 65 and the learning rate is similar now when they train this transformer they trained on 300 billion tokens so again remember ours is about 300,000 so this is about a million fold increase and this number would not be even that large by today's standards you'd be going up 1 trillion and above so they are training a significantly larger model on a good chunk of the internet and that is the pre-training stage but otherwise these hyperparameters should be fairly recognizable to you and the architecture is actually like nearly identical to what we implemented ourselves but of course it's a massive infrastructure challenge to train this you're talking about typically thousands of GPUs having to you know talk to each other to train models of this size so that's just a pre-training stage now after you complete the pre-training stage you don't get something that you don't get something that you don't get something that you don't get a response to your questions with answers and it's not helpful and etc you get a document completer right so it babbles but it doesn't babble Shakespeare it babbles internet it will create arbitrary news articles and documents and it will try to complete documents because that's what it's trained for it's trying to complete the sequence so when you give it a question it would just potentially just give you more questions it would follow with more questions it will do whatever it looks like the some closed document would do in the training data on the internet and so who knows you're getting kind of like undefined behavior it might basically answer with two questions with other questions it might ignore your question it might just try to complete some news article it's totally unaligned as we say so the second fine-tuning stage is to actually align it to be an assistant and this is the second stage and so this chat GPT blog post from opening I talks a little bit about how this stage is achieved we basically roughly three steps to it to this stage so what they do here is they start to collect training data that looks specifically like what an assistant would do so there are documents that have the format where the question is on top and then an answer is below and they have a large number of these but probably not on the order of the internet this is probably on the order of maybe thousands of examples and so they they then fine-tune the model to basically only focus on documents that look like that and so you're starting to slowly align it so it's going to expect a question at the top and it's going to expect to complete the answer and these very very large models are very sample efficient during their fine-tuning so this actually somehow works but that's just step one that's just fine-tuning so then they actually have more steps where okay the second step is you let the model respond and then different raters look at the different responses and rank them for their preferences to which one is better than the other they use that to train a reward model so they can predict a basically using a different network, how much of any candidate response would be desirable. And then once they have a reward model, they run PPO, which is a form of policy gradient reinforcement learning optimizer, to fine-tune this sampling policy so that the answers that the chat GPT now generates are expected to score a high reward according to the reward model. And so basically there's a whole aligning stage here, or fine-tuning stage. It's got multiple steps in between there as well, and it takes the model from being a document completer to a question answerer, and that's like a whole separate stage. A lot of this data is not available publicly. It is internal to OpenAI, and it's much harder to replicate this stage. And so that's roughly what would give you a chat GPT. And NanoGPT focuses on the pre-training stage. Okay. And that's everything that I wanted to cover today. So we trained, to summarize, a decoder-only transformer following this famous paper, Attention is All You Need, from 2017. And so that's basically a GPT. We trained it on tiny Shakespeare and got sensible results. All of the training code is roughly 200 lines of code. I will be releasing this code base. So also it comes with all the... Git log commits along the way, as we built it up. In addition to this code, I'm going to release the notebook, of course, the Google Colab. And I hope that gave you a sense for how you can train these models, like, say, GPT-3, that will be architecturally basically identical to what we have, but they are somewhere between 10,000 and 1 million times bigger, depending on how you count. And so that's all I have for now. We did not talk about any of the fine-tuning stages. That would, typically, go on top of this. So if you're interested in something that's not just language modeling, but you actually want to, you know, say, perform tasks, or you want them to be aligned in a specific way, or you want to detect sentiment or anything like that, basically, any time you don't want something that's just a document completer, you have to complete further stages of fine-tuning, which we did not cover. And that could be simple, supervised fine-tuning, or it can be something more fancy, like we see in ChatGPT, where we actually train a reward model, and then do rounds of PPO to... align it with respect to the reward model. So there's a lot more that can be done on top of it. I think for now, we're starting to get to about two hours, Mark. So I'm going to kind of finish here. I hope you enjoyed the lecture. And yeah, go forth and transform. See you later.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.36, "text": " Hi everyone. So by now you have probably heard of ChatGPT. It has taken the world and the AI", "tokens": [50365, 2421, 1518, 13, 407, 538, 586, 291, 362, 1391, 2198, 295, 27503, 38, 47, 51, 13, 467, 575, 2726, 264, 1002, 293, 264, 7318, 50633], "temperature": 0.0, "avg_logprob": -0.07458256906078707, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04631013795733452}, {"id": 1, "seek": 0, "start": 5.36, "end": 11.700000000000001, "text": " community by storm, and it is a system that allows you to interact with an AI and give it text-based", "tokens": [50633, 1768, 538, 7679, 11, 293, 309, 307, 257, 1185, 300, 4045, 291, 281, 4648, 365, 364, 7318, 293, 976, 309, 2487, 12, 6032, 50950], "temperature": 0.0, "avg_logprob": -0.07458256906078707, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04631013795733452}, {"id": 2, "seek": 0, "start": 11.700000000000001, "end": 17.080000000000002, "text": " tasks. So for example, we can ask ChatGPT to write us a small haiku about how important it is that", "tokens": [50950, 9608, 13, 407, 337, 1365, 11, 321, 393, 1029, 27503, 38, 47, 51, 281, 2464, 505, 257, 1359, 324, 24320, 466, 577, 1021, 309, 307, 300, 51219], "temperature": 0.0, "avg_logprob": -0.07458256906078707, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04631013795733452}, {"id": 3, "seek": 0, "start": 17.080000000000002, "end": 20.72, "text": " people understand AI, and then they can use it to improve the world and make it more prosperous.", "tokens": [51219, 561, 1223, 7318, 11, 293, 550, 436, 393, 764, 309, 281, 3470, 264, 1002, 293, 652, 309, 544, 38928, 13, 51401], "temperature": 0.0, "avg_logprob": -0.07458256906078707, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04631013795733452}, {"id": 4, "seek": 0, "start": 21.42, "end": 27.080000000000002, "text": " So when we run this, AI knowledge brings prosperity for all to see, embrace its power.", "tokens": [51436, 407, 562, 321, 1190, 341, 11, 7318, 3601, 5607, 22434, 337, 439, 281, 536, 11, 14038, 1080, 1347, 13, 51719], "temperature": 0.0, "avg_logprob": -0.07458256906078707, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04631013795733452}, {"id": 5, "seek": 2708, "start": 27.08, "end": 33.3, "text": " Okay, not bad. And so you could see that ChatGPT went from left to right and generated all these", "tokens": [50365, 1033, 11, 406, 1578, 13, 400, 370, 291, 727, 536, 300, 27503, 38, 47, 51, 1437, 490, 1411, 281, 558, 293, 10833, 439, 613, 50676], "temperature": 0.0, "avg_logprob": -0.07858715057373047, "compression_ratio": 1.6791808873720135, "no_speech_prob": 4.2052204662468284e-05}, {"id": 6, "seek": 2708, "start": 33.3, "end": 39.099999999999994, "text": " words sort of sequentially. Now, I asked it already the exact same prompt a little bit earlier,", "tokens": [50676, 2283, 1333, 295, 5123, 3137, 13, 823, 11, 286, 2351, 309, 1217, 264, 1900, 912, 12391, 257, 707, 857, 3071, 11, 50966], "temperature": 0.0, "avg_logprob": -0.07858715057373047, "compression_ratio": 1.6791808873720135, "no_speech_prob": 4.2052204662468284e-05}, {"id": 7, "seek": 2708, "start": 39.3, "end": 44.66, "text": " and it generated a slightly different outcome. AI's power to grow, ignorance holds us back,", "tokens": [50976, 293, 309, 10833, 257, 4748, 819, 9700, 13, 7318, 311, 1347, 281, 1852, 11, 25390, 9190, 505, 646, 11, 51244], "temperature": 0.0, "avg_logprob": -0.07858715057373047, "compression_ratio": 1.6791808873720135, "no_speech_prob": 4.2052204662468284e-05}, {"id": 8, "seek": 2708, "start": 44.92, "end": 50.82, "text": " learn, prosperity waits. So pretty good in both cases and slightly different. So you can see that", "tokens": [51257, 1466, 11, 22434, 40597, 13, 407, 1238, 665, 294, 1293, 3331, 293, 4748, 819, 13, 407, 291, 393, 536, 300, 51552], "temperature": 0.0, "avg_logprob": -0.07858715057373047, "compression_ratio": 1.6791808873720135, "no_speech_prob": 4.2052204662468284e-05}, {"id": 9, "seek": 2708, "start": 50.82, "end": 55.519999999999996, "text": " ChatGPT is a probabilistic system, and for any one prompt, it can give us multiple answers,", "tokens": [51552, 27503, 38, 47, 51, 307, 257, 31959, 3142, 1185, 11, 293, 337, 604, 472, 12391, 11, 309, 393, 976, 505, 3866, 6338, 11, 51787], "temperature": 0.0, "avg_logprob": -0.07858715057373047, "compression_ratio": 1.6791808873720135, "no_speech_prob": 4.2052204662468284e-05}, {"id": 10, "seek": 2708, "start": 55.66, "end": 57.019999999999996, "text": " sort of replying.", "tokens": [51794, 1333, 295, 1085, 7310, 13, 51862], "temperature": 0.0, "avg_logprob": -0.07858715057373047, "compression_ratio": 1.6791808873720135, "no_speech_prob": 4.2052204662468284e-05}, {"id": 11, "seek": 5708, "start": 57.08, "end": 62.06, "text": " Now, this is just one example of a prompt. People have come up with many, many examples,", "tokens": [50365, 823, 11, 341, 307, 445, 472, 1365, 295, 257, 12391, 13, 3432, 362, 808, 493, 365, 867, 11, 867, 5110, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12641911713973336, "compression_ratio": 1.6145454545454545, "no_speech_prob": 9.19694357435219e-05}, {"id": 12, "seek": 5708, "start": 62.519999999999996, "end": 68.22, "text": " and there are entire websites that index interactions with ChatGPT. And so many of", "tokens": [50637, 293, 456, 366, 2302, 12891, 300, 8186, 13280, 365, 27503, 38, 47, 51, 13, 400, 370, 867, 295, 50922], "temperature": 0.0, "avg_logprob": -0.12641911713973336, "compression_ratio": 1.6145454545454545, "no_speech_prob": 9.19694357435219e-05}, {"id": 13, "seek": 5708, "start": 68.22, "end": 73.64, "text": " them are quite humorous. Explain HTML to me like I'm a dog, write release notes for chess too,", "tokens": [50922, 552, 366, 1596, 14318, 563, 13, 39574, 17995, 281, 385, 411, 286, 478, 257, 3000, 11, 2464, 4374, 5570, 337, 24122, 886, 11, 51193], "temperature": 0.0, "avg_logprob": -0.12641911713973336, "compression_ratio": 1.6145454545454545, "no_speech_prob": 9.19694357435219e-05}, {"id": 14, "seek": 5708, "start": 74.58, "end": 79.9, "text": " write a note about Elon Musk buying a Twitter, and so on. So as an example,", "tokens": [51240, 2464, 257, 3637, 466, 28498, 26019, 6382, 257, 5794, 11, 293, 370, 322, 13, 407, 382, 364, 1365, 11, 51506], "temperature": 0.0, "avg_logprob": -0.12641911713973336, "compression_ratio": 1.6145454545454545, "no_speech_prob": 9.19694357435219e-05}, {"id": 15, "seek": 5708, "start": 80.46, "end": 83.18, "text": " please write a breaking news article about a leaf falling from a tree,", "tokens": [51534, 1767, 2464, 257, 7697, 2583, 7222, 466, 257, 10871, 7440, 490, 257, 4230, 11, 51670], "temperature": 0.0, "avg_logprob": -0.12641911713973336, "compression_ratio": 1.6145454545454545, "no_speech_prob": 9.19694357435219e-05}, {"id": 16, "seek": 5708, "start": 83.18, "end": 86.32, "text": " and a shocking turn of events.", "tokens": [51670, 293, 257, 18776, 1261, 295, 3931, 13, 51827], "temperature": 0.0, "avg_logprob": -0.12641911713973336, "compression_ratio": 1.6145454545454545, "no_speech_prob": 9.19694357435219e-05}, {"id": 17, "seek": 8708, "start": 87.08, "end": 90.34, "text": " The leaf falling from a tree in the local park. Witnesses report that the leaf, which was", "tokens": [50365, 440, 10871, 7440, 490, 257, 4230, 294, 264, 2654, 3884, 13, 41366, 279, 2275, 300, 264, 10871, 11, 597, 390, 50528], "temperature": 0.0, "avg_logprob": -0.12969545156014065, "compression_ratio": 1.6963696369636965, "no_speech_prob": 0.03412614017724991}, {"id": 18, "seek": 8708, "start": 90.34, "end": 95.03999999999999, "text": " previously attached to a branch of a tree, detached itself and fell to the ground. Very", "tokens": [50528, 8046, 8570, 281, 257, 9819, 295, 257, 4230, 11, 42050, 2564, 293, 5696, 281, 264, 2727, 13, 4372, 50763], "temperature": 0.0, "avg_logprob": -0.12969545156014065, "compression_ratio": 1.6963696369636965, "no_speech_prob": 0.03412614017724991}, {"id": 19, "seek": 8708, "start": 95.03999999999999, "end": 100.0, "text": " dramatic. So you can see that this is a pretty remarkable system, and it is what we call a", "tokens": [50763, 12023, 13, 407, 291, 393, 536, 300, 341, 307, 257, 1238, 12802, 1185, 11, 293, 309, 307, 437, 321, 818, 257, 51011], "temperature": 0.0, "avg_logprob": -0.12969545156014065, "compression_ratio": 1.6963696369636965, "no_speech_prob": 0.03412614017724991}, {"id": 20, "seek": 8708, "start": 100.0, "end": 107.56, "text": " language model, because it models the sequence of words or characters or tokens more generally,", "tokens": [51011, 2856, 2316, 11, 570, 309, 5245, 264, 8310, 295, 2283, 420, 4342, 420, 22667, 544, 5101, 11, 51389], "temperature": 0.0, "avg_logprob": -0.12969545156014065, "compression_ratio": 1.6963696369636965, "no_speech_prob": 0.03412614017724991}, {"id": 21, "seek": 8708, "start": 107.98, "end": 113.25999999999999, "text": " and it knows how certain words follow each other in English language. And so from its perspective,", "tokens": [51410, 293, 309, 3255, 577, 1629, 2283, 1524, 1184, 661, 294, 3669, 2856, 13, 400, 370, 490, 1080, 4585, 11, 51674], "temperature": 0.0, "avg_logprob": -0.12969545156014065, "compression_ratio": 1.6963696369636965, "no_speech_prob": 0.03412614017724991}, {"id": 22, "seek": 8708, "start": 113.25999999999999, "end": 116.52, "text": " what it is doing is it is completing the sequence.", "tokens": [51674, 437, 309, 307, 884, 307, 309, 307, 19472, 264, 8310, 13, 51837], "temperature": 0.0, "avg_logprob": -0.12969545156014065, "compression_ratio": 1.6963696369636965, "no_speech_prob": 0.03412614017724991}, {"id": 23, "seek": 11708, "start": 117.08, "end": 122.03999999999999, "text": " So I give it the start of a sequence, and it completes the sequence with the outcome.", "tokens": [50365, 407, 286, 976, 309, 264, 722, 295, 257, 8310, 11, 293, 309, 36362, 264, 8310, 365, 264, 9700, 13, 50613], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 24, "seek": 11708, "start": 122.03999999999999, "end": 127.4, "text": " And so it's a language model in that sense. Now, I would like to focus on the under the hood of", "tokens": [50613, 400, 370, 309, 311, 257, 2856, 2316, 294, 300, 2020, 13, 823, 11, 286, 576, 411, 281, 1879, 322, 264, 833, 264, 13376, 295, 50881], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 25, "seek": 11708, "start": 128.35999999999999, "end": 132.92, "text": " under the hood components of what makes ChatGPT work. So what is the neural network under the", "tokens": [50929, 833, 264, 13376, 6677, 295, 437, 1669, 27503, 38, 47, 51, 589, 13, 407, 437, 307, 264, 18161, 3209, 833, 264, 51157], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 26, "seek": 11708, "start": 132.92, "end": 139.07999999999998, "text": " hood that models the sequence of these words? And that comes from this paper called Attention", "tokens": [51157, 13376, 300, 5245, 264, 8310, 295, 613, 2283, 30, 400, 300, 1487, 490, 341, 3035, 1219, 31858, 51465], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 27, "seek": 11708, "start": 139.07999999999998, "end": 146.2, "text": " is All You Need. In 2017, a landmark paper, a landmark paper in AI that produced and proposed", "tokens": [51465, 307, 1057, 509, 16984, 13, 682, 6591, 11, 257, 26962, 3035, 11, 257, 26962, 3035, 294, 7318, 300, 7126, 293, 10348, 51821], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 28, "seek": 11708, "start": 146.2, "end": 146.9, "text": " the Transformer and the Translator. And this paper is called the Transformer and the Translator. And", "tokens": [51821, 264, 27938, 260, 293, 264, 6531, 75, 1639, 13, 400, 341, 3035, 307, 1219, 264, 27938, 260, 293, 264, 6531, 75, 1639, 13, 400, 51856], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 29, "seek": 11708, "start": 146.9, "end": 147.06, "text": " this paper is called the Transformer and the Translator. And this paper is called the Transformer", "tokens": [51856, 341, 3035, 307, 1219, 264, 27938, 260, 293, 264, 6531, 75, 1639, 13, 400, 341, 3035, 307, 1219, 264, 27938, 260, 51864], "temperature": 0.0, "avg_logprob": -0.27417277112419225, "compression_ratio": 2.2440677966101696, "no_speech_prob": 0.004163331352174282}, {"id": 30, "seek": 14708, "start": 147.08, "end": 154.28, "text": " Architecture. So GPT is short for generatively, generatively pre trained transformer. So", "tokens": [50365, 43049, 13, 407, 26039, 51, 307, 2099, 337, 1337, 19020, 11, 1337, 19020, 659, 8895, 31782, 13, 407, 50725], "temperature": 0.0, "avg_logprob": -0.07945935866411995, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.0232473686337471}, {"id": 31, "seek": 14708, "start": 154.28, "end": 158.36, "text": " transformer is the neural net that actually does all the heavy lifting under the hood.", "tokens": [50725, 31782, 307, 264, 18161, 2533, 300, 767, 775, 439, 264, 4676, 15798, 833, 264, 13376, 13, 50929], "temperature": 0.0, "avg_logprob": -0.07945935866411995, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.0232473686337471}, {"id": 32, "seek": 14708, "start": 158.36, "end": 164.44, "text": " It comes from this paper in 2017. Now, if you read this paper, this reads like a pretty random", "tokens": [50929, 467, 1487, 490, 341, 3035, 294, 6591, 13, 823, 11, 498, 291, 1401, 341, 3035, 11, 341, 15700, 411, 257, 1238, 4974, 51233], "temperature": 0.0, "avg_logprob": -0.07945935866411995, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.0232473686337471}, {"id": 33, "seek": 14708, "start": 164.44, "end": 168.12, "text": " machine translation paper. And that's because I think the authors didn't fully anticipate the", "tokens": [51233, 3479, 12853, 3035, 13, 400, 300, 311, 570, 286, 519, 264, 16552, 994, 380, 4498, 21685, 264, 51417], "temperature": 0.0, "avg_logprob": -0.07945935866411995, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.0232473686337471}, {"id": 34, "seek": 14708, "start": 168.12, "end": 172.68, "text": " impact that the transformer would have on the field. And this architecture that they produced", "tokens": [51417, 2712, 300, 264, 31782, 576, 362, 322, 264, 2519, 13, 400, 341, 9482, 300, 436, 7126, 51645], "temperature": 0.0, "avg_logprob": -0.07945935866411995, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.0232473686337471}, {"id": 35, "seek": 14708, "start": 172.68, "end": 176.9, "text": " in the context of machine translation, in their case, actually ended up taking over", "tokens": [51645, 294, 264, 4319, 295, 3479, 12853, 11, 294, 641, 1389, 11, 767, 4590, 493, 1940, 670, 51856], "temperature": 0.0, "avg_logprob": -0.07945935866411995, "compression_ratio": 1.794701986754967, "no_speech_prob": 0.0232473686337471}, {"id": 36, "seek": 17690, "start": 176.9, "end": 183.86, "text": " the rest of AI in the next five years after. And so this architecture with minor changes was copy", "tokens": [50365, 264, 1472, 295, 7318, 294, 264, 958, 1732, 924, 934, 13, 400, 370, 341, 9482, 365, 6696, 2962, 390, 5055, 50713], "temperature": 0.0, "avg_logprob": -0.10245831807454427, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0008813897147774696}, {"id": 37, "seek": 17690, "start": 183.86, "end": 191.06, "text": " pasted into a huge amount of applications in AI in more recent years. And that includes at the core", "tokens": [50713, 1791, 292, 666, 257, 2603, 2372, 295, 5821, 294, 7318, 294, 544, 5162, 924, 13, 400, 300, 5974, 412, 264, 4965, 51073], "temperature": 0.0, "avg_logprob": -0.10245831807454427, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0008813897147774696}, {"id": 38, "seek": 17690, "start": 191.06, "end": 196.34, "text": " of ChatGPT. Now, we are not going to, what I'd like to do now is I'd like to build out", "tokens": [51073, 295, 27503, 38, 47, 51, 13, 823, 11, 321, 366, 406, 516, 281, 11, 437, 286, 1116, 411, 281, 360, 586, 307, 286, 1116, 411, 281, 1322, 484, 51337], "temperature": 0.0, "avg_logprob": -0.10245831807454427, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0008813897147774696}, {"id": 39, "seek": 17690, "start": 196.34, "end": 201.22, "text": " something like ChatGPT. But we're not going to be able to, of course, reproduce ChatGPT.", "tokens": [51337, 746, 411, 27503, 38, 47, 51, 13, 583, 321, 434, 406, 516, 281, 312, 1075, 281, 11, 295, 1164, 11, 29501, 27503, 38, 47, 51, 13, 51581], "temperature": 0.0, "avg_logprob": -0.10245831807454427, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0008813897147774696}, {"id": 40, "seek": 17690, "start": 201.22, "end": 204.58, "text": " This is a very serious production grade system. It is trained on", "tokens": [51581, 639, 307, 257, 588, 3156, 4265, 7204, 1185, 13, 467, 307, 8895, 322, 51749], "temperature": 0.0, "avg_logprob": -0.10245831807454427, "compression_ratio": 1.6404494382022472, "no_speech_prob": 0.0008813897147774696}, {"id": 41, "seek": 20458, "start": 204.58, "end": 211.54000000000002, "text": " a good chunk of internet. And then there's a lot of pre training and fine tuning stages to it.", "tokens": [50365, 257, 665, 16635, 295, 4705, 13, 400, 550, 456, 311, 257, 688, 295, 659, 3097, 293, 2489, 15164, 10232, 281, 309, 13, 50713], "temperature": 0.0, "avg_logprob": -0.19200286865234376, "compression_ratio": 1.8025078369905956, "no_speech_prob": 0.0016373470425605774}, {"id": 42, "seek": 20458, "start": 211.54000000000002, "end": 217.38000000000002, "text": " And so it's very complicated. What I'd like to focus on is just to train a transformer based", "tokens": [50713, 400, 370, 309, 311, 588, 6179, 13, 708, 286, 1116, 411, 281, 1879, 322, 307, 445, 281, 3847, 257, 31782, 2361, 51005], "temperature": 0.0, "avg_logprob": -0.19200286865234376, "compression_ratio": 1.8025078369905956, "no_speech_prob": 0.0016373470425605774}, {"id": 43, "seek": 20458, "start": 217.38000000000002, "end": 222.82000000000002, "text": " language model. And in our case, it's going to be a character level language model. I still think", "tokens": [51005, 2856, 2316, 13, 400, 294, 527, 1389, 11, 309, 311, 516, 281, 312, 257, 2517, 1496, 2856, 2316, 13, 286, 920, 519, 51277], "temperature": 0.0, "avg_logprob": -0.19200286865234376, "compression_ratio": 1.8025078369905956, "no_speech_prob": 0.0016373470425605774}, {"id": 44, "seek": 20458, "start": 222.82000000000002, "end": 227.62, "text": " that is a very educational with respect to how these systems work. So I don't want to train on", "tokens": [51277, 300, 307, 257, 588, 10189, 365, 3104, 281, 577, 613, 3652, 589, 13, 407, 286, 500, 380, 528, 281, 3847, 322, 51517], "temperature": 0.0, "avg_logprob": -0.19200286865234376, "compression_ratio": 1.8025078369905956, "no_speech_prob": 0.0016373470425605774}, {"id": 45, "seek": 20458, "start": 227.62, "end": 232.74, "text": " the chunk of internet, we need a smaller data set. In this case, I propose that we work with", "tokens": [51517, 264, 16635, 295, 4705, 11, 321, 643, 257, 4356, 1412, 992, 13, 682, 341, 1389, 11, 286, 17421, 300, 321, 589, 365, 51773], "temperature": 0.0, "avg_logprob": -0.19200286865234376, "compression_ratio": 1.8025078369905956, "no_speech_prob": 0.0016373470425605774}, {"id": 46, "seek": 20458, "start": 232.74, "end": 234.56, "text": " my favorite toy data set. It's called ChatGPT. And I'm going to show you how that works. I'm going to", "tokens": [51773, 452, 2954, 12058, 1412, 992, 13, 467, 311, 1219, 27503, 38, 47, 51, 13, 400, 286, 478, 516, 281, 855, 291, 577, 300, 1985, 13, 286, 478, 516, 281, 51864], "temperature": 0.0, "avg_logprob": -0.19200286865234376, "compression_ratio": 1.8025078369905956, "no_speech_prob": 0.0016373470425605774}, {"id": 47, "seek": 23458, "start": 234.58, "end": 237.12, "text": " show you what it looks like. So first, I'm going to create a little tiny Shakespeare. And what it", "tokens": [50365, 855, 291, 437, 309, 1542, 411, 13, 407, 700, 11, 286, 478, 516, 281, 1884, 257, 707, 5870, 22825, 13, 400, 437, 309, 50492], "temperature": 0.6000000000000001, "avg_logprob": -0.3579172445527205, "compression_ratio": 1.9223300970873787, "no_speech_prob": 0.0011355211026966572}, {"id": 48, "seek": 23458, "start": 237.12, "end": 241.92000000000002, "text": " is is basically it's a concatenation of all of the works of Shakespeare in my understanding. And so", "tokens": [50492, 307, 307, 1936, 309, 311, 257, 1588, 7186, 399, 295, 439, 295, 264, 1985, 295, 22825, 294, 452, 3701, 13, 400, 370, 50732], "temperature": 0.6000000000000001, "avg_logprob": -0.3579172445527205, "compression_ratio": 1.9223300970873787, "no_speech_prob": 0.0011355211026966572}, {"id": 49, "seek": 23458, "start": 241.92000000000002, "end": 247.96, "text": " this is all of Shakespeare in a single file. This file is about one megabyte. And it's just all of", "tokens": [50732, 341, 307, 439, 295, 22825, 294, 257, 2167, 3991, 13, 639, 3991, 307, 466, 472, 10816, 34529, 13, 400, 309, 311, 445, 439, 295, 51034], "temperature": 0.6000000000000001, "avg_logprob": -0.3579172445527205, "compression_ratio": 1.9223300970873787, "no_speech_prob": 0.0011355211026966572}, {"id": 50, "seek": 23458, "start": 247.96, "end": 253.92000000000002, "text": " Shakespeare. And what we are going to do now is we're going to basically model how these characters", "tokens": [51034, 22825, 13, 400, 437, 321, 366, 516, 281, 360, 586, 307, 321, 434, 516, 281, 1936, 2316, 577, 613, 4342, 51332], "temperature": 0.6000000000000001, "avg_logprob": -0.3579172445527205, "compression_ratio": 1.9223300970873787, "no_speech_prob": 0.0011355211026966572}, {"id": 51, "seek": 23458, "start": 253.92000000000002, "end": 260.44, "text": " follow each other. So for example, given a chunk of these characters like this, given some context", "tokens": [51332, 1524, 1184, 661, 13, 407, 337, 1365, 11, 2212, 257, 16635, 295, 613, 4342, 411, 341, 11, 2212, 512, 4319, 51658], "temperature": 0.6000000000000001, "avg_logprob": -0.3579172445527205, "compression_ratio": 1.9223300970873787, "no_speech_prob": 0.0011355211026966572}, {"id": 52, "seek": 23458, "start": 260.44, "end": 264.56, "text": " of characters in the past, the transformer neural network will look at the model of the character.", "tokens": [51658, 295, 4342, 294, 264, 1791, 11, 264, 31782, 18161, 3209, 486, 574, 412, 264, 2316, 295, 264, 2517, 13, 51864], "temperature": 0.6000000000000001, "avg_logprob": -0.3579172445527205, "compression_ratio": 1.9223300970873787, "no_speech_prob": 0.0011355211026966572}, {"id": 53, "seek": 26458, "start": 264.58, "end": 268.9, "text": " characters that i've highlighted and it's going to predict that g is likely to come next in the", "tokens": [50365, 4342, 300, 741, 600, 17173, 293, 309, 311, 516, 281, 6069, 300, 290, 307, 3700, 281, 808, 958, 294, 264, 50581], "temperature": 0.0, "avg_logprob": -0.0642705140290437, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.10790791362524033}, {"id": 54, "seek": 26458, "start": 268.9, "end": 274.02, "text": " sequence and it's going to do that because we're going to train that transformer on shakespeare", "tokens": [50581, 8310, 293, 309, 311, 516, 281, 360, 300, 570, 321, 434, 516, 281, 3847, 300, 31782, 322, 37891, 22446, 50837], "temperature": 0.0, "avg_logprob": -0.0642705140290437, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.10790791362524033}, {"id": 55, "seek": 26458, "start": 274.02, "end": 280.18, "text": " and it's just going to try to produce character sequences that look like this and in that process", "tokens": [50837, 293, 309, 311, 445, 516, 281, 853, 281, 5258, 2517, 22978, 300, 574, 411, 341, 293, 294, 300, 1399, 51145], "temperature": 0.0, "avg_logprob": -0.0642705140290437, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.10790791362524033}, {"id": 56, "seek": 26458, "start": 280.18, "end": 285.21999999999997, "text": " is going to model all the patterns inside this data so once we've trained the system i just", "tokens": [51145, 307, 516, 281, 2316, 439, 264, 8294, 1854, 341, 1412, 370, 1564, 321, 600, 8895, 264, 1185, 741, 445, 51397], "temperature": 0.0, "avg_logprob": -0.0642705140290437, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.10790791362524033}, {"id": 57, "seek": 26458, "start": 285.21999999999997, "end": 290.9, "text": " like to give you a preview we can generate infinite shakespeare and of course it's a fake", "tokens": [51397, 411, 281, 976, 291, 257, 14281, 321, 393, 8460, 13785, 37891, 22446, 293, 295, 1164, 309, 311, 257, 7592, 51681], "temperature": 0.0, "avg_logprob": -0.0642705140290437, "compression_ratio": 1.9224489795918367, "no_speech_prob": 0.10790791362524033}, {"id": 58, "seek": 29090, "start": 290.9, "end": 299.29999999999995, "text": " thing that looks kind of like shakespeare um apologies for there's some jank that i'm not", "tokens": [50365, 551, 300, 1542, 733, 295, 411, 37891, 22446, 1105, 34929, 337, 456, 311, 512, 361, 657, 300, 741, 478, 406, 50785], "temperature": 0.0, "avg_logprob": -0.05328947774479898, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.66129485075362e-05}, {"id": 59, "seek": 29090, "start": 299.29999999999995, "end": 306.82, "text": " able to resolve in in here but um you can see how this is going character by character and it's kind", "tokens": [50785, 1075, 281, 14151, 294, 294, 510, 457, 1105, 291, 393, 536, 577, 341, 307, 516, 2517, 538, 2517, 293, 309, 311, 733, 51161], "temperature": 0.0, "avg_logprob": -0.05328947774479898, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.66129485075362e-05}, {"id": 60, "seek": 29090, "start": 306.82, "end": 314.02, "text": " of like predicting shakespeare-like language so verily my lord the sights have left the again", "tokens": [51161, 295, 411, 32884, 37891, 22446, 12, 4092, 2856, 370, 1306, 953, 452, 15448, 264, 29363, 362, 1411, 264, 797, 51521], "temperature": 0.0, "avg_logprob": -0.05328947774479898, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.66129485075362e-05}, {"id": 61, "seek": 29090, "start": 314.02, "end": 320.5, "text": " the king coming with my curses with precious pale and then tronio says something else etc", "tokens": [51521, 264, 4867, 1348, 365, 452, 1262, 6196, 365, 12406, 19546, 293, 550, 504, 266, 1004, 1619, 746, 1646, 5183, 51845], "temperature": 0.0, "avg_logprob": -0.05328947774479898, "compression_ratio": 1.7077625570776256, "no_speech_prob": 8.66129485075362e-05}, {"id": 62, "seek": 32090, "start": 320.9, "end": 325.38, "text": " and this is just coming out of the transformer in a very similar manner as it would come out in", "tokens": [50365, 293, 341, 307, 445, 1348, 484, 295, 264, 31782, 294, 257, 588, 2531, 9060, 382, 309, 576, 808, 484, 294, 50589], "temperature": 0.0, "avg_logprob": -0.058555885753800385, "compression_ratio": 1.875, "no_speech_prob": 0.00024133459373842925}, {"id": 63, "seek": 32090, "start": 325.38, "end": 332.17999999999995, "text": " chat gpt in our case character by character in chat gpt it's coming out on the token by token", "tokens": [50589, 5081, 290, 662, 294, 527, 1389, 2517, 538, 2517, 294, 5081, 290, 662, 309, 311, 1348, 484, 322, 264, 14862, 538, 14862, 50929], "temperature": 0.0, "avg_logprob": -0.058555885753800385, "compression_ratio": 1.875, "no_speech_prob": 0.00024133459373842925}, {"id": 64, "seek": 32090, "start": 332.17999999999995, "end": 336.82, "text": " level and tokens are these sort of like little subword pieces so they're not word level they're", "tokens": [50929, 1496, 293, 22667, 366, 613, 1333, 295, 411, 707, 1422, 7462, 3755, 370, 436, 434, 406, 1349, 1496, 436, 434, 51161], "temperature": 0.0, "avg_logprob": -0.058555885753800385, "compression_ratio": 1.875, "no_speech_prob": 0.00024133459373842925}, {"id": 65, "seek": 32090, "start": 336.82, "end": 344.82, "text": " kind of like word chunk level um and now i've already written this entire code uh to train", "tokens": [51161, 733, 295, 411, 1349, 16635, 1496, 1105, 293, 586, 741, 600, 1217, 3720, 341, 2302, 3089, 2232, 281, 3847, 51561], "temperature": 0.0, "avg_logprob": -0.058555885753800385, "compression_ratio": 1.875, "no_speech_prob": 0.00024133459373842925}, {"id": 66, "seek": 32090, "start": 344.82, "end": 350.58, "text": " these transformers um and it is in a github repository that you can find and it's called", "tokens": [51561, 613, 4088, 433, 1105, 293, 309, 307, 294, 257, 290, 355, 836, 25841, 300, 291, 393, 915, 293, 309, 311, 1219, 51849], "temperature": 0.0, "avg_logprob": -0.058555885753800385, "compression_ratio": 1.875, "no_speech_prob": 0.00024133459373842925}, {"id": 67, "seek": 35090, "start": 350.9, "end": 357.38, "text": " nano gpt so nano gpt is a repository that you can find on my github and it's a repository for", "tokens": [50365, 30129, 290, 662, 370, 30129, 290, 662, 307, 257, 25841, 300, 291, 393, 915, 322, 452, 290, 355, 836, 293, 309, 311, 257, 25841, 337, 50689], "temperature": 0.0, "avg_logprob": -0.045052561552628226, "compression_ratio": 1.924, "no_speech_prob": 0.0006892073433846235}, {"id": 68, "seek": 35090, "start": 357.38, "end": 362.9, "text": " training transformers um on any given text and what i think is interesting about it because", "tokens": [50689, 3097, 4088, 433, 1105, 322, 604, 2212, 2487, 293, 437, 741, 519, 307, 1880, 466, 309, 570, 50965], "temperature": 0.0, "avg_logprob": -0.045052561552628226, "compression_ratio": 1.924, "no_speech_prob": 0.0006892073433846235}, {"id": 69, "seek": 35090, "start": 362.9, "end": 367.62, "text": " there's many ways to train transformers but this is a very simple implementation so it's just two", "tokens": [50965, 456, 311, 867, 2098, 281, 3847, 4088, 433, 457, 341, 307, 257, 588, 2199, 11420, 370, 309, 311, 445, 732, 51201], "temperature": 0.0, "avg_logprob": -0.045052561552628226, "compression_ratio": 1.924, "no_speech_prob": 0.0006892073433846235}, {"id": 70, "seek": 35090, "start": 367.62, "end": 374.26, "text": " files of 300 lines of code each one file defines the gpt model the transformer and one file trains", "tokens": [51201, 7098, 295, 6641, 3876, 295, 3089, 1184, 472, 3991, 23122, 264, 290, 662, 2316, 264, 31782, 293, 472, 3991, 16329, 51533], "temperature": 0.0, "avg_logprob": -0.045052561552628226, "compression_ratio": 1.924, "no_speech_prob": 0.0006892073433846235}, {"id": 71, "seek": 35090, "start": 374.26, "end": 379.62, "text": " it on some given text dataset and here i'm showing that if you train it on a open web text dataset", "tokens": [51533, 309, 322, 512, 2212, 2487, 28872, 293, 510, 741, 478, 4099, 300, 498, 291, 3847, 309, 322, 257, 1269, 3670, 2487, 28872, 51801], "temperature": 0.0, "avg_logprob": -0.045052561552628226, "compression_ratio": 1.924, "no_speech_prob": 0.0006892073433846235}, {"id": 72, "seek": 38090, "start": 380.9, "end": 390.17999999999995, "text": " web pages then i reproduce the the performance of gpt2 so gpt2 is an early version of openai's gpt", "tokens": [50365, 3670, 7183, 550, 741, 29501, 264, 264, 3389, 295, 290, 662, 17, 370, 290, 662, 17, 307, 364, 2440, 3037, 295, 1269, 1301, 311, 290, 662, 50829], "temperature": 0.0, "avg_logprob": -0.07594103489107298, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.00019833078840747476}, {"id": 73, "seek": 38090, "start": 391.14, "end": 396.73999999999995, "text": " from 2017 if i recall correctly and i've only so far reproduced the the smallest 124 million", "tokens": [50877, 490, 6591, 498, 741, 9901, 8944, 293, 741, 600, 787, 370, 1400, 11408, 1232, 264, 264, 16998, 2272, 19, 2459, 51157], "temperature": 0.0, "avg_logprob": -0.07594103489107298, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.00019833078840747476}, {"id": 74, "seek": 38090, "start": 396.73999999999995, "end": 401.53999999999996, "text": " parameter model but basically this is just proving that the code base is correctly arranged and i'm", "tokens": [51157, 13075, 2316, 457, 1936, 341, 307, 445, 27221, 300, 264, 3089, 3096, 307, 8944, 18721, 293, 741, 478, 51397], "temperature": 0.0, "avg_logprob": -0.07594103489107298, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.00019833078840747476}, {"id": 75, "seek": 38090, "start": 401.53999999999996, "end": 408.41999999999996, "text": " able to load the neural network weights that openai has released later so you can take a look", "tokens": [51397, 1075, 281, 3677, 264, 18161, 3209, 17443, 300, 1269, 1301, 575, 4736, 1780, 370, 291, 393, 747, 257, 574, 51741], "temperature": 0.0, "avg_logprob": -0.07594103489107298, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.00019833078840747476}, {"id": 76, "seek": 38090, "start": 408.41999999999996, "end": 410.41999999999996, "text": " at the finished code here in nano gpt", "tokens": [51741, 412, 264, 4335, 3089, 510, 294, 30129, 290, 662, 51841], "temperature": 0.0, "avg_logprob": -0.07594103489107298, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.00019833078840747476}, {"id": 77, "seek": 41090, "start": 410.9, "end": 416.17999999999995, "text": " what i would like to do in this lecture is i would like to basically write this repository", "tokens": [50365, 437, 741, 576, 411, 281, 360, 294, 341, 7991, 307, 741, 576, 411, 281, 1936, 2464, 341, 25841, 50629], "temperature": 0.0, "avg_logprob": -0.07111653110437226, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.0004335641278885305}, {"id": 78, "seek": 41090, "start": 416.17999999999995, "end": 421.78, "text": " from scratch so we're going to begin with an empty file and we're going to define a transformer piece", "tokens": [50629, 490, 8459, 370, 321, 434, 516, 281, 1841, 365, 364, 6707, 3991, 293, 321, 434, 516, 281, 6964, 257, 31782, 2522, 50909], "temperature": 0.0, "avg_logprob": -0.07111653110437226, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.0004335641278885305}, {"id": 79, "seek": 41090, "start": 421.78, "end": 427.62, "text": " by piece we're going to train it on the tiny shakespeare dataset and we'll see how we can then", "tokens": [50909, 538, 2522, 321, 434, 516, 281, 3847, 309, 322, 264, 5870, 37891, 22446, 28872, 293, 321, 603, 536, 577, 321, 393, 550, 51201], "temperature": 0.0, "avg_logprob": -0.07111653110437226, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.0004335641278885305}, {"id": 80, "seek": 41090, "start": 428.26, "end": 433.46, "text": " generate infinite shakespeare and of course this can copy paste to any arbitrary text dataset", "tokens": [51233, 8460, 13785, 37891, 22446, 293, 295, 1164, 341, 393, 5055, 9163, 281, 604, 23211, 2487, 28872, 51493], "temperature": 0.0, "avg_logprob": -0.07111653110437226, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.0004335641278885305}, {"id": 81, "seek": 41090, "start": 433.46, "end": 437.78, "text": " that you like but my goal really here is to just make you understand and appreciate", "tokens": [51493, 300, 291, 411, 457, 452, 3387, 534, 510, 307, 281, 445, 652, 291, 1223, 293, 4449, 51709], "temperature": 0.0, "avg_logprob": -0.07111653110437226, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.0004335641278885305}, {"id": 82, "seek": 41090, "start": 438.65999999999997, "end": 440.5, "text": " how under the hood chat gpt works", "tokens": [51753, 577, 833, 264, 13376, 5081, 290, 662, 1985, 51845], "temperature": 0.0, "avg_logprob": -0.07111653110437226, "compression_ratio": 1.8759398496240602, "no_speech_prob": 0.0004335641278885305}, {"id": 83, "seek": 44090, "start": 441.21999999999997, "end": 449.06, "text": " and really all that's required is a proficiency in python and some basic understanding of calculus", "tokens": [50381, 293, 534, 439, 300, 311, 4739, 307, 257, 1740, 42081, 294, 38797, 293, 512, 3875, 3701, 295, 33400, 50773], "temperature": 0.0, "avg_logprob": -0.054100366348915914, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0008216523565351963}, {"id": 84, "seek": 44090, "start": 449.06, "end": 454.65999999999997, "text": " and statistics and it would help if you also see my previous videos on the same youtube channel", "tokens": [50773, 293, 12523, 293, 309, 576, 854, 498, 291, 611, 536, 452, 3894, 2145, 322, 264, 912, 12487, 2269, 51053], "temperature": 0.0, "avg_logprob": -0.054100366348915914, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0008216523565351963}, {"id": 85, "seek": 44090, "start": 454.65999999999997, "end": 462.82, "text": " in particular my make more series where i define smaller and simpler neural network language models", "tokens": [51053, 294, 1729, 452, 652, 544, 2638, 689, 741, 6964, 4356, 293, 18587, 18161, 3209, 2856, 5245, 51461], "temperature": 0.0, "avg_logprob": -0.054100366348915914, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0008216523565351963}, {"id": 86, "seek": 44090, "start": 462.82, "end": 467.7, "text": " so multi-layered perceptrons and so on it really introduces the language modeling framework", "tokens": [51461, 370, 4825, 12, 8376, 4073, 43276, 13270, 293, 370, 322, 309, 534, 31472, 264, 2856, 15983, 8388, 51705], "temperature": 0.0, "avg_logprob": -0.054100366348915914, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0008216523565351963}, {"id": 87, "seek": 44090, "start": 467.7, "end": 470.73999999999995, "text": " and then here in this video we're going to focus on the transformer", "tokens": [51705, 293, 550, 510, 294, 341, 960, 321, 434, 516, 281, 1879, 322, 264, 31782, 51857], "temperature": 0.0, "avg_logprob": -0.054100366348915914, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0008216523565351963}, {"id": 88, "seek": 47090, "start": 470.9, "end": 473.94, "text": " so let's look at the general structure of the neural network itself", "tokens": [50365, 370, 718, 311, 574, 412, 264, 2674, 3877, 295, 264, 18161, 3209, 2564, 50517], "temperature": 0.4, "avg_logprob": -0.3268140059251052, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00026467093266546726}, {"id": 89, "seek": 47090, "start": 474.82, "end": 480.34, "text": " okay so i created a new google collab jupiter notebook here and this will allow me to later", "tokens": [50561, 1392, 370, 741, 2942, 257, 777, 20742, 44228, 361, 1010, 1681, 21060, 510, 293, 341, 486, 2089, 385, 281, 1780, 50837], "temperature": 0.4, "avg_logprob": -0.3268140059251052, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00026467093266546726}, {"id": 90, "seek": 47090, "start": 480.34, "end": 484.9, "text": " easily share this code that we're going to develop together with you so you can follow along so this", "tokens": [50837, 3612, 2073, 341, 3089, 300, 321, 434, 516, 281, 1499, 1214, 365, 291, 370, 291, 393, 1524, 2051, 370, 341, 51065], "temperature": 0.4, "avg_logprob": -0.3268140059251052, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00026467093266546726}, {"id": 91, "seek": 47090, "start": 484.9, "end": 490.41999999999996, "text": " will be in a video description later now here i've just done some preliminaries i downloaded", "tokens": [51065, 486, 312, 294, 257, 960, 3855, 1780, 586, 510, 741, 600, 445, 1096, 512, 26414, 259, 4889, 741, 21748, 51341], "temperature": 0.4, "avg_logprob": -0.3268140059251052, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00026467093266546726}, {"id": 92, "seek": 47090, "start": 490.41999999999996, "end": 495.06, "text": " the dataset the tiny shakespeare dataset at this url and you can see that it's about a one megabyte", "tokens": [51341, 264, 28872, 264, 5870, 37891, 22446, 28872, 412, 341, 4038, 75, 293, 291, 393, 536, 300, 309, 311, 466, 257, 472, 10816, 34529, 51573], "temperature": 0.4, "avg_logprob": -0.3268140059251052, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00026467093266546726}, {"id": 93, "seek": 47090, "start": 495.06, "end": 499.85999999999996, "text": " file then here i open the input.txt file and just read in all the text of the string", "tokens": [51573, 3991, 550, 510, 741, 1269, 264, 4846, 13, 83, 734, 3991, 293, 445, 1401, 294, 439, 264, 2487, 295, 264, 6798, 51813], "temperature": 0.4, "avg_logprob": -0.3268140059251052, "compression_ratio": 1.787375415282392, "no_speech_prob": 0.00026467093266546726}, {"id": 94, "seek": 49986, "start": 499.86, "end": 502.90000000000003, "text": " and you can see that we are working with one million characters roughly", "tokens": [50365, 293, 291, 393, 536, 300, 321, 366, 1364, 365, 472, 2459, 4342, 9810, 50517], "temperature": 0.0, "avg_logprob": -0.13216751593130607, "compression_ratio": 1.9065040650406504, "no_speech_prob": 0.0005753511795774102}, {"id": 95, "seek": 49986, "start": 503.86, "end": 507.86, "text": " and the first 1000 characters if we just print them out are basically what you would expect", "tokens": [50565, 293, 264, 700, 9714, 4342, 498, 321, 445, 4482, 552, 484, 366, 1936, 437, 291, 576, 2066, 50765], "temperature": 0.0, "avg_logprob": -0.13216751593130607, "compression_ratio": 1.9065040650406504, "no_speech_prob": 0.0005753511795774102}, {"id": 96, "seek": 49986, "start": 507.86, "end": 512.98, "text": " this is the first 1000 characters of the tiny shakespeare dataset roughly up to here", "tokens": [50765, 341, 307, 264, 700, 9714, 4342, 295, 264, 5870, 37891, 22446, 28872, 9810, 493, 281, 510, 51021], "temperature": 0.0, "avg_logprob": -0.13216751593130607, "compression_ratio": 1.9065040650406504, "no_speech_prob": 0.0005753511795774102}, {"id": 97, "seek": 49986, "start": 514.1, "end": 519.62, "text": " so so far so good next we're going to take this text and the text is a sequence of characters", "tokens": [51077, 370, 370, 1400, 370, 665, 958, 321, 434, 516, 281, 747, 341, 2487, 293, 264, 2487, 307, 257, 8310, 295, 4342, 51353], "temperature": 0.0, "avg_logprob": -0.13216751593130607, "compression_ratio": 1.9065040650406504, "no_speech_prob": 0.0005753511795774102}, {"id": 98, "seek": 49986, "start": 519.62, "end": 525.78, "text": " in python so when i call the set constructor on it i'm just going to get the set of all the", "tokens": [51353, 294, 38797, 370, 562, 741, 818, 264, 992, 47479, 322, 309, 741, 478, 445, 516, 281, 483, 264, 992, 295, 439, 264, 51661], "temperature": 0.0, "avg_logprob": -0.13216751593130607, "compression_ratio": 1.9065040650406504, "no_speech_prob": 0.0005753511795774102}, {"id": 99, "seek": 49986, "start": 525.78, "end": 527.78, "text": " characters that occur in this text", "tokens": [51661, 4342, 300, 5160, 294, 341, 2487, 51761], "temperature": 0.0, "avg_logprob": -0.13216751593130607, "compression_ratio": 1.9065040650406504, "no_speech_prob": 0.0005753511795774102}, {"id": 100, "seek": 52986, "start": 529.86, "end": 533.94, "text": " and then i'm just going to set the set of all the characters that occur in this text", "tokens": [50365, 293, 550, 741, 478, 445, 516, 281, 992, 264, 992, 295, 439, 264, 4342, 300, 5160, 294, 341, 2487, 50569], "temperature": 0.6000000000000001, "avg_logprob": -0.3863544464111328, "compression_ratio": 2.164285714285714, "no_speech_prob": 0.00031407849746756256}, {"id": 101, "seek": 52986, "start": 533.94, "end": 538.26, "text": " and then i'm going to I'm going to sort that to create a list of those characters instead of just", "tokens": [50569, 293, 550, 741, 478, 516, 281, 286, 478, 516, 281, 1333, 300, 281, 1884, 257, 1329, 295, 729, 4342, 2602, 295, 445, 50785], "temperature": 0.6000000000000001, "avg_logprob": -0.3863544464111328, "compression_ratio": 2.164285714285714, "no_speech_prob": 0.00031407849746756256}, {"id": 102, "seek": 52986, "start": 538.26, "end": 544.02, "text": " a set so that i have an ordering an arbitrary ordering and then i sort that so basically we", "tokens": [50785, 257, 992, 370, 300, 741, 362, 364, 21739, 364, 23211, 21739, 293, 550, 741, 1333, 300, 370, 1936, 321, 51073], "temperature": 0.6000000000000001, "avg_logprob": -0.3863544464111328, "compression_ratio": 2.164285714285714, "no_speech_prob": 0.00031407849746756256}, {"id": 103, "seek": 52986, "start": 544.02, "end": 548.74, "text": " get just all the characters that occur in the entire data set and they're sorted now the number", "tokens": [51073, 483, 445, 439, 264, 4342, 300, 5160, 294, 264, 2302, 1412, 992, 293, 436, 434, 25462, 586, 264, 1230, 51309], "temperature": 0.6000000000000001, "avg_logprob": -0.3863544464111328, "compression_ratio": 2.164285714285714, "no_speech_prob": 0.00031407849746756256}, {"id": 104, "seek": 52986, "start": 548.74, "end": 553.94, "text": " of them is going to be our vocabulary size these are the possible elements of our sequences and", "tokens": [51309, 295, 552, 307, 516, 281, 312, 527, 19864, 2744, 613, 366, 264, 1944, 4959, 295, 527, 22978, 293, 51569], "temperature": 0.6000000000000001, "avg_logprob": -0.3863544464111328, "compression_ratio": 2.164285714285714, "no_speech_prob": 0.00031407849746756256}, {"id": 105, "seek": 52986, "start": 553.94, "end": 559.62, "text": " we see that when i print here the characters there's 65 of them in total there's a space character and then all kinds of special characters", "tokens": [51569, 321, 536, 300, 562, 741, 4482, 510, 264, 4342, 456, 311, 11624, 295, 552, 294, 3217, 456, 311, 257, 1901, 2517, 293, 550, 439, 3685, 295, 2121, 4342, 51853], "temperature": 0.6000000000000001, "avg_logprob": -0.3863544464111328, "compression_ratio": 2.164285714285714, "no_speech_prob": 0.00031407849746756256}, {"id": 106, "seek": 55986, "start": 559.86, "end": 565.46, "text": " lowercase letters so that's our vocabulary and that's the sort of like possible characters that", "tokens": [50365, 3126, 9765, 7825, 370, 300, 311, 527, 19864, 293, 300, 311, 264, 1333, 295, 411, 1944, 4342, 300, 50645], "temperature": 0.0, "avg_logprob": -0.055310355292426214, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.03526560217142105}, {"id": 107, "seek": 55986, "start": 565.46, "end": 572.5, "text": " the model can see or emit okay so next we would like to develop some strategy to tokenize the", "tokens": [50645, 264, 2316, 393, 536, 420, 32084, 1392, 370, 958, 321, 576, 411, 281, 1499, 512, 5206, 281, 14862, 1125, 264, 50997], "temperature": 0.0, "avg_logprob": -0.055310355292426214, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.03526560217142105}, {"id": 108, "seek": 55986, "start": 572.5, "end": 579.14, "text": " input text now when people say tokenize they mean convert the raw text as a string to some", "tokens": [50997, 4846, 2487, 586, 562, 561, 584, 14862, 1125, 436, 914, 7620, 264, 8936, 2487, 382, 257, 6798, 281, 512, 51329], "temperature": 0.0, "avg_logprob": -0.055310355292426214, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.03526560217142105}, {"id": 109, "seek": 55986, "start": 579.14, "end": 584.66, "text": " sequence of integers according to some notebook according to some vocabulary of possible elements", "tokens": [51329, 8310, 295, 41674, 4650, 281, 512, 21060, 4650, 281, 512, 19864, 295, 1944, 4959, 51605], "temperature": 0.0, "avg_logprob": -0.055310355292426214, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.03526560217142105}, {"id": 110, "seek": 55986, "start": 585.38, "end": 589.38, "text": " so as an example here we are going to be building a character level language model", "tokens": [51641, 370, 382, 364, 1365, 510, 321, 366, 516, 281, 312, 2390, 257, 2517, 1496, 2856, 2316, 51841], "temperature": 0.0, "avg_logprob": -0.055310355292426214, "compression_ratio": 1.8663967611336032, "no_speech_prob": 0.03526560217142105}, {"id": 111, "seek": 58938, "start": 589.38, "end": 592.66, "text": " so we're simply going to be translating individual characters into integers", "tokens": [50365, 370, 321, 434, 2935, 516, 281, 312, 35030, 2609, 4342, 666, 41674, 50529], "temperature": 0.2, "avg_logprob": -0.1460153710751133, "compression_ratio": 2.025925925925926, "no_speech_prob": 8.442301623290405e-05}, {"id": 112, "seek": 58938, "start": 593.38, "end": 598.1, "text": " so let me show you a chunk of code that sort of does that for us so we're building both the", "tokens": [50565, 370, 718, 385, 855, 291, 257, 16635, 295, 3089, 300, 1333, 295, 775, 300, 337, 505, 370, 321, 434, 2390, 1293, 264, 50801], "temperature": 0.2, "avg_logprob": -0.1460153710751133, "compression_ratio": 2.025925925925926, "no_speech_prob": 8.442301623290405e-05}, {"id": 113, "seek": 58938, "start": 598.1, "end": 604.1, "text": " encoder and the decoder and let me just talk through what's happening here when we encode", "tokens": [50801, 2058, 19866, 293, 264, 979, 19866, 293, 718, 385, 445, 751, 807, 437, 311, 2737, 510, 562, 321, 2058, 1429, 51101], "temperature": 0.2, "avg_logprob": -0.1460153710751133, "compression_ratio": 2.025925925925926, "no_speech_prob": 8.442301623290405e-05}, {"id": 114, "seek": 58938, "start": 604.1, "end": 610.66, "text": " an arbitrary text like hi there we're going to receive a list of integers that represents that", "tokens": [51101, 364, 23211, 2487, 411, 4879, 456, 321, 434, 516, 281, 4774, 257, 1329, 295, 41674, 300, 8855, 300, 51429], "temperature": 0.2, "avg_logprob": -0.1460153710751133, "compression_ratio": 2.025925925925926, "no_speech_prob": 8.442301623290405e-05}, {"id": 115, "seek": 58938, "start": 610.66, "end": 618.74, "text": " string so for example 46 47 etc and then we also have the reverse mapping so we can take this list", "tokens": [51429, 6798, 370, 337, 1365, 17835, 16953, 5183, 293, 550, 321, 611, 362, 264, 9943, 18350, 370, 321, 393, 747, 341, 1329, 51833], "temperature": 0.2, "avg_logprob": -0.1460153710751133, "compression_ratio": 2.025925925925926, "no_speech_prob": 8.442301623290405e-05}, {"id": 116, "seek": 58938, "start": 618.74, "end": 619.36, "text": " and decode it into a string so we can take this list and decode it into a string so we can take", "tokens": [51833, 293, 979, 1429, 309, 666, 257, 6798, 370, 321, 393, 747, 341, 1329, 293, 979, 1429, 309, 666, 257, 6798, 370, 321, 393, 747, 51864], "temperature": 0.2, "avg_logprob": -0.1460153710751133, "compression_ratio": 2.025925925925926, "no_speech_prob": 8.442301623290405e-05}, {"id": 117, "seek": 61936, "start": 619.36, "end": 623.84, "text": " this list and decode it to get back the exact same string so it's really just like a translation", "tokens": [50365, 341, 1329, 293, 979, 1429, 309, 281, 483, 646, 264, 1900, 912, 6798, 370, 309, 311, 534, 445, 411, 257, 12853, 50589], "temperature": 0.0, "avg_logprob": -0.0771941016702091, "compression_ratio": 1.9148148148148147, "no_speech_prob": 0.0005068851169198751}, {"id": 118, "seek": 61936, "start": 623.84, "end": 628.8000000000001, "text": " to integers and back for arbitrary string and for us it is done on a character level", "tokens": [50589, 281, 41674, 293, 646, 337, 23211, 6798, 293, 337, 505, 309, 307, 1096, 322, 257, 2517, 1496, 50837], "temperature": 0.0, "avg_logprob": -0.0771941016702091, "compression_ratio": 1.9148148148148147, "no_speech_prob": 0.0005068851169198751}, {"id": 119, "seek": 61936, "start": 630.0, "end": 633.84, "text": " now the way this was achieved is we just iterate over all the characters here", "tokens": [50897, 586, 264, 636, 341, 390, 11042, 307, 321, 445, 44497, 670, 439, 264, 4342, 510, 51089], "temperature": 0.0, "avg_logprob": -0.0771941016702091, "compression_ratio": 1.9148148148148147, "no_speech_prob": 0.0005068851169198751}, {"id": 120, "seek": 61936, "start": 633.84, "end": 638.0, "text": " and create a lookup table from the character to the integer and vice versa", "tokens": [51089, 293, 1884, 257, 574, 1010, 3199, 490, 264, 2517, 281, 264, 24922, 293, 11964, 25650, 51297], "temperature": 0.0, "avg_logprob": -0.0771941016702091, "compression_ratio": 1.9148148148148147, "no_speech_prob": 0.0005068851169198751}, {"id": 121, "seek": 61936, "start": 638.0, "end": 642.4, "text": " and then to encode some string we simply translate all the characters individually", "tokens": [51297, 293, 550, 281, 2058, 1429, 512, 6798, 321, 2935, 13799, 439, 264, 4342, 16652, 51517], "temperature": 0.0, "avg_logprob": -0.0771941016702091, "compression_ratio": 1.9148148148148147, "no_speech_prob": 0.0005068851169198751}, {"id": 122, "seek": 61936, "start": 642.4, "end": 649.36, "text": " and to decode it back we use the reverse mapping concatenate all of it now this is only one of many", "tokens": [51517, 293, 281, 979, 1429, 309, 646, 321, 764, 264, 9943, 18350, 1588, 7186, 473, 439, 295, 309, 586, 341, 307, 787, 472, 295, 867, 51865], "temperature": 0.0, "avg_logprob": -0.0771941016702091, "compression_ratio": 1.9148148148148147, "no_speech_prob": 0.0005068851169198751}, {"id": 123, "seek": 64936, "start": 649.36, "end": 654.64, "text": " possible encodings or many possible tokenizers and it's a very simple one but there's many", "tokens": [50365, 1944, 2058, 378, 1109, 420, 867, 1944, 14862, 22525, 293, 309, 311, 257, 588, 2199, 472, 457, 456, 311, 867, 50629], "temperature": 1.0, "avg_logprob": -2.3183147618588844, "compression_ratio": 1.874762808349146, "no_speech_prob": 0.0014065112918615341}, {"id": 124, "seek": 64936, "start": 654.64, "end": 659.6, "text": " other schemas that people have come up with in practice so for example Google uses SENTENCEPIECE", "tokens": [50629, 661, 22627, 296, 300, 561, 362, 808, 493, 365, 294, 3124, 370, 337, 1365, 3329, 4960, 318, 9536, 2195, 34, 8929, 6550, 4969, 50877], "temperature": 1.0, "avg_logprob": -2.3183147618588844, "compression_ratio": 1.874762808349146, "no_speech_prob": 0.0014065112918615341}, {"id": 125, "seek": 64936, "start": 660.8000000000001, "end": 666.72, "text": " so SENTENCEPIECE will also encode text into integers but in a different schema", "tokens": [50937, 370, 318, 9536, 2195, 34, 8929, 6550, 4969, 486, 611, 2058, 1429, 2487, 666, 41674, 457, 294, 257, 819, 34078, 51233], "temperature": 1.0, "avg_logprob": -2.3183147618588844, "compression_ratio": 1.874762808349146, "no_speech_prob": 0.0014065112918615341}, {"id": 126, "seek": 64936, "start": 666.72, "end": 673.6, "text": " and using a different vocabulary and SENTENCEPIECE is a sub-word sort of tokenizer and what that", "tokens": [51233, 293, 1228, 257, 819, 19864, 293, 318, 9536, 2195, 34, 8929, 6550, 4969, 307, 257, 1422, 12, 7462, 1333, 295, 14862, 6545, 293, 437, 300, 51577], "temperature": 1.0, "avg_logprob": -2.3183147618588844, "compression_ratio": 1.874762808349146, "no_speech_prob": 0.0014065112918615341}, {"id": 127, "seek": 64936, "start": 673.6, "end": 679.12, "text": " means is that you're not encoding entire words but you're not also encoding individual characters", "tokens": [51577, 1355, 307, 300, 291, 434, 406, 43430, 2302, 2283, 457, 291, 434, 406, 611, 43430, 2609, 4342, 51853], "temperature": 1.0, "avg_logprob": -2.3183147618588844, "compression_ratio": 1.874762808349146, "no_speech_prob": 0.0014065112918615341}, {"id": 128, "seek": 67912, "start": 679.12, "end": 684.96, "text": " it's a subword unit level and that's usually what's adopted in practice. For example also", "tokens": [50365, 309, 311, 257, 1422, 7462, 4985, 1496, 293, 300, 311, 2673, 437, 311, 12175, 294, 3124, 13, 1171, 1365, 611, 50657], "temperature": 0.0, "avg_logprob": -0.15068060628483804, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.06152305379509926}, {"id": 129, "seek": 67912, "start": 684.96, "end": 689.92, "text": " OpenAI has this library called tiktoken that uses a byte pair encoding tokenizer", "tokens": [50657, 7238, 48698, 575, 341, 6405, 1219, 256, 9874, 8406, 300, 4960, 257, 40846, 6119, 43430, 14862, 6545, 50905], "temperature": 0.0, "avg_logprob": -0.15068060628483804, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.06152305379509926}, {"id": 130, "seek": 67912, "start": 690.96, "end": 697.6, "text": " and that's what GPT uses and you can also just encode words into like hello world into lists", "tokens": [50957, 293, 300, 311, 437, 26039, 51, 4960, 293, 291, 393, 611, 445, 2058, 1429, 2283, 666, 411, 7751, 1002, 666, 14511, 51289], "temperature": 0.0, "avg_logprob": -0.15068060628483804, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.06152305379509926}, {"id": 131, "seek": 67912, "start": 697.6, "end": 703.04, "text": " of integers. So as an example I'm using the tiktoken library here I'm getting the encoding", "tokens": [51289, 295, 41674, 13, 407, 382, 364, 1365, 286, 478, 1228, 264, 256, 9874, 8406, 6405, 510, 286, 478, 1242, 264, 43430, 51561], "temperature": 0.0, "avg_logprob": -0.15068060628483804, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.06152305379509926}, {"id": 132, "seek": 70304, "start": 703.04, "end": 710.0799999999999, "text": " for GPT-2 or that was used for GPT-2. Instead of just having 65 possible characters or tokens", "tokens": [50365, 337, 26039, 51, 12, 17, 420, 300, 390, 1143, 337, 26039, 51, 12, 17, 13, 7156, 295, 445, 1419, 11624, 1944, 4342, 420, 22667, 50717], "temperature": 0.0, "avg_logprob": -0.19883740031635844, "compression_ratio": 2.3346774193548385, "no_speech_prob": 7.850968540878966e-05}, {"id": 133, "seek": 70304, "start": 710.0799999999999, "end": 717.04, "text": " they have 50 000 tokens and so when they encode the exact same string high there we only get a", "tokens": [50717, 436, 362, 2625, 13711, 22667, 293, 370, 562, 436, 2058, 1429, 264, 1900, 912, 6798, 1090, 456, 321, 787, 483, 257, 51065], "temperature": 0.0, "avg_logprob": -0.19883740031635844, "compression_ratio": 2.3346774193548385, "no_speech_prob": 7.850968540878966e-05}, {"id": 134, "seek": 70304, "start": 717.04, "end": 725.8399999999999, "text": " list of three integers but those integers are not between 0 and 64 they are between 0 and 50 256.", "tokens": [51065, 1329, 295, 1045, 41674, 457, 729, 41674, 366, 406, 1296, 1958, 293, 12145, 436, 366, 1296, 1958, 293, 2625, 38882, 13, 51505], "temperature": 0.0, "avg_logprob": -0.19883740031635844, "compression_ratio": 2.3346774193548385, "no_speech_prob": 7.850968540878966e-05}, {"id": 135, "seek": 70304, "start": 726.7199999999999, "end": 732.8, "text": " So basically you can trade off the codebook size and the sequence lengths so you can have a very", "tokens": [51549, 407, 1936, 291, 393, 4923, 766, 264, 3089, 2939, 2744, 293, 264, 8310, 26329, 370, 291, 393, 362, 257, 588, 51853], "temperature": 0.0, "avg_logprob": -0.19883740031635844, "compression_ratio": 2.3346774193548385, "no_speech_prob": 7.850968540878966e-05}, {"id": 136, "seek": 70304, "start": 732.8, "end": 733.0, "text": " long string and you can have a very long string and you can have a very long string and you can", "tokens": [51853, 938, 6798, 293, 291, 393, 362, 257, 588, 938, 6798, 293, 291, 393, 362, 257, 588, 938, 6798, 293, 291, 393, 51863], "temperature": 0.0, "avg_logprob": -0.19883740031635844, "compression_ratio": 2.3346774193548385, "no_speech_prob": 7.850968540878966e-05}, {"id": 137, "seek": 70304, "start": 733.0, "end": 733.02, "text": " have a very long string and you can have a very long string and you can have a very long string and", "tokens": [51863, 362, 257, 588, 938, 6798, 293, 291, 393, 362, 257, 588, 938, 6798, 293, 291, 393, 362, 257, 588, 938, 6798, 293, 51864], "temperature": 0.0, "avg_logprob": -0.19883740031635844, "compression_ratio": 2.3346774193548385, "no_speech_prob": 7.850968540878966e-05}, {"id": 138, "seek": 73302, "start": 733.02, "end": 733.26, "text": " you can have a very long string and you can have a very long string and you can have a very long", "tokens": [50365, 291, 393, 362, 257, 588, 938, 6798, 293, 291, 393, 362, 257, 588, 938, 6798, 293, 291, 393, 362, 257, 588, 938, 50377], "temperature": 0.0, "avg_logprob": -0.06960677378105395, "compression_ratio": 2.204, "no_speech_prob": 0.00037307821912690997}, {"id": 139, "seek": 73302, "start": 733.26, "end": 737.34, "text": " sequences of integers with very small vocabularies or you can have short", "tokens": [50377, 22978, 295, 41674, 365, 588, 1359, 2329, 455, 1040, 530, 420, 291, 393, 362, 2099, 50581], "temperature": 0.0, "avg_logprob": -0.06960677378105395, "compression_ratio": 2.204, "no_speech_prob": 0.00037307821912690997}, {"id": 140, "seek": 73302, "start": 739.18, "end": 745.98, "text": " sequences of integers with very large vocabularies and so typically people use in practice these", "tokens": [50673, 22978, 295, 41674, 365, 588, 2416, 2329, 455, 1040, 530, 293, 370, 5850, 561, 764, 294, 3124, 613, 51013], "temperature": 0.0, "avg_logprob": -0.06960677378105395, "compression_ratio": 2.204, "no_speech_prob": 0.00037307821912690997}, {"id": 141, "seek": 73302, "start": 745.98, "end": 751.1, "text": " subword encodings but I'd like to keep our tokenizer very simple so we're using character", "tokens": [51013, 1422, 7462, 2058, 378, 1109, 457, 286, 1116, 411, 281, 1066, 527, 14862, 6545, 588, 2199, 370, 321, 434, 1228, 2517, 51269], "temperature": 0.0, "avg_logprob": -0.06960677378105395, "compression_ratio": 2.204, "no_speech_prob": 0.00037307821912690997}, {"id": 142, "seek": 73302, "start": 751.1, "end": 756.46, "text": " level tokenizer and that means that we have very small codebooks we have very simple encode and", "tokens": [51269, 1496, 14862, 6545, 293, 300, 1355, 300, 321, 362, 588, 1359, 3089, 15170, 321, 362, 588, 2199, 2058, 1429, 293, 51537], "temperature": 0.0, "avg_logprob": -0.06960677378105395, "compression_ratio": 2.204, "no_speech_prob": 0.00037307821912690997}, {"id": 143, "seek": 73302, "start": 756.46, "end": 762.98, "text": " decode functions but we do get very long sequences as a result but that's the level at which we're", "tokens": [51537, 979, 1429, 6828, 457, 321, 360, 483, 588, 938, 22978, 382, 257, 1874, 457, 300, 311, 264, 1496, 412, 597, 321, 434, 51863], "temperature": 0.0, "avg_logprob": -0.06960677378105395, "compression_ratio": 2.204, "no_speech_prob": 0.00037307821912690997}, {"id": 144, "seek": 76298, "start": 762.98, "end": 767.14, "text": " going to stick with this lecture because it's the simplest thing okay so now that we have an encoder", "tokens": [50365, 516, 281, 2897, 365, 341, 7991, 570, 309, 311, 264, 22811, 551, 1392, 370, 586, 300, 321, 362, 364, 2058, 19866, 50573], "temperature": 0.0, "avg_logprob": -0.07642075635384822, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.001083104987628758}, {"id": 145, "seek": 76298, "start": 767.14, "end": 772.4200000000001, "text": " and a decoder effectively a tokenizer we can tokenize the entire training set of Shakespeare", "tokens": [50573, 293, 257, 979, 19866, 8659, 257, 14862, 6545, 321, 393, 14862, 1125, 264, 2302, 3097, 992, 295, 22825, 50837], "temperature": 0.0, "avg_logprob": -0.07642075635384822, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.001083104987628758}, {"id": 146, "seek": 76298, "start": 772.98, "end": 777.22, "text": " so here's a chunk of code that does that and I'm going to start to use the pytorch library", "tokens": [50865, 370, 510, 311, 257, 16635, 295, 3089, 300, 775, 300, 293, 286, 478, 516, 281, 722, 281, 764, 264, 25878, 284, 339, 6405, 51077], "temperature": 0.0, "avg_logprob": -0.07642075635384822, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.001083104987628758}, {"id": 147, "seek": 76298, "start": 777.22, "end": 782.66, "text": " and specifically the torch.tensor from the pytorch library so we're going to take all of the text", "tokens": [51077, 293, 4682, 264, 27822, 13, 83, 23153, 490, 264, 25878, 284, 339, 6405, 370, 321, 434, 516, 281, 747, 439, 295, 264, 2487, 51349], "temperature": 0.0, "avg_logprob": -0.07642075635384822, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.001083104987628758}, {"id": 148, "seek": 76298, "start": 782.66, "end": 789.14, "text": " in tiny Shakespeare encode it and then wrap it into a torch.tensor to get the data tensor so", "tokens": [51349, 294, 5870, 22825, 2058, 1429, 309, 293, 550, 7019, 309, 666, 257, 27822, 13, 83, 23153, 281, 483, 264, 1412, 40863, 370, 51673], "temperature": 0.0, "avg_logprob": -0.07642075635384822, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.001083104987628758}, {"id": 149, "seek": 76298, "start": 789.14, "end": 792.9, "text": " here's what the data tensor looks like when I look at just the first one thousand character", "tokens": [51673, 510, 311, 437, 264, 1412, 40863, 1542, 411, 562, 286, 574, 412, 445, 264, 700, 472, 4714, 2517, 51861], "temperature": 0.0, "avg_logprob": -0.07642075635384822, "compression_ratio": 1.9090909090909092, "no_speech_prob": 0.001083104987628758}, {"id": 150, "seek": 79298, "start": 792.98, "end": 797.54, "text": " or the one thousand elements of it so we see that we have a massive sequence of integers", "tokens": [50365, 420, 264, 472, 4714, 4959, 295, 309, 370, 321, 536, 300, 321, 362, 257, 5994, 8310, 295, 41674, 50593], "temperature": 0.0, "avg_logprob": -0.07828211128164869, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0003319337556604296}, {"id": 151, "seek": 79298, "start": 798.1, "end": 803.54, "text": " and this sequence of integers here is basically an identical translation of the first 1000 characters", "tokens": [50621, 293, 341, 8310, 295, 41674, 510, 307, 1936, 364, 14800, 12853, 295, 264, 700, 9714, 4342, 50893], "temperature": 0.0, "avg_logprob": -0.07828211128164869, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0003319337556604296}, {"id": 152, "seek": 79298, "start": 803.54, "end": 810.58, "text": " here so I believe for example that zero is a new line character and maybe one is a space I'm not", "tokens": [50893, 510, 370, 286, 1697, 337, 1365, 300, 4018, 307, 257, 777, 1622, 2517, 293, 1310, 472, 307, 257, 1901, 286, 478, 406, 51245], "temperature": 0.0, "avg_logprob": -0.07828211128164869, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0003319337556604296}, {"id": 153, "seek": 79298, "start": 810.58, "end": 816.1800000000001, "text": " 100 sure but from now on the entire data set of text is re-represented as just it's just stretched", "tokens": [51245, 2319, 988, 457, 490, 586, 322, 264, 2302, 1412, 992, 295, 2487, 307, 319, 12, 38293, 382, 445, 309, 311, 445, 23563, 51525], "temperature": 0.0, "avg_logprob": -0.07828211128164869, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0003319337556604296}, {"id": 154, "seek": 79298, "start": 816.1800000000001, "end": 822.4200000000001, "text": " out as a single very large sequence of integers let me do one more thing before we move on here", "tokens": [51525, 484, 382, 257, 2167, 588, 2416, 8310, 295, 41674, 718, 385, 360, 472, 544, 551, 949, 321, 1286, 322, 510, 51837], "temperature": 0.0, "avg_logprob": -0.07828211128164869, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0003319337556604296}, {"id": 155, "seek": 82298, "start": 822.98, "end": 828.58, "text": " we're going to separate out our data set into a train and a validation split so in particular", "tokens": [50365, 321, 434, 516, 281, 4994, 484, 527, 1412, 992, 666, 257, 3847, 293, 257, 24071, 7472, 370, 294, 1729, 50645], "temperature": 0.0, "avg_logprob": -0.10763849953348323, "compression_ratio": 1.9387755102040816, "no_speech_prob": 0.00043392751831561327}, {"id": 156, "seek": 82298, "start": 828.58, "end": 833.38, "text": " we're going to take the first 90 of the data set and consider that to be the training data", "tokens": [50645, 321, 434, 516, 281, 747, 264, 700, 4289, 295, 264, 1412, 992, 293, 1949, 300, 281, 312, 264, 3097, 1412, 50885], "temperature": 0.0, "avg_logprob": -0.10763849953348323, "compression_ratio": 1.9387755102040816, "no_speech_prob": 0.00043392751831561327}, {"id": 157, "seek": 82298, "start": 833.38, "end": 838.74, "text": " for the transformer and we're going to withhold the last 10 at the end of it to be the validation", "tokens": [50885, 337, 264, 31782, 293, 321, 434, 516, 281, 48867, 264, 1036, 1266, 412, 264, 917, 295, 309, 281, 312, 264, 24071, 51153], "temperature": 0.0, "avg_logprob": -0.10763849953348323, "compression_ratio": 1.9387755102040816, "no_speech_prob": 0.00043392751831561327}, {"id": 158, "seek": 82298, "start": 838.74, "end": 843.38, "text": " data and this will help us understand to what extent our model is overfitting so we're going", "tokens": [51153, 1412, 293, 341, 486, 854, 505, 1223, 281, 437, 8396, 527, 2316, 307, 670, 69, 2414, 370, 321, 434, 516, 51385], "temperature": 0.0, "avg_logprob": -0.10763849953348323, "compression_ratio": 1.9387755102040816, "no_speech_prob": 0.00043392751831561327}, {"id": 159, "seek": 82298, "start": 843.38, "end": 848.02, "text": " to basically hide and keep the validation data on the side because we don't want just a perfect", "tokens": [51385, 281, 1936, 6479, 293, 1066, 264, 24071, 1412, 322, 264, 1252, 570, 321, 500, 380, 528, 445, 257, 2176, 51617], "temperature": 0.0, "avg_logprob": -0.10763849953348323, "compression_ratio": 1.9387755102040816, "no_speech_prob": 0.00043392751831561327}, {"id": 160, "seek": 82298, "start": 848.02, "end": 852.98, "text": " memorization of this exact Shakespeare we want a neural network that sort of creates Shakespeare's", "tokens": [51617, 10560, 2144, 295, 341, 1900, 22825, 321, 528, 257, 18161, 3209, 300, 1333, 295, 7829, 22825, 311, 51865], "temperature": 0.0, "avg_logprob": -0.10763849953348323, "compression_ratio": 1.9387755102040816, "no_speech_prob": 0.00043392751831561327}, {"id": 161, "seek": 85298, "start": 852.98, "end": 860.26, "text": " like text and so it should be fairly likely for it to produce the actual like stowed away", "tokens": [50365, 411, 2487, 293, 370, 309, 820, 312, 6457, 3700, 337, 309, 281, 5258, 264, 3539, 411, 342, 24347, 1314, 50729], "temperature": 0.0, "avg_logprob": -0.06432769794275264, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0006812915671616793}, {"id": 162, "seek": 85298, "start": 861.38, "end": 867.78, "text": " true Shakespeare text and so we're going to use this to get a sense of the overfitting okay so now", "tokens": [50785, 2074, 22825, 2487, 293, 370, 321, 434, 516, 281, 764, 341, 281, 483, 257, 2020, 295, 264, 670, 69, 2414, 1392, 370, 586, 51105], "temperature": 0.0, "avg_logprob": -0.06432769794275264, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0006812915671616793}, {"id": 163, "seek": 85298, "start": 867.78, "end": 872.1, "text": " we would like to start plugging these text sequences or integer sequences into the", "tokens": [51105, 321, 576, 411, 281, 722, 42975, 613, 2487, 22978, 420, 24922, 22978, 666, 264, 51321], "temperature": 0.0, "avg_logprob": -0.06432769794275264, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0006812915671616793}, {"id": 164, "seek": 85298, "start": 872.1, "end": 877.78, "text": " transformer so that it can train and learn those patterns now the important thing to realize is", "tokens": [51321, 31782, 370, 300, 309, 393, 3847, 293, 1466, 729, 8294, 586, 264, 1021, 551, 281, 4325, 307, 51605], "temperature": 0.0, "avg_logprob": -0.06432769794275264, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0006812915671616793}, {"id": 165, "seek": 85298, "start": 877.78, "end": 882.02, "text": " we're never going to actually feed entire text into transformer all at once that would be", "tokens": [51605, 321, 434, 1128, 516, 281, 767, 3154, 2302, 2487, 666, 31782, 439, 412, 1564, 300, 576, 312, 51817], "temperature": 0.0, "avg_logprob": -0.06432769794275264, "compression_ratio": 1.8502024291497976, "no_speech_prob": 0.0006812915671616793}, {"id": 166, "seek": 88298, "start": 882.98, "end": 888.02, "text": " very expensive and prohibitive so when we actually train a transformer on a lot of these data sets", "tokens": [50365, 588, 5124, 293, 16015, 2187, 370, 562, 321, 767, 3847, 257, 31782, 322, 257, 688, 295, 613, 1412, 6352, 50617], "temperature": 0.0, "avg_logprob": -0.046471735017489545, "compression_ratio": 2.0343511450381677, "no_speech_prob": 0.00026870128931477666}, {"id": 167, "seek": 88298, "start": 888.02, "end": 893.0600000000001, "text": " we only work with chunks of the data set and when we train the transformer we basically sample random", "tokens": [50617, 321, 787, 589, 365, 24004, 295, 264, 1412, 992, 293, 562, 321, 3847, 264, 31782, 321, 1936, 6889, 4974, 50869], "temperature": 0.0, "avg_logprob": -0.046471735017489545, "compression_ratio": 2.0343511450381677, "no_speech_prob": 0.00026870128931477666}, {"id": 168, "seek": 88298, "start": 893.0600000000001, "end": 898.4200000000001, "text": " little chunks out of the training set and train them just chunks at a time and these chunks have", "tokens": [50869, 707, 24004, 484, 295, 264, 3097, 992, 293, 3847, 552, 445, 24004, 412, 257, 565, 293, 613, 24004, 362, 51137], "temperature": 0.0, "avg_logprob": -0.046471735017489545, "compression_ratio": 2.0343511450381677, "no_speech_prob": 0.00026870128931477666}, {"id": 169, "seek": 88298, "start": 898.4200000000001, "end": 905.46, "text": " basically some kind of a length and some maximum length now the maximum length typically at least", "tokens": [51137, 1936, 512, 733, 295, 257, 4641, 293, 512, 6674, 4641, 586, 264, 6674, 4641, 5850, 412, 1935, 51489], "temperature": 0.0, "avg_logprob": -0.046471735017489545, "compression_ratio": 2.0343511450381677, "no_speech_prob": 0.00026870128931477666}, {"id": 170, "seek": 88298, "start": 905.46, "end": 911.22, "text": " in the code i usually write is called block size you can you can find it under different names", "tokens": [51489, 294, 264, 3089, 741, 2673, 2464, 307, 1219, 3461, 2744, 291, 393, 291, 393, 915, 309, 833, 819, 5288, 51777], "temperature": 0.0, "avg_logprob": -0.046471735017489545, "compression_ratio": 2.0343511450381677, "no_speech_prob": 0.00026870128931477666}, {"id": 171, "seek": 88298, "start": 911.22, "end": 912.82, "text": " like context length or something like that", "tokens": [51777, 411, 4319, 4641, 420, 746, 411, 300, 51857], "temperature": 0.0, "avg_logprob": -0.046471735017489545, "compression_ratio": 2.0343511450381677, "no_speech_prob": 0.00026870128931477666}, {"id": 172, "seek": 91298, "start": 913.3000000000001, "end": 917.86, "text": " let's start with the block size of just eight and let me look at the first train data characters", "tokens": [50381, 718, 311, 722, 365, 264, 3461, 2744, 295, 445, 3180, 293, 718, 385, 574, 412, 264, 700, 3847, 1412, 4342, 50609], "temperature": 0.0, "avg_logprob": -0.057538552327199025, "compression_ratio": 2.016326530612245, "no_speech_prob": 0.0002740852942224592}, {"id": 173, "seek": 91298, "start": 918.5, "end": 922.34, "text": " the first block size plus one characters i'll explain why plus one in a second", "tokens": [50641, 264, 700, 3461, 2744, 1804, 472, 4342, 741, 603, 2903, 983, 1804, 472, 294, 257, 1150, 50833], "temperature": 0.0, "avg_logprob": -0.057538552327199025, "compression_ratio": 2.016326530612245, "no_speech_prob": 0.0002740852942224592}, {"id": 174, "seek": 91298, "start": 923.7, "end": 929.62, "text": " so this is the first nine characters in the sequence in the training set now what i'd like", "tokens": [50901, 370, 341, 307, 264, 700, 4949, 4342, 294, 264, 8310, 294, 264, 3097, 992, 586, 437, 741, 1116, 411, 51197], "temperature": 0.0, "avg_logprob": -0.057538552327199025, "compression_ratio": 2.016326530612245, "no_speech_prob": 0.0002740852942224592}, {"id": 175, "seek": 91298, "start": 929.62, "end": 934.74, "text": " to point out is that when you sample a chunk of data like this so say these nine characters out", "tokens": [51197, 281, 935, 484, 307, 300, 562, 291, 6889, 257, 16635, 295, 1412, 411, 341, 370, 584, 613, 4949, 4342, 484, 51453], "temperature": 0.0, "avg_logprob": -0.057538552327199025, "compression_ratio": 2.016326530612245, "no_speech_prob": 0.0002740852942224592}, {"id": 176, "seek": 91298, "start": 934.74, "end": 941.0600000000001, "text": " of the training set this actually has multiple examples packed into it and that's because all", "tokens": [51453, 295, 264, 3097, 992, 341, 767, 575, 3866, 5110, 13265, 666, 309, 293, 300, 311, 570, 439, 51769], "temperature": 0.0, "avg_logprob": -0.057538552327199025, "compression_ratio": 2.016326530612245, "no_speech_prob": 0.0002740852942224592}, {"id": 177, "seek": 91298, "start": 941.0600000000001, "end": 942.74, "text": " of these characters follow each other", "tokens": [51769, 295, 613, 4342, 1524, 1184, 661, 51853], "temperature": 0.0, "avg_logprob": -0.057538552327199025, "compression_ratio": 2.016326530612245, "no_speech_prob": 0.0002740852942224592}, {"id": 178, "seek": 94298, "start": 942.98, "end": 949.0600000000001, "text": " and so what this thing is going to say when we plug it into a transformer is we're going to", "tokens": [50365, 293, 370, 437, 341, 551, 307, 516, 281, 584, 562, 321, 5452, 309, 666, 257, 31782, 307, 321, 434, 516, 281, 50669], "temperature": 1.0, "avg_logprob": -0.2716348414518395, "compression_ratio": 1.8232758620689655, "no_speech_prob": 0.00014990601630415767}, {"id": 179, "seek": 94298, "start": 949.0600000000001, "end": 952.9, "text": " actually simultaneously train it to make a prediction at every one of these positions", "tokens": [50669, 767, 16561, 3847, 309, 281, 652, 257, 17630, 412, 633, 472, 295, 613, 8432, 50861], "temperature": 1.0, "avg_logprob": -0.2716348414518395, "compression_ratio": 1.8232758620689655, "no_speech_prob": 0.00014990601630415767}, {"id": 180, "seek": 94298, "start": 953.7, "end": 960.02, "text": " now in the in a chunk of nine characters there's actually eight individual examples packed in there", "tokens": [50901, 586, 294, 264, 294, 257, 16635, 295, 4949, 4342, 456, 311, 767, 3180, 2609, 5110, 13265, 294, 456, 51217], "temperature": 1.0, "avg_logprob": -0.2716348414518395, "compression_ratio": 1.8232758620689655, "no_speech_prob": 0.00014990601630415767}, {"id": 181, "seek": 94298, "start": 960.5, "end": 968.34, "text": " so there's the example that when 18 when in the context of 18 47 likely comes next in a context of", "tokens": [51241, 370, 456, 311, 264, 1365, 300, 562, 2443, 562, 294, 264, 4319, 295, 2443, 16953, 3700, 1487, 958, 294, 257, 4319, 295, 51633], "temperature": 1.0, "avg_logprob": -0.2716348414518395, "compression_ratio": 1.8232758620689655, "no_speech_prob": 0.00014990601630415767}, {"id": 182, "seek": 94298, "start": 968.34, "end": 972.26, "text": " 18 and 47 56 comes next in the context of 1847", "tokens": [51633, 2443, 293, 16953, 19687, 1487, 958, 294, 264, 4319, 295, 2443, 14060, 51829], "temperature": 1.0, "avg_logprob": -0.2716348414518395, "compression_ratio": 1.8232758620689655, "no_speech_prob": 0.00014990601630415767}, {"id": 183, "seek": 97298, "start": 972.98, "end": 976.88, "text": " 47, 56, 57 can come next, and so on.", "tokens": [50365, 16953, 11, 19687, 11, 21423, 393, 808, 958, 11, 293, 370, 322, 13, 50560], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 184, "seek": 97298, "start": 977.28, "end": 979.1800000000001, "text": " So that's the eight individual examples.", "tokens": [50580, 407, 300, 311, 264, 3180, 2609, 5110, 13, 50675], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 185, "seek": 97298, "start": 979.64, "end": 981.26, "text": " Let me actually spell it out with code.", "tokens": [50698, 961, 385, 767, 9827, 309, 484, 365, 3089, 13, 50779], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 186, "seek": 97298, "start": 982.48, "end": 984.16, "text": " So here's a chunk of code to illustrate.", "tokens": [50840, 407, 510, 311, 257, 16635, 295, 3089, 281, 23221, 13, 50924], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 187, "seek": 97298, "start": 985.08, "end": 986.9200000000001, "text": " X are the inputs to the transformer.", "tokens": [50970, 1783, 366, 264, 15743, 281, 264, 31782, 13, 51062], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 188, "seek": 97298, "start": 987.1800000000001, "end": 989.24, "text": " It will just be the first block size characters.", "tokens": [51075, 467, 486, 445, 312, 264, 700, 3461, 2744, 4342, 13, 51178], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 189, "seek": 97298, "start": 990.1, "end": 993.96, "text": " Y will be the next block size characters.", "tokens": [51221, 398, 486, 312, 264, 958, 3461, 2744, 4342, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 190, "seek": 97298, "start": 993.96, "end": 995.34, "text": " So it's offset by one.", "tokens": [51414, 407, 309, 311, 18687, 538, 472, 13, 51483], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 191, "seek": 97298, "start": 996.2, "end": 1000.46, "text": " And that's because Y are the targets for each position in the input.", "tokens": [51526, 400, 300, 311, 570, 398, 366, 264, 12911, 337, 1184, 2535, 294, 264, 4846, 13, 51739], "temperature": 0.0, "avg_logprob": -0.23443475775762435, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.058639366179704666}, {"id": 192, "seek": 100046, "start": 1000.46, "end": 1004.12, "text": " And then here I'm iterating over all the block size of eight.", "tokens": [50365, 400, 550, 510, 286, 478, 17138, 990, 670, 439, 264, 3461, 2744, 295, 3180, 13, 50548], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 193, "seek": 100046, "start": 1004.9200000000001, "end": 1009.88, "text": " And the context is always all the characters in X up to T and including T.", "tokens": [50588, 400, 264, 4319, 307, 1009, 439, 264, 4342, 294, 1783, 493, 281, 314, 293, 3009, 314, 13, 50836], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 194, "seek": 100046, "start": 1010.58, "end": 1015.5, "text": " And the target is always the T character, but in the targets array Y.", "tokens": [50871, 400, 264, 3779, 307, 1009, 264, 314, 2517, 11, 457, 294, 264, 12911, 10225, 398, 13, 51117], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 195, "seek": 100046, "start": 1016.1800000000001, "end": 1016.96, "text": " So let me just run this.", "tokens": [51151, 407, 718, 385, 445, 1190, 341, 13, 51190], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 196, "seek": 100046, "start": 1018.14, "end": 1020.6, "text": " And basically it spells out what I said in words.", "tokens": [51249, 400, 1936, 309, 25053, 484, 437, 286, 848, 294, 2283, 13, 51372], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 197, "seek": 100046, "start": 1021.2, "end": 1024.9, "text": " These are the eight examples hidden in a chunk of nine characters", "tokens": [51402, 1981, 366, 264, 3180, 5110, 7633, 294, 257, 16635, 295, 4949, 4342, 51587], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 198, "seek": 100046, "start": 1024.9, "end": 1028.82, "text": " that we sampled from the training set.", "tokens": [51587, 300, 321, 3247, 15551, 490, 264, 3097, 992, 13, 51783], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 199, "seek": 100046, "start": 1029.68, "end": 1030.44, "text": " I want to make sure that I'm not missing anything.", "tokens": [51826, 286, 528, 281, 652, 988, 300, 286, 478, 406, 5361, 1340, 13, 51864], "temperature": 0.0, "avg_logprob": -0.16641395112388155, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.253428288502619e-06}, {"id": 200, "seek": 103044, "start": 1030.44, "end": 1031.1200000000001, "text": " Let me just mention one more thing.", "tokens": [50365, 961, 385, 445, 2152, 472, 544, 551, 13, 50399], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 201, "seek": 103044, "start": 1031.7, "end": 1036.3400000000001, "text": " We train on all the eight examples here with context between one", "tokens": [50428, 492, 3847, 322, 439, 264, 3180, 5110, 510, 365, 4319, 1296, 472, 50660], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 202, "seek": 103044, "start": 1036.3400000000001, "end": 1038.22, "text": " all the way up to context of block size.", "tokens": [50660, 439, 264, 636, 493, 281, 4319, 295, 3461, 2744, 13, 50754], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 203, "seek": 103044, "start": 1038.74, "end": 1041.16, "text": " And we train on that not just for computational reasons", "tokens": [50780, 400, 321, 3847, 322, 300, 406, 445, 337, 28270, 4112, 50901], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 204, "seek": 103044, "start": 1041.16, "end": 1043.7, "text": " because we happen to have the sequence already or something like that.", "tokens": [50901, 570, 321, 1051, 281, 362, 264, 8310, 1217, 420, 746, 411, 300, 13, 51028], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 205, "seek": 103044, "start": 1043.74, "end": 1045.16, "text": " It's not just done for efficiency.", "tokens": [51030, 467, 311, 406, 445, 1096, 337, 10493, 13, 51101], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 206, "seek": 103044, "start": 1045.66, "end": 1051.5, "text": " It's also done to make the transformer network be used to seeing contexts", "tokens": [51126, 467, 311, 611, 1096, 281, 652, 264, 31782, 3209, 312, 1143, 281, 2577, 30628, 51418], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 207, "seek": 103044, "start": 1051.5, "end": 1055.14, "text": " all the way from as little as one all the way to block size.", "tokens": [51418, 439, 264, 636, 490, 382, 707, 382, 472, 439, 264, 636, 281, 3461, 2744, 13, 51600], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 208, "seek": 103044, "start": 1055.7, "end": 1058.9, "text": " And we'd like the transformer to be used to seeing everything in between.", "tokens": [51628, 400, 321, 1116, 411, 264, 31782, 281, 312, 1143, 281, 2577, 1203, 294, 1296, 13, 51788], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 209, "seek": 103044, "start": 1058.9, "end": 1060.3200000000002, "text": " And that's going to be useful.", "tokens": [51788, 400, 300, 311, 516, 281, 312, 4420, 13, 51859], "temperature": 0.0, "avg_logprob": -0.13728134667695457, "compression_ratio": 1.9532374100719425, "no_speech_prob": 0.0064772674813866615}, {"id": 210, "seek": 106044, "start": 1060.44, "end": 1063.02, "text": " Later during inference, because while we're sampling,", "tokens": [50365, 11965, 1830, 38253, 11, 570, 1339, 321, 434, 21179, 11, 50494], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 211, "seek": 106044, "start": 1063.38, "end": 1067.3, "text": " we can start to set a sampling generation with as little as one character of context.", "tokens": [50512, 321, 393, 722, 281, 992, 257, 21179, 5125, 365, 382, 707, 382, 472, 2517, 295, 4319, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 212, "seek": 106044, "start": 1067.68, "end": 1069.88, "text": " And the transformer knows how to predict the next character", "tokens": [50727, 400, 264, 31782, 3255, 577, 281, 6069, 264, 958, 2517, 50837], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 213, "seek": 106044, "start": 1070.16, "end": 1072.2, "text": " with all the way up to just context of one.", "tokens": [50851, 365, 439, 264, 636, 493, 281, 445, 4319, 295, 472, 13, 50953], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 214, "seek": 106044, "start": 1072.74, "end": 1075.1000000000001, "text": " And so then it can predict everything up to block size.", "tokens": [50980, 400, 370, 550, 309, 393, 6069, 1203, 493, 281, 3461, 2744, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 215, "seek": 106044, "start": 1075.44, "end": 1079.5, "text": " And after block size, we have to start truncating because the transformer will never", "tokens": [51115, 400, 934, 3461, 2744, 11, 321, 362, 281, 722, 504, 409, 66, 990, 570, 264, 31782, 486, 1128, 51318], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 216, "seek": 106044, "start": 1080.2, "end": 1083.66, "text": " receive more than block size inputs when it's predicting the next character.", "tokens": [51353, 4774, 544, 813, 3461, 2744, 15743, 562, 309, 311, 32884, 264, 958, 2517, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 217, "seek": 106044, "start": 1084.78, "end": 1089.3, "text": " Okay, so we've looked at the time dimension of the tensors that are going to be feeding into the transformer.", "tokens": [51582, 1033, 11, 370, 321, 600, 2956, 412, 264, 565, 10139, 295, 264, 10688, 830, 300, 366, 516, 281, 312, 12919, 666, 264, 31782, 13, 51808], "temperature": 0.0, "avg_logprob": -0.2460192210638701, "compression_ratio": 1.9096989966555185, "no_speech_prob": 0.0016823670594021678}, {"id": 218, "seek": 108930, "start": 1089.3, "end": 1092.12, "text": " There's one more dimension to care about, and that is the batch dimension.", "tokens": [50365, 821, 311, 472, 544, 10139, 281, 1127, 466, 11, 293, 300, 307, 264, 15245, 10139, 13, 50506], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 219, "seek": 108930, "start": 1092.82, "end": 1098.96, "text": " And so as we're sampling these chunks of text, we're going to be actually every time we're going to feed them into a transformer,", "tokens": [50541, 400, 370, 382, 321, 434, 21179, 613, 24004, 295, 2487, 11, 321, 434, 516, 281, 312, 767, 633, 565, 321, 434, 516, 281, 3154, 552, 666, 257, 31782, 11, 50848], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 220, "seek": 108930, "start": 1099.32, "end": 1104.06, "text": " we're going to have many batches of multiple chunks of text that are all like stacked up in a single tensor.", "tokens": [50866, 321, 434, 516, 281, 362, 867, 15245, 279, 295, 3866, 24004, 295, 2487, 300, 366, 439, 411, 28867, 493, 294, 257, 2167, 40863, 13, 51103], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 221, "seek": 108930, "start": 1104.6599999999999, "end": 1112.5, "text": " And that's just done for efficiency just so that we can keep the GPUs busy because they are very good at parallel processing of data.", "tokens": [51133, 400, 300, 311, 445, 1096, 337, 10493, 445, 370, 300, 321, 393, 1066, 264, 18407, 82, 5856, 570, 436, 366, 588, 665, 412, 8952, 9007, 295, 1412, 13, 51525], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 222, "seek": 108930, "start": 1113.06, "end": 1116.6, "text": " And so we just want to process multiple chunks all at the same time.", "tokens": [51553, 400, 370, 321, 445, 528, 281, 1399, 3866, 24004, 439, 412, 264, 912, 565, 13, 51730], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 223, "seek": 108930, "start": 1116.8999999999999, "end": 1118.94, "text": " But those chunks are processed completely independently.", "tokens": [51745, 583, 729, 24004, 366, 18846, 2584, 21761, 13, 51847], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 224, "seek": 108930, "start": 1118.94, "end": 1119.28, "text": " They don't take up too much space.", "tokens": [51847, 814, 500, 380, 747, 493, 886, 709, 1901, 13, 51864], "temperature": 0.2, "avg_logprob": -0.20812112216291756, "compression_ratio": 1.853658536585366, "no_speech_prob": 0.0013653405476361513}, {"id": 225, "seek": 111928, "start": 1119.28, "end": 1120.8999999999999, "text": " They don't talk to each other and so on.", "tokens": [50365, 814, 500, 380, 751, 281, 1184, 661, 293, 370, 322, 13, 50446], "temperature": 0.0, "avg_logprob": -0.21949607222827514, "compression_ratio": 1.779935275080906, "no_speech_prob": 8.466751751257107e-05}, {"id": 226, "seek": 111928, "start": 1121.56, "end": 1124.72, "text": " So let me basically just generalize this and introduce a batch dimension.", "tokens": [50479, 407, 718, 385, 1936, 445, 2674, 1125, 341, 293, 5366, 257, 15245, 10139, 13, 50637], "temperature": 0.0, "avg_logprob": -0.21949607222827514, "compression_ratio": 1.779935275080906, "no_speech_prob": 8.466751751257107e-05}, {"id": 227, "seek": 111928, "start": 1125.06, "end": 1125.96, "text": " Here's a chunk of code.", "tokens": [50654, 1692, 311, 257, 16635, 295, 3089, 13, 50699], "temperature": 0.0, "avg_logprob": -0.21949607222827514, "compression_ratio": 1.779935275080906, "no_speech_prob": 8.466751751257107e-05}, {"id": 228, "seek": 111928, "start": 1127.34, "end": 1129.56, "text": " Let me just run it and then I'm going to explain what it does.", "tokens": [50768, 961, 385, 445, 1190, 309, 293, 550, 286, 478, 516, 281, 2903, 437, 309, 775, 13, 50879], "temperature": 0.0, "avg_logprob": -0.21949607222827514, "compression_ratio": 1.779935275080906, "no_speech_prob": 8.466751751257107e-05}, {"id": 229, "seek": 111928, "start": 1131.8, "end": 1145.3799999999999, "text": " So here, because we're going to start sampling random locations in the data sets to pull chunks from, I am setting the seed so that in the random number generator, so that the numbers I see here are going to be the same numbers you see later if you try to reproduce this.", "tokens": [50991, 407, 510, 11, 570, 321, 434, 516, 281, 722, 21179, 4974, 9253, 294, 264, 1412, 6352, 281, 2235, 24004, 490, 11, 286, 669, 3287, 264, 8871, 370, 300, 294, 264, 4974, 1230, 19265, 11, 370, 300, 264, 3547, 286, 536, 510, 366, 516, 281, 312, 264, 912, 3547, 291, 536, 1780, 498, 291, 853, 281, 29501, 341, 13, 51670], "temperature": 0.0, "avg_logprob": -0.21949607222827514, "compression_ratio": 1.779935275080906, "no_speech_prob": 8.466751751257107e-05}, {"id": 230, "seek": 111928, "start": 1146.46, "end": 1148.86, "text": " Now, the batch size here is how many independent sequences we are producing.", "tokens": [51724, 823, 11, 264, 15245, 2744, 510, 307, 577, 867, 6695, 22978, 321, 366, 10501, 13, 51844], "temperature": 0.0, "avg_logprob": -0.21949607222827514, "compression_ratio": 1.779935275080906, "no_speech_prob": 8.466751751257107e-05}, {"id": 231, "seek": 114928, "start": 1149.28, "end": 1152.3, "text": " We're processing every forward backward pass of the transformer.", "tokens": [50365, 492, 434, 9007, 633, 2128, 23897, 1320, 295, 264, 31782, 13, 50516], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 232, "seek": 114928, "start": 1153.78, "end": 1157.8, "text": " The block size, as I explained, is the maximum context length to make those predictions.", "tokens": [50590, 440, 3461, 2744, 11, 382, 286, 8825, 11, 307, 264, 6674, 4319, 4641, 281, 652, 729, 21264, 13, 50791], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 233, "seek": 114928, "start": 1158.44, "end": 1160.5, "text": " So let's say batch size four, block size eight.", "tokens": [50823, 407, 718, 311, 584, 15245, 2744, 1451, 11, 3461, 2744, 3180, 13, 50926], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 234, "seek": 114928, "start": 1160.92, "end": 1164.16, "text": " And then here's how we get batch for any arbitrary split.", "tokens": [50947, 400, 550, 510, 311, 577, 321, 483, 15245, 337, 604, 23211, 7472, 13, 51109], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 235, "seek": 114928, "start": 1164.82, "end": 1168.72, "text": " If the split is a training split, then we're going to look at train data, otherwise at val data.", "tokens": [51142, 759, 264, 7472, 307, 257, 3097, 7472, 11, 550, 321, 434, 516, 281, 574, 412, 3847, 1412, 11, 5911, 412, 1323, 1412, 13, 51337], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 236, "seek": 114928, "start": 1170.04, "end": 1172.1399999999999, "text": " That gives us the data array.", "tokens": [51403, 663, 2709, 505, 264, 1412, 10225, 13, 51508], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 237, "seek": 114928, "start": 1172.86, "end": 1178.62, "text": " And then when I generate random positions to grab a chunk out of, I actually grab, I actually generate random data.", "tokens": [51544, 400, 550, 562, 286, 8460, 4974, 8432, 281, 4444, 257, 16635, 484, 295, 11, 286, 767, 4444, 11, 286, 767, 8460, 4974, 1412, 13, 51832], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 238, "seek": 114928, "start": 1178.62, "end": 1179.1399999999999, "text": " I actually generate random positions to grab a chunk out of.", "tokens": [51832, 286, 767, 8460, 4974, 8432, 281, 4444, 257, 16635, 484, 295, 13, 51858], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 239, "seek": 114928, "start": 1179.1399999999999, "end": 1179.24, "text": " I actually generate random positions to grab a chunk out of.", "tokens": [51858, 286, 767, 8460, 4974, 8432, 281, 4444, 257, 16635, 484, 295, 13, 51863], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 240, "seek": 114928, "start": 1179.24, "end": 1179.26, "text": " I actually generate random positions to grab a chunk out of.", "tokens": [51863, 286, 767, 8460, 4974, 8432, 281, 4444, 257, 16635, 484, 295, 13, 51864], "temperature": 0.0, "avg_logprob": -0.28796601728959514, "compression_ratio": 2.2607260726072607, "no_speech_prob": 0.0031270154286175966}, {"id": 241, "seek": 117928, "start": 1179.28, "end": 1182.96, "text": " assumeonline, will generate batch size number of random offsets.", "tokens": [50365, 6552, 266, 1889, 11, 486, 8460, 15245, 2744, 1230, 295, 4974, 39457, 1385, 13, 50549], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 242, "seek": 117928, "start": 1183.2, "end": 1188.08, "text": " So because this is four, we are, i, x is going to be a four numbers that are randomly generated between 0 and len of data minus block size.", "tokens": [50561, 407, 570, 341, 307, 1451, 11, 321, 366, 11, 741, 11, 2031, 307, 516, 281, 312, 257, 1451, 3547, 300, 366, 16979, 10833, 1296, 1958, 293, 40116, 295, 1412, 3175, 3461, 2744, 13, 50805], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 243, "seek": 117928, "start": 1188.08, "end": 1192.06, "text": " are randomly generated between 0 and len of data minus block size.", "tokens": [50805, 366, 16979, 10833, 1296, 1958, 293, 40116, 295, 1412, 3175, 3461, 2744, 13, 51004], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 244, "seek": 117928, "start": 1193.46, "end": 1194.84, "text": " So it's just random offsets into the training set.", "tokens": [51074, 407, 309, 311, 445, 4974, 39457, 1385, 666, 264, 3097, 992, 13, 51143], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 245, "seek": 117928, "start": 1195.62, "end": 1203.52, "text": " And then x' as I explained are the first block size characters, starting at i.", "tokens": [51182, 400, 550, 2031, 6, 382, 286, 8825, 366, 264, 700, 3461, 2744, 4342, 11, 2891, 412, 741, 13, 51577], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 246, "seek": 117928, "start": 1203.92, "end": 1206.1399999999999, "text": " The y' are the offset by 1 of that.", "tokens": [51597, 440, 288, 6, 366, 264, 18687, 538, 502, 295, 300, 13, 51708], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 247, "seek": 117928, "start": 1207.28, "end": 1207.96, "text": " So just add plus 1.", "tokens": [51765, 407, 445, 909, 1804, 502, 13, 51799], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 248, "seek": 117928, "start": 1208.04, "end": 1208.6399999999999, "text": " And then we're going to get roughly how many random fields are generated.", "tokens": [51803, 400, 550, 321, 434, 516, 281, 483, 9810, 577, 867, 4974, 7909, 366, 10833, 13, 51833], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 249, "seek": 117928, "start": 1208.6399999999999, "end": 1209.22, "text": " So just add plus 1.", "tokens": [51833, 407, 445, 909, 1804, 502, 13, 51862], "temperature": 1.0, "avg_logprob": -1.191544094601193, "compression_ratio": 2.0183150183150182, "no_speech_prob": 0.005663761403411627}, {"id": 250, "seek": 120922, "start": 1209.22, "end": 1216.34, "text": " get those chunks for every one of integers i in ix and use a torch.stack to take all those", "tokens": [50365, 483, 729, 24004, 337, 633, 472, 295, 41674, 741, 294, 741, 87, 293, 764, 257, 27822, 13, 372, 501, 281, 747, 439, 729, 50721], "temperature": 0.0, "avg_logprob": -0.12526001728756328, "compression_ratio": 1.5889570552147239, "no_speech_prob": 0.051050055772066116}, {"id": 251, "seek": 120922, "start": 1217.54, "end": 1223.78, "text": " one-dimensional tensors as we saw here and we're going to stack them up as rows", "tokens": [50781, 472, 12, 18759, 10688, 830, 382, 321, 1866, 510, 293, 321, 434, 516, 281, 8630, 552, 493, 382, 13241, 51093], "temperature": 0.0, "avg_logprob": -0.12526001728756328, "compression_ratio": 1.5889570552147239, "no_speech_prob": 0.051050055772066116}, {"id": 252, "seek": 120922, "start": 1224.9, "end": 1230.66, "text": " and so they all become a row in a four by eight tensor so here's where i'm printing them", "tokens": [51149, 293, 370, 436, 439, 1813, 257, 5386, 294, 257, 1451, 538, 3180, 40863, 370, 510, 311, 689, 741, 478, 14699, 552, 51437], "temperature": 0.0, "avg_logprob": -0.12526001728756328, "compression_ratio": 1.5889570552147239, "no_speech_prob": 0.051050055772066116}, {"id": 253, "seek": 123066, "start": 1230.66, "end": 1239.6000000000001, "text": " when i sample a batch xb and yb the inputs the transformer now are the input x is the four by", "tokens": [50365, 562, 741, 6889, 257, 15245, 2031, 65, 293, 288, 65, 264, 15743, 264, 31782, 586, 366, 264, 4846, 2031, 307, 264, 1451, 538, 50812], "temperature": 0.0, "avg_logprob": -0.07922752972306876, "compression_ratio": 1.8601036269430051, "no_speech_prob": 9.950075764209032e-05}, {"id": 254, "seek": 123066, "start": 1239.6000000000001, "end": 1247.92, "text": " eight tensor four rows of eight columns and each one of these is a chunk of the training set", "tokens": [50812, 3180, 40863, 1451, 13241, 295, 3180, 13766, 293, 1184, 472, 295, 613, 307, 257, 16635, 295, 264, 3097, 992, 51228], "temperature": 0.0, "avg_logprob": -0.07922752972306876, "compression_ratio": 1.8601036269430051, "no_speech_prob": 9.950075764209032e-05}, {"id": 255, "seek": 123066, "start": 1247.92, "end": 1255.1000000000001, "text": " and then the targets here are in the associated array y and they will come in to the transformer", "tokens": [51228, 293, 550, 264, 12911, 510, 366, 294, 264, 6615, 10225, 288, 293, 436, 486, 808, 294, 281, 264, 31782, 51587], "temperature": 0.0, "avg_logprob": -0.07922752972306876, "compression_ratio": 1.8601036269430051, "no_speech_prob": 9.950075764209032e-05}, {"id": 256, "seek": 123066, "start": 1255.1000000000001, "end": 1260.66, "text": " all the way at the end to create the loss function so they will give us the", "tokens": [51587, 439, 264, 636, 412, 264, 917, 281, 1884, 264, 4470, 2445, 370, 436, 486, 976, 505, 264, 51865], "temperature": 0.0, "avg_logprob": -0.07922752972306876, "compression_ratio": 1.8601036269430051, "no_speech_prob": 9.950075764209032e-05}, {"id": 257, "seek": 126066, "start": 1260.66, "end": 1267.22, "text": " correct answer for every single position inside x and then these are the four independent rows", "tokens": [50365, 3006, 1867, 337, 633, 2167, 2535, 1854, 2031, 293, 550, 613, 366, 264, 1451, 6695, 13241, 50693], "temperature": 0.0, "avg_logprob": -0.09023589122144482, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.00013676205708179623}, {"id": 258, "seek": 126066, "start": 1268.9, "end": 1276.5, "text": " so spelled out as we did before this 4x8 array contains a total of 32 examples", "tokens": [50777, 370, 34388, 484, 382, 321, 630, 949, 341, 1017, 87, 23, 10225, 8306, 257, 3217, 295, 8858, 5110, 51157], "temperature": 0.0, "avg_logprob": -0.09023589122144482, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.00013676205708179623}, {"id": 259, "seek": 126066, "start": 1277.0600000000002, "end": 1280.1000000000001, "text": " and they're completely independent as far as the transformer is concerned", "tokens": [51185, 293, 436, 434, 2584, 6695, 382, 1400, 382, 264, 31782, 307, 5922, 51337], "temperature": 0.0, "avg_logprob": -0.09023589122144482, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.00013676205708179623}, {"id": 260, "seek": 126066, "start": 1282.02, "end": 1290.5800000000002, "text": " so when the input is 24 the target is 43 or rather 43 here in the y array when the input is 2443", "tokens": [51433, 370, 562, 264, 4846, 307, 4022, 264, 3779, 307, 17914, 420, 2831, 17914, 510, 294, 264, 288, 10225, 562, 264, 4846, 307, 4022, 17201, 51861], "temperature": 0.0, "avg_logprob": -0.09023589122144482, "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.00013676205708179623}, {"id": 261, "seek": 129066, "start": 1290.66, "end": 1299.22, "text": " the target is 58. when the input is 2443 58 the target is 5 etc or like when it is a 52581 the", "tokens": [50365, 264, 3779, 307, 21786, 13, 562, 264, 4846, 307, 4022, 17201, 21786, 264, 3779, 307, 1025, 5183, 420, 411, 562, 309, 307, 257, 1025, 6074, 32875, 264, 50793], "temperature": 0.0, "avg_logprob": -0.08665872162038629, "compression_ratio": 1.7044334975369457, "no_speech_prob": 0.0001978445507120341}, {"id": 262, "seek": 129066, "start": 1299.22, "end": 1306.5800000000002, "text": " target is 58 right so you can sort of see this spelled out these are the 32 independent examples", "tokens": [50793, 3779, 307, 21786, 558, 370, 291, 393, 1333, 295, 536, 341, 34388, 484, 613, 366, 264, 8858, 6695, 5110, 51161], "temperature": 0.0, "avg_logprob": -0.08665872162038629, "compression_ratio": 1.7044334975369457, "no_speech_prob": 0.0001978445507120341}, {"id": 263, "seek": 129066, "start": 1306.5800000000002, "end": 1312.74, "text": " packed in to a single batch of the input x and then the desired targets are in y", "tokens": [51161, 13265, 294, 281, 257, 2167, 15245, 295, 264, 4846, 2031, 293, 550, 264, 14721, 12911, 366, 294, 288, 51469], "temperature": 0.0, "avg_logprob": -0.08665872162038629, "compression_ratio": 1.7044334975369457, "no_speech_prob": 0.0001978445507120341}, {"id": 264, "seek": 129066, "start": 1313.78, "end": 1320.5800000000002, "text": " and so now this integer tensor of x is going to feed into the transformer", "tokens": [51521, 293, 370, 586, 341, 24922, 40863, 295, 2031, 307, 516, 281, 3154, 666, 264, 31782, 51861], "temperature": 0.0, "avg_logprob": -0.08665872162038629, "compression_ratio": 1.7044334975369457, "no_speech_prob": 0.0001978445507120341}, {"id": 265, "seek": 132066, "start": 1321.3000000000002, "end": 1325.8600000000001, "text": " and that transformer is going to simultaneously process all these examples and then look up the", "tokens": [50397, 293, 300, 31782, 307, 516, 281, 16561, 1399, 439, 613, 5110, 293, 550, 574, 493, 264, 50625], "temperature": 0.0, "avg_logprob": -0.03649928530708688, "compression_ratio": 1.9241379310344828, "no_speech_prob": 0.00017560817650519311}, {"id": 266, "seek": 132066, "start": 1325.8600000000001, "end": 1332.26, "text": " correct integers to predict in every one of these positions in the tensor y okay so now", "tokens": [50625, 3006, 41674, 281, 6069, 294, 633, 472, 295, 613, 8432, 294, 264, 40863, 288, 1392, 370, 586, 50945], "temperature": 0.0, "avg_logprob": -0.03649928530708688, "compression_ratio": 1.9241379310344828, "no_speech_prob": 0.00017560817650519311}, {"id": 267, "seek": 132066, "start": 1332.26, "end": 1336.66, "text": " that we have our batch of input that we'd like to feed into a transformer let's start basically", "tokens": [50945, 300, 321, 362, 527, 15245, 295, 4846, 300, 321, 1116, 411, 281, 3154, 666, 257, 31782, 718, 311, 722, 1936, 51165], "temperature": 0.0, "avg_logprob": -0.03649928530708688, "compression_ratio": 1.9241379310344828, "no_speech_prob": 0.00017560817650519311}, {"id": 268, "seek": 132066, "start": 1336.66, "end": 1341.0600000000002, "text": " feeding this into neural networks now we're going to start off with the simplest possible", "tokens": [51165, 12919, 341, 666, 18161, 9590, 586, 321, 434, 516, 281, 722, 766, 365, 264, 22811, 1944, 51385], "temperature": 0.0, "avg_logprob": -0.03649928530708688, "compression_ratio": 1.9241379310344828, "no_speech_prob": 0.00017560817650519311}, {"id": 269, "seek": 132066, "start": 1341.0600000000002, "end": 1344.66, "text": " neural network which in the case of language modeling in my opinion is the bigram language", "tokens": [51385, 18161, 3209, 597, 294, 264, 1389, 295, 2856, 15983, 294, 452, 4800, 307, 264, 955, 2356, 2856, 51565], "temperature": 0.0, "avg_logprob": -0.03649928530708688, "compression_ratio": 1.9241379310344828, "no_speech_prob": 0.00017560817650519311}, {"id": 270, "seek": 132066, "start": 1344.66, "end": 1350.1000000000001, "text": " model and we've covered the bigram language model in my make more series in a lot of depth and so", "tokens": [51565, 2316, 293, 321, 600, 5343, 264, 955, 2356, 2856, 2316, 294, 452, 652, 544, 2638, 294, 257, 688, 295, 7161, 293, 370, 51837], "temperature": 0.0, "avg_logprob": -0.03649928530708688, "compression_ratio": 1.9241379310344828, "no_speech_prob": 0.00017560817650519311}, {"id": 271, "seek": 135010, "start": 1350.1, "end": 1354.98, "text": " here i'm going to sort of go faster and let's just implement the pytorch module directly that", "tokens": [50365, 510, 741, 478, 516, 281, 1333, 295, 352, 4663, 293, 718, 311, 445, 4445, 264, 25878, 284, 339, 10088, 3838, 300, 50609], "temperature": 0.2, "avg_logprob": -0.13426646901600398, "compression_ratio": 2.3119658119658117, "no_speech_prob": 0.0003135084407404065}, {"id": 272, "seek": 135010, "start": 1354.98, "end": 1360.74, "text": " implements the bigram language model so i'm importing the pytorch nn module", "tokens": [50609, 704, 17988, 264, 955, 2356, 2856, 2316, 370, 741, 478, 43866, 264, 25878, 284, 339, 297, 77, 10088, 50897], "temperature": 0.2, "avg_logprob": -0.13426646901600398, "compression_ratio": 2.3119658119658117, "no_speech_prob": 0.0003135084407404065}, {"id": 273, "seek": 135010, "start": 1362.1799999999998, "end": 1367.06, "text": " for reproducibility and then here i'm constructing a bigram language model which is a subclass of", "tokens": [50969, 337, 11408, 537, 39802, 293, 550, 510, 741, 478, 39969, 257, 955, 2356, 2856, 2316, 597, 307, 257, 1422, 11665, 295, 51213], "temperature": 0.2, "avg_logprob": -0.13426646901600398, "compression_ratio": 2.3119658119658117, "no_speech_prob": 0.0003135084407404065}, {"id": 274, "seek": 135010, "start": 1367.06, "end": 1372.74, "text": " nn module and then i'm calling it and i'm passing in the inputs and the targets", "tokens": [51213, 297, 77, 10088, 293, 550, 741, 478, 5141, 309, 293, 741, 478, 8437, 294, 264, 15743, 293, 264, 12911, 51497], "temperature": 0.2, "avg_logprob": -0.13426646901600398, "compression_ratio": 2.3119658119658117, "no_speech_prob": 0.0003135084407404065}, {"id": 275, "seek": 135010, "start": 1373.6999999999998, "end": 1378.4199999999998, "text": " and i'm just printing now when the inputs and targets come here you see that i'm just taking the", "tokens": [51545, 293, 741, 478, 445, 14699, 586, 562, 264, 15743, 293, 12911, 808, 510, 291, 536, 300, 741, 478, 445, 1940, 264, 51781], "temperature": 0.2, "avg_logprob": -0.13426646901600398, "compression_ratio": 2.3119658119658117, "no_speech_prob": 0.0003135084407404065}, {"id": 276, "seek": 135010, "start": 1378.4199999999998, "end": 1380.02, "text": " index the inputs and targets and then i'm just printing the inputs and targets and then i'm just", "tokens": [51781, 8186, 264, 15743, 293, 12911, 293, 550, 741, 478, 445, 14699, 264, 15743, 293, 12911, 293, 550, 741, 478, 445, 51861], "temperature": 0.2, "avg_logprob": -0.13426646901600398, "compression_ratio": 2.3119658119658117, "no_speech_prob": 0.0003135084407404065}, {"id": 277, "seek": 138002, "start": 1380.02, "end": 1384.42, "text": " printing the inputs x here which i rename to idx and i'm just passing them into this token", "tokens": [50365, 14699, 264, 15743, 2031, 510, 597, 741, 36741, 281, 4496, 87, 293, 741, 478, 445, 8437, 552, 666, 341, 14862, 50585], "temperature": 0.0, "avg_logprob": -0.08568752223047717, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.0008175506955012679}, {"id": 278, "seek": 138002, "start": 1384.42, "end": 1390.58, "text": " embedding table so what's going on here is that here in the constructor we are creating a token", "tokens": [50585, 12240, 3584, 3199, 370, 437, 311, 516, 322, 510, 307, 300, 510, 294, 264, 47479, 321, 366, 4084, 257, 14862, 50893], "temperature": 0.0, "avg_logprob": -0.08568752223047717, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.0008175506955012679}, {"id": 279, "seek": 138002, "start": 1390.58, "end": 1397.94, "text": " embedding table and it is of size vocab size by vocab size and we're using an endot embedding", "tokens": [50893, 12240, 3584, 3199, 293, 309, 307, 295, 2744, 2329, 455, 2744, 538, 2329, 455, 2744, 293, 321, 434, 1228, 364, 917, 310, 12240, 3584, 51261], "temperature": 0.0, "avg_logprob": -0.08568752223047717, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.0008175506955012679}, {"id": 280, "seek": 138002, "start": 1397.94, "end": 1403.3, "text": " which is a very thin wrapper around basically a tensor of shape vocab size by vocab size", "tokens": [51261, 597, 307, 257, 588, 5862, 46906, 926, 1936, 257, 40863, 295, 3909, 2329, 455, 2744, 538, 2329, 455, 2744, 51529], "temperature": 0.0, "avg_logprob": -0.08568752223047717, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.0008175506955012679}, {"id": 281, "seek": 138002, "start": 1404.02, "end": 1410.02, "text": " and what's happening here is that when we pass idx here every single integer in our input is going to", "tokens": [51565, 293, 437, 311, 2737, 510, 307, 300, 562, 321, 1320, 4496, 87, 510, 633, 2167, 24922, 294, 527, 4846, 307, 516, 281, 51865], "temperature": 0.0, "avg_logprob": -0.08568752223047717, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.0008175506955012679}, {"id": 282, "seek": 141002, "start": 1410.02, "end": 1415.3799999999999, "text": " refer to this embedding table and is going to pluck out a row of that embedding table corresponding", "tokens": [50365, 2864, 281, 341, 12240, 3584, 3199, 293, 307, 516, 281, 41514, 484, 257, 5386, 295, 300, 12240, 3584, 3199, 11760, 50633], "temperature": 0.0, "avg_logprob": -0.04038758277893066, "compression_ratio": 1.955223880597015, "no_speech_prob": 0.0005247395602054894}, {"id": 283, "seek": 141002, "start": 1415.3799999999999, "end": 1422.74, "text": " to its index so 24 here will go to the embedding table and we'll pluck out the 24th row and then 43", "tokens": [50633, 281, 1080, 8186, 370, 4022, 510, 486, 352, 281, 264, 12240, 3584, 3199, 293, 321, 603, 41514, 484, 264, 4022, 392, 5386, 293, 550, 17914, 51001], "temperature": 0.0, "avg_logprob": -0.04038758277893066, "compression_ratio": 1.955223880597015, "no_speech_prob": 0.0005247395602054894}, {"id": 284, "seek": 141002, "start": 1422.74, "end": 1428.42, "text": " will go here and pluck out the 43rd row etc and then pytorch is going to arrange all of this into", "tokens": [51001, 486, 352, 510, 293, 41514, 484, 264, 17914, 7800, 5386, 5183, 293, 550, 25878, 284, 339, 307, 516, 281, 9424, 439, 295, 341, 666, 51285], "temperature": 0.0, "avg_logprob": -0.04038758277893066, "compression_ratio": 1.955223880597015, "no_speech_prob": 0.0005247395602054894}, {"id": 285, "seek": 141002, "start": 1428.42, "end": 1438.5, "text": " a batch by time by channel tensor in this case batch is 4 time is 8 and c which is the channels", "tokens": [51285, 257, 15245, 538, 565, 538, 2269, 40863, 294, 341, 1389, 15245, 307, 1017, 565, 307, 1649, 293, 269, 597, 307, 264, 9235, 51789], "temperature": 0.0, "avg_logprob": -0.04038758277893066, "compression_ratio": 1.955223880597015, "no_speech_prob": 0.0005247395602054894}, {"id": 286, "seek": 144002, "start": 1440.74, "end": 1444.9, "text": " and so we're just going to pluck out all those rows arrange them in a b by t by c", "tokens": [50401, 293, 370, 321, 434, 445, 516, 281, 41514, 484, 439, 729, 13241, 9424, 552, 294, 257, 272, 538, 256, 538, 269, 50609], "temperature": 0.0, "avg_logprob": -0.06031329123700251, "compression_ratio": 1.7951388888888888, "no_speech_prob": 0.0002671998809091747}, {"id": 287, "seek": 144002, "start": 1445.7, "end": 1449.54, "text": " and now we're going to interpret this as the logits which are basically the scores", "tokens": [50649, 293, 586, 321, 434, 516, 281, 7302, 341, 382, 264, 3565, 1208, 597, 366, 1936, 264, 13444, 50841], "temperature": 0.0, "avg_logprob": -0.06031329123700251, "compression_ratio": 1.7951388888888888, "no_speech_prob": 0.0002671998809091747}, {"id": 288, "seek": 144002, "start": 1450.1, "end": 1454.26, "text": " for the next character in a sequence and so what's happening here is", "tokens": [50869, 337, 264, 958, 2517, 294, 257, 8310, 293, 370, 437, 311, 2737, 510, 307, 51077], "temperature": 0.0, "avg_logprob": -0.06031329123700251, "compression_ratio": 1.7951388888888888, "no_speech_prob": 0.0002671998809091747}, {"id": 289, "seek": 144002, "start": 1454.26, "end": 1459.3, "text": " we are predicting what comes next based on just the individual identity of a single token", "tokens": [51077, 321, 366, 32884, 437, 1487, 958, 2361, 322, 445, 264, 2609, 6575, 295, 257, 2167, 14862, 51329], "temperature": 0.0, "avg_logprob": -0.06031329123700251, "compression_ratio": 1.7951388888888888, "no_speech_prob": 0.0002671998809091747}, {"id": 290, "seek": 144002, "start": 1459.94, "end": 1464.74, "text": " and you can do that because um i mean currently the tokens are not talking to each other and", "tokens": [51361, 293, 291, 393, 360, 300, 570, 1105, 741, 914, 4362, 264, 22667, 366, 406, 1417, 281, 1184, 661, 293, 51601], "temperature": 0.0, "avg_logprob": -0.06031329123700251, "compression_ratio": 1.7951388888888888, "no_speech_prob": 0.0002671998809091747}, {"id": 291, "seek": 144002, "start": 1464.74, "end": 1469.94, "text": " they're not seeing any context except for they're just seeing themselves so i'm a i'm a token number", "tokens": [51601, 436, 434, 406, 2577, 604, 4319, 3993, 337, 436, 434, 445, 2577, 2969, 370, 741, 478, 257, 741, 478, 257, 14862, 1230, 51861], "temperature": 0.0, "avg_logprob": -0.06031329123700251, "compression_ratio": 1.7951388888888888, "no_speech_prob": 0.0002671998809091747}, {"id": 292, "seek": 147002, "start": 1470.02, "end": 1475.22, "text": " five and then i can actually make pretty decent predictions about what comes next just by knowing", "tokens": [50365, 1732, 293, 550, 741, 393, 767, 652, 1238, 8681, 21264, 466, 437, 1487, 958, 445, 538, 5276, 50625], "temperature": 1.0, "avg_logprob": -1.1302468003026698, "compression_ratio": 2.0492753623188404, "no_speech_prob": 0.00022298109252005816}, {"id": 293, "seek": 147002, "start": 1475.22, "end": 1482.58, "text": " that i'm token 5 because some characters know um follow other characters in typical scenarios", "tokens": [50625, 300, 741, 478, 14862, 1025, 570, 512, 4342, 458, 1105, 1524, 661, 4342, 294, 7476, 15077, 50993], "temperature": 1.0, "avg_logprob": -1.1302468003026698, "compression_ratio": 2.0492753623188404, "no_speech_prob": 0.00022298109252005816}, {"id": 294, "seek": 147002, "start": 1482.58, "end": 1487.3, "text": " so we saw a lot of this in a lot more depth in the make more series and here if i just run this", "tokens": [50993, 370, 321, 1866, 257, 688, 295, 341, 294, 257, 688, 544, 7161, 294, 264, 652, 544, 2638, 293, 510, 498, 741, 445, 1190, 341, 51229], "temperature": 1.0, "avg_logprob": -1.1302468003026698, "compression_ratio": 2.0492753623188404, "no_speech_prob": 0.00022298109252005816}, {"id": 295, "seek": 147002, "start": 1488.02, "end": 1494.42, "text": " then we currently get the predictions the scores the logits for every one of the four by eight", "tokens": [51265, 550, 321, 4362, 483, 264, 21264, 264, 13444, 264, 3565, 1208, 337, 633, 472, 295, 264, 1451, 538, 3180, 51585], "temperature": 1.0, "avg_logprob": -1.1302468003026698, "compression_ratio": 2.0492753623188404, "no_speech_prob": 0.00022298109252005816}, {"id": 296, "seek": 147002, "start": 1494.42, "end": 1498.74, "text": " positions now that we've made predictions about what comes next we'd like to evaluate the loss", "tokens": [51585, 8432, 586, 300, 321, 600, 1027, 21264, 466, 437, 1487, 958, 321, 1116, 411, 281, 13059, 264, 4470, 51801], "temperature": 1.0, "avg_logprob": -1.1302468003026698, "compression_ratio": 2.0492753623188404, "no_speech_prob": 0.00022298109252005816}, {"id": 297, "seek": 147002, "start": 1498.74, "end": 1500.0, "text": " function and so in this case we want to make predictions about how would the loss function Te intentions are impact the sc noises you would get this this existence and then we oriented out we would get the loss function and so at", "tokens": [51801, 2445, 293, 370, 294, 341, 1389, 321, 528, 281, 652, 21264, 466, 577, 576, 264, 4470, 2445, 1989, 19354, 366, 2712, 264, 795, 14620, 291, 576, 483, 341, 341, 9123, 293, 550, 321, 21841, 484, 321, 576, 483, 264, 4470, 2445, 293, 370, 412, 51864], "temperature": 1.0, "avg_logprob": -1.1302468003026698, "compression_ratio": 2.0492753623188404, "no_speech_prob": 0.00022298109252005816}, {"id": 298, "seek": 150002, "start": 1500.02, "end": 1505.68, "text": " make more series we saw that a good way to measure a loss or like a quality of the predictions is to", "tokens": [50365, 652, 544, 2638, 321, 1866, 300, 257, 665, 636, 281, 3481, 257, 4470, 420, 411, 257, 3125, 295, 264, 21264, 307, 281, 50648], "temperature": 0.0, "avg_logprob": -0.0445404052734375, "compression_ratio": 1.8910505836575875, "no_speech_prob": 0.07733619958162308}, {"id": 299, "seek": 150002, "start": 1505.68, "end": 1510.42, "text": " use the negative log likelihood loss which is also implemented in PyTorch under the name cross", "tokens": [50648, 764, 264, 3671, 3565, 22119, 4470, 597, 307, 611, 12270, 294, 9953, 51, 284, 339, 833, 264, 1315, 3278, 50885], "temperature": 0.0, "avg_logprob": -0.0445404052734375, "compression_ratio": 1.8910505836575875, "no_speech_prob": 0.07733619958162308}, {"id": 300, "seek": 150002, "start": 1510.42, "end": 1517.8, "text": " entropy. So what we'd like to do here is loss is the cross entropy on the predictions and the", "tokens": [50885, 30867, 13, 407, 437, 321, 1116, 411, 281, 360, 510, 307, 4470, 307, 264, 3278, 30867, 322, 264, 21264, 293, 264, 51254], "temperature": 0.0, "avg_logprob": -0.0445404052734375, "compression_ratio": 1.8910505836575875, "no_speech_prob": 0.07733619958162308}, {"id": 301, "seek": 150002, "start": 1517.8, "end": 1523.26, "text": " targets and so this measures the quality of the logits with respect to the targets. In other words", "tokens": [51254, 12911, 293, 370, 341, 8000, 264, 3125, 295, 264, 3565, 1208, 365, 3104, 281, 264, 12911, 13, 682, 661, 2283, 51527], "temperature": 0.0, "avg_logprob": -0.0445404052734375, "compression_ratio": 1.8910505836575875, "no_speech_prob": 0.07733619958162308}, {"id": 302, "seek": 150002, "start": 1523.26, "end": 1528.96, "text": " we have the identity of the next character so how well are we predicting the next character based", "tokens": [51527, 321, 362, 264, 6575, 295, 264, 958, 2517, 370, 577, 731, 366, 321, 32884, 264, 958, 2517, 2361, 51812], "temperature": 0.0, "avg_logprob": -0.0445404052734375, "compression_ratio": 1.8910505836575875, "no_speech_prob": 0.07733619958162308}, {"id": 303, "seek": 152896, "start": 1528.96, "end": 1536.88, "text": " on the logits and intuitively the correct dimension of logits depending on whatever", "tokens": [50365, 322, 264, 3565, 1208, 293, 46506, 264, 3006, 10139, 295, 3565, 1208, 5413, 322, 2035, 50761], "temperature": 0.0, "avg_logprob": -0.10560424960389429, "compression_ratio": 1.9859154929577465, "no_speech_prob": 1.8828772226697765e-05}, {"id": 304, "seek": 152896, "start": 1536.88, "end": 1541.04, "text": " the target is should have a very high number and all the other dimensions should be very low number", "tokens": [50761, 264, 3779, 307, 820, 362, 257, 588, 1090, 1230, 293, 439, 264, 661, 12819, 820, 312, 588, 2295, 1230, 50969], "temperature": 0.0, "avg_logprob": -0.10560424960389429, "compression_ratio": 1.9859154929577465, "no_speech_prob": 1.8828772226697765e-05}, {"id": 305, "seek": 152896, "start": 1541.04, "end": 1546.92, "text": " right. Now the issue is that this won't actually this is what we want we want to basically output", "tokens": [50969, 558, 13, 823, 264, 2734, 307, 300, 341, 1582, 380, 767, 341, 307, 437, 321, 528, 321, 528, 281, 1936, 5598, 51263], "temperature": 0.0, "avg_logprob": -0.10560424960389429, "compression_ratio": 1.9859154929577465, "no_speech_prob": 1.8828772226697765e-05}, {"id": 306, "seek": 152896, "start": 1546.92, "end": 1556.4, "text": " the logits and the loss this is what we want but unfortunately this won't actually run we get an", "tokens": [51263, 264, 3565, 1208, 293, 264, 4470, 341, 307, 437, 321, 528, 457, 7015, 341, 1582, 380, 767, 1190, 321, 483, 364, 51737], "temperature": 0.0, "avg_logprob": -0.10560424960389429, "compression_ratio": 1.9859154929577465, "no_speech_prob": 1.8828772226697765e-05}, {"id": 307, "seek": 152896, "start": 1556.4, "end": 1558.54, "text": " error message but intuitively we want to", "tokens": [51737, 6713, 3636, 457, 46506, 321, 528, 281, 51844], "temperature": 0.0, "avg_logprob": -0.10560424960389429, "compression_ratio": 1.9859154929577465, "no_speech_prob": 1.8828772226697765e-05}, {"id": 308, "seek": 152896, "start": 1558.54, "end": 1558.94, "text": " you", "tokens": [51844, 291, 51864], "temperature": 0.0, "avg_logprob": -0.10560424960389429, "compression_ratio": 1.9859154929577465, "no_speech_prob": 1.8828772226697765e-05}, {"id": 309, "seek": 155894, "start": 1558.94, "end": 1565.64, "text": " measure this. Now when we go to the PyTorch cross entropy documentation here", "tokens": [50365, 3481, 341, 13, 823, 562, 321, 352, 281, 264, 9953, 51, 284, 339, 3278, 30867, 14333, 510, 50700], "temperature": 0.0, "avg_logprob": -0.096948922570072, "compression_ratio": 2.12992125984252, "no_speech_prob": 0.000570898235309869}, {"id": 310, "seek": 155894, "start": 1565.64, "end": 1571.76, "text": " we're trying to call the cross entropy in its functional form so that means we don't have to", "tokens": [50700, 321, 434, 1382, 281, 818, 264, 3278, 30867, 294, 1080, 11745, 1254, 370, 300, 1355, 321, 500, 380, 362, 281, 51006], "temperature": 0.0, "avg_logprob": -0.096948922570072, "compression_ratio": 2.12992125984252, "no_speech_prob": 0.000570898235309869}, {"id": 311, "seek": 155894, "start": 1571.76, "end": 1577.5, "text": " create like a module for it but here when we go to the documentation you have to look into the", "tokens": [51006, 1884, 411, 257, 10088, 337, 309, 457, 510, 562, 321, 352, 281, 264, 14333, 291, 362, 281, 574, 666, 264, 51293], "temperature": 0.0, "avg_logprob": -0.096948922570072, "compression_ratio": 2.12992125984252, "no_speech_prob": 0.000570898235309869}, {"id": 312, "seek": 155894, "start": 1577.5, "end": 1583.14, "text": " details of how PyTorch expects these inputs and basically the issue here is PyTorch expects", "tokens": [51293, 4365, 295, 577, 9953, 51, 284, 339, 33280, 613, 15743, 293, 1936, 264, 2734, 510, 307, 9953, 51, 284, 339, 33280, 51575], "temperature": 0.0, "avg_logprob": -0.096948922570072, "compression_ratio": 2.12992125984252, "no_speech_prob": 0.000570898235309869}, {"id": 313, "seek": 155894, "start": 1583.14, "end": 1587.8200000000002, "text": " if you have multi-dimensional input which we do because we have a b by t by c tensor", "tokens": [51575, 498, 291, 362, 4825, 12, 18759, 4846, 597, 321, 360, 570, 321, 362, 257, 272, 538, 256, 538, 269, 40863, 51809], "temperature": 0.0, "avg_logprob": -0.096948922570072, "compression_ratio": 2.12992125984252, "no_speech_prob": 0.000570898235309869}, {"id": 314, "seek": 155894, "start": 1587.8200000000002, "end": 1588.52, "text": " then it actually expects a multi-dimensional input which we do because we have a b by t by c tensor", "tokens": [51809, 550, 309, 767, 33280, 257, 4825, 12, 18759, 4846, 597, 321, 360, 570, 321, 362, 257, 272, 538, 256, 538, 269, 40863, 51844], "temperature": 0.0, "avg_logprob": -0.096948922570072, "compression_ratio": 2.12992125984252, "no_speech_prob": 0.000570898235309869}, {"id": 315, "seek": 158852, "start": 1588.52, "end": 1595.22, "text": " then it actually really wants the channels to be the second dimension here so if you", "tokens": [50365, 550, 309, 767, 534, 2738, 264, 9235, 281, 312, 264, 1150, 10139, 510, 370, 498, 291, 50700], "temperature": 1.0, "avg_logprob": -0.29433122114701704, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.001923919771797955}, {"id": 316, "seek": 158852, "start": 1596.62, "end": 1604.62, "text": " so basically it wants a b by c by t instead of a b by t by c and so just the details of how PyTorch", "tokens": [50770, 370, 1936, 309, 2738, 257, 272, 538, 269, 538, 256, 2602, 295, 257, 272, 538, 256, 538, 269, 293, 370, 445, 264, 4365, 295, 577, 9953, 51, 284, 339, 51170], "temperature": 1.0, "avg_logprob": -0.29433122114701704, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.001923919771797955}, {"id": 317, "seek": 158852, "start": 1604.62, "end": 1611.3799999999999, "text": " treats these kinds of inputs and so we don't actually want to deal with that so what we're", "tokens": [51170, 19566, 613, 3685, 295, 15743, 293, 370, 321, 500, 380, 767, 528, 281, 2028, 365, 300, 370, 437, 321, 434, 51508], "temperature": 1.0, "avg_logprob": -0.29433122114701704, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.001923919771797955}, {"id": 318, "seek": 158852, "start": 1611.3799999999999, "end": 1615.52, "text": " going to do instead is we need to basically reshape our logits. So here's what I like to do I", "tokens": [51508, 516, 281, 360, 2602, 307, 321, 643, 281, 1936, 725, 42406, 527, 3565, 1208, 13, 407, 510, 311, 437, 286, 411, 281, 360, 286, 51715], "temperature": 1.0, "avg_logprob": -0.29433122114701704, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.001923919771797955}, {"id": 319, "seek": 158852, "start": 1615.52, "end": 1618.42, "text": " like to take basically give names to the dimensions", "tokens": [51715, 411, 281, 747, 1936, 976, 5288, 281, 264, 12819, 51860], "temperature": 1.0, "avg_logprob": -0.29433122114701704, "compression_ratio": 1.8146551724137931, "no_speech_prob": 0.001923919771797955}, {"id": 320, "seek": 161842, "start": 1618.42, "end": 1622.66, "text": " So logits.shape is B by T by C and unpack those numbers.", "tokens": [50365, 407, 3565, 1208, 13, 82, 42406, 307, 363, 538, 314, 538, 383, 293, 26699, 729, 3547, 13, 50577], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 321, "seek": 161842, "start": 1622.66, "end": 1627.24, "text": " And then let's say that logits equals logits.view.", "tokens": [50577, 400, 550, 718, 311, 584, 300, 3565, 1208, 6915, 3565, 1208, 13, 1759, 13, 50806], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 322, "seek": 161842, "start": 1627.24, "end": 1630.78, "text": " And we want it to be a B times C, B times T by C.", "tokens": [50806, 400, 321, 528, 309, 281, 312, 257, 363, 1413, 383, 11, 363, 1413, 314, 538, 383, 13, 50983], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 323, "seek": 161842, "start": 1630.78, "end": 1634.0, "text": " So just a two-dimensional array, right?", "tokens": [50983, 407, 445, 257, 732, 12, 18759, 10225, 11, 558, 30, 51144], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 324, "seek": 161842, "start": 1634.0, "end": 1636.1200000000001, "text": " So we're going to take all the,", "tokens": [51144, 407, 321, 434, 516, 281, 747, 439, 264, 11, 51250], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 325, "seek": 161842, "start": 1636.1200000000001, "end": 1639.8400000000001, "text": " we're going to take all of these positions here", "tokens": [51250, 321, 434, 516, 281, 747, 439, 295, 613, 8432, 510, 51436], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 326, "seek": 161842, "start": 1639.8400000000001, "end": 1641.48, "text": " and we're going to stretch them out", "tokens": [51436, 293, 321, 434, 516, 281, 5985, 552, 484, 51518], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 327, "seek": 161842, "start": 1641.48, "end": 1643.78, "text": " in a one-dimensional sequence", "tokens": [51518, 294, 257, 472, 12, 18759, 8310, 51633], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 328, "seek": 161842, "start": 1643.78, "end": 1646.8400000000001, "text": " and preserve the channel dimension as the second dimension.", "tokens": [51633, 293, 15665, 264, 2269, 10139, 382, 264, 1150, 10139, 13, 51786], "temperature": 0.0, "avg_logprob": -0.13846857685688113, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.021826565265655518}, {"id": 329, "seek": 164684, "start": 1646.84, "end": 1649.4599999999998, "text": " So we're just kind of like stretching out the array", "tokens": [50365, 407, 321, 434, 445, 733, 295, 411, 19632, 484, 264, 10225, 50496], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 330, "seek": 164684, "start": 1649.4599999999998, "end": 1650.84, "text": " so it's two-dimensional.", "tokens": [50496, 370, 309, 311, 732, 12, 18759, 13, 50565], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 331, "seek": 164684, "start": 1650.84, "end": 1652.8799999999999, "text": " And in that case, it's going to better conform", "tokens": [50565, 400, 294, 300, 1389, 11, 309, 311, 516, 281, 1101, 18975, 50667], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 332, "seek": 164684, "start": 1652.8799999999999, "end": 1656.3799999999999, "text": " to what PyTorch sort of expects in its dimensions.", "tokens": [50667, 281, 437, 9953, 51, 284, 339, 1333, 295, 33280, 294, 1080, 12819, 13, 50842], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 333, "seek": 164684, "start": 1656.3799999999999, "end": 1658.6, "text": " Now we have to do the same to targets", "tokens": [50842, 823, 321, 362, 281, 360, 264, 912, 281, 12911, 50953], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 334, "seek": 164684, "start": 1658.6, "end": 1663.6, "text": " because currently targets are of shape B by T", "tokens": [50953, 570, 4362, 12911, 366, 295, 3909, 363, 538, 314, 51203], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 335, "seek": 164684, "start": 1664.72, "end": 1666.98, "text": " and we want it to be just B times T.", "tokens": [51259, 293, 321, 528, 309, 281, 312, 445, 363, 1413, 314, 13, 51372], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 336, "seek": 164684, "start": 1666.98, "end": 1668.62, "text": " So one-dimensional.", "tokens": [51372, 407, 472, 12, 18759, 13, 51454], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 337, "seek": 164684, "start": 1668.62, "end": 1671.86, "text": " Now, alternatively, you could always still just do minus one", "tokens": [51454, 823, 11, 8535, 356, 11, 291, 727, 1009, 920, 445, 360, 3175, 472, 51616], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 338, "seek": 164684, "start": 1671.86, "end": 1674.08, "text": " because PyTorch will guess what this should be", "tokens": [51616, 570, 9953, 51, 284, 339, 486, 2041, 437, 341, 820, 312, 51727], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 339, "seek": 164684, "start": 1674.08, "end": 1675.3799999999999, "text": " if you want to lay it out.", "tokens": [51727, 498, 291, 528, 281, 2360, 309, 484, 13, 51792], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 340, "seek": 164684, "start": 1675.3799999999999, "end": 1676.5, "text": " But let me just be explicit", "tokens": [51792, 583, 718, 385, 445, 312, 13691, 51848], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 341, "seek": 164684, "start": 1676.5, "end": 1676.82, "text": " and say if you can see it.", "tokens": [51848, 293, 584, 498, 291, 393, 536, 309, 13, 51864], "temperature": 0.0, "avg_logprob": -0.16060592651367187, "compression_ratio": 1.6889632107023411, "no_speech_prob": 0.00018857343820855021}, {"id": 342, "seek": 167682, "start": 1676.82, "end": 1678.2, "text": " If you can see it, it's going to be B times T.", "tokens": [50365, 759, 291, 393, 536, 309, 11, 309, 311, 516, 281, 312, 363, 1413, 314, 13, 50434], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 343, "seek": 167682, "start": 1678.2, "end": 1679.72, "text": " Once we reshape this,", "tokens": [50434, 3443, 321, 725, 42406, 341, 11, 50510], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 344, "seek": 167682, "start": 1679.72, "end": 1682.8999999999999, "text": " it will match the cross-entropy case", "tokens": [50510, 309, 486, 2995, 264, 3278, 12, 317, 27514, 1389, 50669], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 345, "seek": 167682, "start": 1682.8999999999999, "end": 1685.2, "text": " and then we should be able to evaluate our loss.", "tokens": [50669, 293, 550, 321, 820, 312, 1075, 281, 13059, 527, 4470, 13, 50784], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 346, "seek": 167682, "start": 1687.1, "end": 1691.0, "text": " Okay, so that right now, and we can do loss.", "tokens": [50879, 1033, 11, 370, 300, 558, 586, 11, 293, 321, 393, 360, 4470, 13, 51074], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 347, "seek": 167682, "start": 1691.0, "end": 1694.62, "text": " And so currently we see that the loss is 4.87.", "tokens": [51074, 400, 370, 4362, 321, 536, 300, 264, 4470, 307, 1017, 13, 23853, 13, 51255], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 348, "seek": 167682, "start": 1694.62, "end": 1699.1, "text": " Now, because we have 65 possible vocabulary elements,", "tokens": [51255, 823, 11, 570, 321, 362, 11624, 1944, 19864, 4959, 11, 51479], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 349, "seek": 167682, "start": 1699.1, "end": 1701.6799999999998, "text": " we can actually guess at what the loss should be.", "tokens": [51479, 321, 393, 767, 2041, 412, 437, 264, 4470, 820, 312, 13, 51608], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 350, "seek": 167682, "start": 1701.6799999999998, "end": 1703.2, "text": " And in particular,", "tokens": [51608, 400, 294, 1729, 11, 51684], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 351, "seek": 167682, "start": 1703.2, "end": 1705.9199999999998, "text": " we covered negative log likelihood in a lot of detail.", "tokens": [51684, 321, 5343, 3671, 3565, 22119, 294, 257, 688, 295, 2607, 13, 51820], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 352, "seek": 167682, "start": 1705.9199999999998, "end": 1706.48, "text": " We are expecting,", "tokens": [51820, 492, 366, 9650, 11, 51848], "temperature": 0.0, "avg_logprob": -0.23606760447261899, "compression_ratio": 1.6014492753623188, "no_speech_prob": 0.0005819842335768044}, {"id": 353, "seek": 170648, "start": 1706.48, "end": 1711.48, "text": " we're expecting log or lon of one over 65", "tokens": [50365, 321, 434, 9650, 3565, 420, 9155, 295, 472, 670, 11624, 50615], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 354, "seek": 170648, "start": 1712.38, "end": 1713.94, "text": " and negative of that.", "tokens": [50660, 293, 3671, 295, 300, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 355, "seek": 170648, "start": 1713.94, "end": 1717.52, "text": " So we're expecting the loss to be about 4.17,", "tokens": [50738, 407, 321, 434, 9650, 264, 4470, 281, 312, 466, 1017, 13, 7773, 11, 50917], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 356, "seek": 170648, "start": 1717.52, "end": 1719.26, "text": " but we're getting 4.87.", "tokens": [50917, 457, 321, 434, 1242, 1017, 13, 23853, 13, 51004], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 357, "seek": 170648, "start": 1719.26, "end": 1721.2, "text": " And so that's telling us that the initial predictions", "tokens": [51004, 400, 370, 300, 311, 3585, 505, 300, 264, 5883, 21264, 51101], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 358, "seek": 170648, "start": 1721.2, "end": 1723.06, "text": " are not super diffuse.", "tokens": [51101, 366, 406, 1687, 42165, 13, 51194], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 359, "seek": 170648, "start": 1723.06, "end": 1724.76, "text": " They've got a little bit of entropy.", "tokens": [51194, 814, 600, 658, 257, 707, 857, 295, 30867, 13, 51279], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 360, "seek": 170648, "start": 1724.76, "end": 1726.16, "text": " And so we're guessing wrong.", "tokens": [51279, 400, 370, 321, 434, 17939, 2085, 13, 51349], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 361, "seek": 170648, "start": 1727.2, "end": 1732.2, "text": " So yes, but actually we are able to evaluate the loss.", "tokens": [51401, 407, 2086, 11, 457, 767, 321, 366, 1075, 281, 13059, 264, 4470, 13, 51651], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 362, "seek": 170648, "start": 1732.8, "end": 1735.78, "text": " Okay, so now that we can evaluate the quality of the model", "tokens": [51681, 1033, 11, 370, 586, 300, 321, 393, 13059, 264, 3125, 295, 264, 2316, 51830], "temperature": 0.0, "avg_logprob": -0.17994603696076766, "compression_ratio": 1.625, "no_speech_prob": 0.0010185320861637592}, {"id": 363, "seek": 173578, "start": 1735.78, "end": 1737.18, "text": " on some data,", "tokens": [50365, 322, 512, 1412, 11, 50435], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 364, "seek": 173578, "start": 1737.18, "end": 1739.3, "text": " we'd like to also be able to generate from the model.", "tokens": [50435, 321, 1116, 411, 281, 611, 312, 1075, 281, 8460, 490, 264, 2316, 13, 50541], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 365, "seek": 173578, "start": 1739.3, "end": 1741.2, "text": " So let's do the generation.", "tokens": [50541, 407, 718, 311, 360, 264, 5125, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 366, "seek": 173578, "start": 1741.2, "end": 1743.04, "text": " Now I'm going to go again a little bit faster here", "tokens": [50636, 823, 286, 478, 516, 281, 352, 797, 257, 707, 857, 4663, 510, 50728], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 367, "seek": 173578, "start": 1743.04, "end": 1746.68, "text": " because I covered all this already in the previous videos.", "tokens": [50728, 570, 286, 5343, 439, 341, 1217, 294, 264, 3894, 2145, 13, 50910], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 368, "seek": 173578, "start": 1746.68, "end": 1750.24, "text": " So here's a generate function for the model.", "tokens": [50910, 407, 510, 311, 257, 8460, 2445, 337, 264, 2316, 13, 51088], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 369, "seek": 173578, "start": 1752.42, "end": 1753.8, "text": " So we take some,", "tokens": [51197, 407, 321, 747, 512, 11, 51266], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 370, "seek": 173578, "start": 1753.8, "end": 1757.46, "text": " we take the same kind of input IDX here.", "tokens": [51266, 321, 747, 264, 912, 733, 295, 4846, 7348, 55, 510, 13, 51449], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 371, "seek": 173578, "start": 1757.46, "end": 1762.46, "text": " And basically this is the current context of some characters", "tokens": [51449, 400, 1936, 341, 307, 264, 2190, 4319, 295, 512, 4342, 51699], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 372, "seek": 173578, "start": 1763.24, "end": 1765.18, "text": " in a batch, in some batch.", "tokens": [51738, 294, 257, 15245, 11, 294, 512, 15245, 13, 51835], "temperature": 0.0, "avg_logprob": -0.15013085092817033, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0005770438001491129}, {"id": 373, "seek": 176578, "start": 1765.78, "end": 1767.48, "text": " So it's also B by T.", "tokens": [50365, 407, 309, 311, 611, 363, 538, 314, 13, 50450], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 374, "seek": 176578, "start": 1767.48, "end": 1770.82, "text": " And the job of generate is to basically take this B by T", "tokens": [50450, 400, 264, 1691, 295, 8460, 307, 281, 1936, 747, 341, 363, 538, 314, 50617], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 375, "seek": 176578, "start": 1770.82, "end": 1773.6399999999999, "text": " and extend it to be B by T plus one, plus two, plus three.", "tokens": [50617, 293, 10101, 309, 281, 312, 363, 538, 314, 1804, 472, 11, 1804, 732, 11, 1804, 1045, 13, 50758], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 376, "seek": 176578, "start": 1773.6399999999999, "end": 1774.7, "text": " And so it's just basically,", "tokens": [50758, 400, 370, 309, 311, 445, 1936, 11, 50811], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 377, "seek": 176578, "start": 1774.7, "end": 1777.54, "text": " it continues the generation in all the batch dimensions", "tokens": [50811, 309, 6515, 264, 5125, 294, 439, 264, 15245, 12819, 50953], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 378, "seek": 176578, "start": 1777.54, "end": 1779.32, "text": " in the time dimension.", "tokens": [50953, 294, 264, 565, 10139, 13, 51042], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 379, "seek": 176578, "start": 1779.32, "end": 1780.5, "text": " So that's its job.", "tokens": [51042, 407, 300, 311, 1080, 1691, 13, 51101], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 380, "seek": 176578, "start": 1780.5, "end": 1783.08, "text": " And it will do that for max new tokens.", "tokens": [51101, 400, 309, 486, 360, 300, 337, 11469, 777, 22667, 13, 51230], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 381, "seek": 176578, "start": 1783.08, "end": 1784.5, "text": " So you can see here on the bottom,", "tokens": [51230, 407, 291, 393, 536, 510, 322, 264, 2767, 11, 51301], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 382, "seek": 176578, "start": 1784.5, "end": 1785.92, "text": " there's going to be some stuff here,", "tokens": [51301, 456, 311, 516, 281, 312, 512, 1507, 510, 11, 51372], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 383, "seek": 176578, "start": 1785.92, "end": 1786.82, "text": " but on the bottom,", "tokens": [51372, 457, 322, 264, 2767, 11, 51417], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 384, "seek": 176578, "start": 1786.82, "end": 1789.76, "text": " whatever is predicted is concatenated", "tokens": [51417, 2035, 307, 19147, 307, 1588, 7186, 770, 51564], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 385, "seek": 176578, "start": 1789.76, "end": 1793.08, "text": " on top of the previous IDX along the first dimension,", "tokens": [51564, 322, 1192, 295, 264, 3894, 7348, 55, 2051, 264, 700, 10139, 11, 51730], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 386, "seek": 176578, "start": 1793.08, "end": 1795.7, "text": " which is the time dimension to create a B by T plus one.", "tokens": [51730, 597, 307, 264, 565, 10139, 281, 1884, 257, 363, 538, 314, 1804, 472, 13, 51861], "temperature": 0.0, "avg_logprob": -0.10854511260986328, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00011619721044553444}, {"id": 387, "seek": 179578, "start": 1795.78, "end": 1798.28, "text": " So that becomes a new IDX.", "tokens": [50365, 407, 300, 3643, 257, 777, 7348, 55, 13, 50490], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 388, "seek": 179578, "start": 1798.28, "end": 1800.5, "text": " So the job of generate is to take a B by T", "tokens": [50490, 407, 264, 1691, 295, 8460, 307, 281, 747, 257, 363, 538, 314, 50601], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 389, "seek": 179578, "start": 1800.5, "end": 1803.72, "text": " and make it a B by T plus one, plus two, plus three,", "tokens": [50601, 293, 652, 309, 257, 363, 538, 314, 1804, 472, 11, 1804, 732, 11, 1804, 1045, 11, 50762], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 390, "seek": 179578, "start": 1803.72, "end": 1805.82, "text": " as many as we want max new tokens.", "tokens": [50762, 382, 867, 382, 321, 528, 11469, 777, 22667, 13, 50867], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 391, "seek": 179578, "start": 1805.82, "end": 1808.32, "text": " So this is the generation from the model.", "tokens": [50867, 407, 341, 307, 264, 5125, 490, 264, 2316, 13, 50992], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 392, "seek": 179578, "start": 1808.32, "end": 1810.8999999999999, "text": " Now inside the generation, what are we doing?", "tokens": [50992, 823, 1854, 264, 5125, 11, 437, 366, 321, 884, 30, 51121], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 393, "seek": 179578, "start": 1810.8999999999999, "end": 1812.74, "text": " We're taking the current indices.", "tokens": [51121, 492, 434, 1940, 264, 2190, 43840, 13, 51213], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 394, "seek": 179578, "start": 1812.74, "end": 1814.42, "text": " We're getting the predictions.", "tokens": [51213, 492, 434, 1242, 264, 21264, 13, 51297], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 395, "seek": 179578, "start": 1814.42, "end": 1818.08, "text": " So we get those are in the logits.", "tokens": [51297, 407, 321, 483, 729, 366, 294, 264, 3565, 1208, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 396, "seek": 179578, "start": 1818.08, "end": 1820.02, "text": " And then the loss here is going to be ignored", "tokens": [51480, 400, 550, 264, 4470, 510, 307, 516, 281, 312, 19735, 51577], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 397, "seek": 179578, "start": 1820.02, "end": 1822.24, "text": " because we're not using that.", "tokens": [51577, 570, 321, 434, 406, 1228, 300, 13, 51688], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 398, "seek": 179578, "start": 1822.24, "end": 1825.46, "text": " And we have no targets that are sort of ground truth targets", "tokens": [51688, 400, 321, 362, 572, 12911, 300, 366, 1333, 295, 2727, 3494, 12911, 51849], "temperature": 0.0, "avg_logprob": -0.12233762338127889, "compression_ratio": 1.7720588235294117, "no_speech_prob": 6.88986256136559e-05}, {"id": 399, "seek": 182546, "start": 1825.46, "end": 1827.3600000000001, "text": " that we're going to be comparing with.", "tokens": [50365, 300, 321, 434, 516, 281, 312, 15763, 365, 13, 50460], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 400, "seek": 182546, "start": 1828.6200000000001, "end": 1830.02, "text": " Then once we get the logits,", "tokens": [50523, 1396, 1564, 321, 483, 264, 3565, 1208, 11, 50593], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 401, "seek": 182546, "start": 1830.02, "end": 1832.6000000000001, "text": " we are only focusing on the last step.", "tokens": [50593, 321, 366, 787, 8416, 322, 264, 1036, 1823, 13, 50722], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 402, "seek": 182546, "start": 1832.6000000000001, "end": 1834.94, "text": " So instead of a B by T by C,", "tokens": [50722, 407, 2602, 295, 257, 363, 538, 314, 538, 383, 11, 50839], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 403, "seek": 182546, "start": 1834.94, "end": 1837.64, "text": " we're going to pluck out the negative one,", "tokens": [50839, 321, 434, 516, 281, 41514, 484, 264, 3671, 472, 11, 50974], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 404, "seek": 182546, "start": 1837.64, "end": 1840.16, "text": " the last element in the time dimension,", "tokens": [50974, 264, 1036, 4478, 294, 264, 565, 10139, 11, 51100], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 405, "seek": 182546, "start": 1840.16, "end": 1842.68, "text": " because those are the predictions for what comes next.", "tokens": [51100, 570, 729, 366, 264, 21264, 337, 437, 1487, 958, 13, 51226], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 406, "seek": 182546, "start": 1842.68, "end": 1844.1000000000001, "text": " So that gives us the logits,", "tokens": [51226, 407, 300, 2709, 505, 264, 3565, 1208, 11, 51297], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 407, "seek": 182546, "start": 1844.1000000000001, "end": 1847.6000000000001, "text": " which we then convert to probabilities via softmax.", "tokens": [51297, 597, 321, 550, 7620, 281, 33783, 5766, 2787, 41167, 13, 51472], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 408, "seek": 182546, "start": 1847.6000000000001, "end": 1848.96, "text": " And then we use torch.multinomial", "tokens": [51472, 400, 550, 321, 764, 27822, 13, 76, 723, 259, 47429, 51540], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 409, "seek": 182546, "start": 1848.96, "end": 1850.76, "text": " to sample from those probabilities.", "tokens": [51540, 281, 6889, 490, 729, 33783, 13, 51630], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 410, "seek": 182546, "start": 1850.76, "end": 1853.9, "text": " And we ask PyTorch to give us one sample.", "tokens": [51630, 400, 321, 1029, 9953, 51, 284, 339, 281, 976, 505, 472, 6889, 13, 51787], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 411, "seek": 182546, "start": 1853.9, "end": 1855.0, "text": " And so IDX next,", "tokens": [51787, 400, 370, 7348, 55, 958, 11, 51842], "temperature": 0.0, "avg_logprob": -0.10214330081281991, "compression_ratio": 1.7311827956989247, "no_speech_prob": 0.0001446763490093872}, {"id": 412, "seek": 185500, "start": 1855.0, "end": 1856.98, "text": " we'll become a B by one,", "tokens": [50365, 321, 603, 1813, 257, 363, 538, 472, 11, 50464], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 413, "seek": 185500, "start": 1856.98, "end": 1860.1, "text": " because in each one of the batch dimensions,", "tokens": [50464, 570, 294, 1184, 472, 295, 264, 15245, 12819, 11, 50620], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 414, "seek": 185500, "start": 1860.1, "end": 1862.42, "text": " we're going to have a single prediction for what comes next.", "tokens": [50620, 321, 434, 516, 281, 362, 257, 2167, 17630, 337, 437, 1487, 958, 13, 50736], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 415, "seek": 185500, "start": 1862.42, "end": 1864.5, "text": " So this numSamples equals one,", "tokens": [50736, 407, 341, 1031, 28743, 2622, 6915, 472, 11, 50840], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 416, "seek": 185500, "start": 1864.5, "end": 1866.6, "text": " will make this be a one.", "tokens": [50840, 486, 652, 341, 312, 257, 472, 13, 50945], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 417, "seek": 185500, "start": 1866.6, "end": 1869.1, "text": " And then we're going to take those integers", "tokens": [50945, 400, 550, 321, 434, 516, 281, 747, 729, 41674, 51070], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 418, "seek": 185500, "start": 1869.1, "end": 1870.82, "text": " that come from the sampling process", "tokens": [51070, 300, 808, 490, 264, 21179, 1399, 51156], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 419, "seek": 185500, "start": 1870.82, "end": 1873.34, "text": " according to the probability distribution given here.", "tokens": [51156, 4650, 281, 264, 8482, 7316, 2212, 510, 13, 51282], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 420, "seek": 185500, "start": 1873.34, "end": 1875.34, "text": " And those integers got just concatenated", "tokens": [51282, 400, 729, 41674, 658, 445, 1588, 7186, 770, 51382], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 421, "seek": 185500, "start": 1875.34, "end": 1879.02, "text": " on top of the current sort of like running stream of integers.", "tokens": [51382, 322, 1192, 295, 264, 2190, 1333, 295, 411, 2614, 4309, 295, 41674, 13, 51566], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 422, "seek": 185500, "start": 1879.02, "end": 1881.62, "text": " And this gives us a B by T plus one.", "tokens": [51566, 400, 341, 2709, 505, 257, 363, 538, 314, 1804, 472, 13, 51696], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 423, "seek": 185500, "start": 1881.62, "end": 1883.16, "text": " And then we can return that.", "tokens": [51696, 400, 550, 321, 393, 2736, 300, 13, 51773], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 424, "seek": 185500, "start": 1883.16, "end": 1884.16, "text": " Now, one thing here is,", "tokens": [51773, 823, 11, 472, 551, 510, 307, 11, 51823], "temperature": 0.6000000000000001, "avg_logprob": -0.2737845031308456, "compression_ratio": 1.7972027972027973, "no_speech_prob": 7.380259194178507e-05}, {"id": 425, "seek": 188416, "start": 1884.16, "end": 1890.3200000000002, "text": " here is you see how i'm calling self of idx which will end up going to the forward function", "tokens": [50365, 510, 307, 291, 536, 577, 741, 478, 5141, 2698, 295, 4496, 87, 597, 486, 917, 493, 516, 281, 264, 2128, 2445, 50673], "temperature": 0.0, "avg_logprob": -0.07936580529373684, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.04016120731830597}, {"id": 426, "seek": 188416, "start": 1890.96, "end": 1896.5600000000002, "text": " i'm not providing any targets so currently this would give an error because targets is uh is uh", "tokens": [50705, 741, 478, 406, 6530, 604, 12911, 370, 4362, 341, 576, 976, 364, 6713, 570, 12911, 307, 2232, 307, 2232, 50985], "temperature": 0.0, "avg_logprob": -0.07936580529373684, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.04016120731830597}, {"id": 427, "seek": 188416, "start": 1896.5600000000002, "end": 1902.24, "text": " sort of like not given so target has to be optional so targets is none by default and", "tokens": [50985, 1333, 295, 411, 406, 2212, 370, 3779, 575, 281, 312, 17312, 370, 12911, 307, 6022, 538, 7576, 293, 51269], "temperature": 0.0, "avg_logprob": -0.07936580529373684, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.04016120731830597}, {"id": 428, "seek": 188416, "start": 1902.24, "end": 1909.2, "text": " then if targets is none then there's no loss to create so it's just loss is none but else", "tokens": [51269, 550, 498, 12911, 307, 6022, 550, 456, 311, 572, 4470, 281, 1884, 370, 309, 311, 445, 4470, 307, 6022, 457, 1646, 51617], "temperature": 0.0, "avg_logprob": -0.07936580529373684, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.04016120731830597}, {"id": 429, "seek": 190920, "start": 1909.2, "end": 1916.32, "text": " all of this happens and we can create a loss so this will make it so um if we have the targets", "tokens": [50365, 439, 295, 341, 2314, 293, 321, 393, 1884, 257, 4470, 370, 341, 486, 652, 309, 370, 1105, 498, 321, 362, 264, 12911, 50721], "temperature": 0.0, "avg_logprob": -0.05647748124365713, "compression_ratio": 1.933649289099526, "no_speech_prob": 0.00011841779632959515}, {"id": 430, "seek": 190920, "start": 1916.32, "end": 1922.16, "text": " we provide them and get a loss if we have no targets we'll just get the logits so this here", "tokens": [50721, 321, 2893, 552, 293, 483, 257, 4470, 498, 321, 362, 572, 12911, 321, 603, 445, 483, 264, 3565, 1208, 370, 341, 510, 51013], "temperature": 0.0, "avg_logprob": -0.05647748124365713, "compression_ratio": 1.933649289099526, "no_speech_prob": 0.00011841779632959515}, {"id": 431, "seek": 190920, "start": 1922.16, "end": 1931.76, "text": " will generate from the model and let's take that for a ride now oops so i have another code chunk", "tokens": [51013, 486, 8460, 490, 264, 2316, 293, 718, 311, 747, 300, 337, 257, 5077, 586, 34166, 370, 741, 362, 1071, 3089, 16635, 51493], "temperature": 0.0, "avg_logprob": -0.05647748124365713, "compression_ratio": 1.933649289099526, "no_speech_prob": 0.00011841779632959515}, {"id": 432, "seek": 190920, "start": 1931.76, "end": 1936.64, "text": " here which will generate for the model from the model and okay this is kind of crazy so maybe let", "tokens": [51493, 510, 597, 486, 8460, 337, 264, 2316, 490, 264, 2316, 293, 1392, 341, 307, 733, 295, 3219, 370, 1310, 718, 51737], "temperature": 0.0, "avg_logprob": -0.05647748124365713, "compression_ratio": 1.933649289099526, "no_speech_prob": 0.00011841779632959515}, {"id": 433, "seek": 190920, "start": 1936.64, "end": 1938.4, "text": " me let me break this down", "tokens": [51737, 385, 718, 385, 1821, 341, 760, 51825], "temperature": 0.0, "avg_logprob": -0.05647748124365713, "compression_ratio": 1.933649289099526, "no_speech_prob": 0.00011841779632959515}, {"id": 434, "seek": 193920, "start": 1939.2, "end": 1940.96, "text": " so these are the idx right", "tokens": [50365, 370, 613, 366, 264, 4496, 87, 558, 50453], "temperature": 0.0, "avg_logprob": -0.056819189162481396, "compression_ratio": 1.8385650224215246, "no_speech_prob": 0.00021904661844018847}, {"id": 435, "seek": 193920, "start": 1944.72, "end": 1948.72, "text": " i'm creating a batch will be just one time will be just one", "tokens": [50641, 741, 478, 4084, 257, 15245, 486, 312, 445, 472, 565, 486, 312, 445, 472, 50841], "temperature": 0.0, "avg_logprob": -0.056819189162481396, "compression_ratio": 1.8385650224215246, "no_speech_prob": 0.00021904661844018847}, {"id": 436, "seek": 193920, "start": 1949.6000000000001, "end": 1955.52, "text": " so i'm creating a little one by one tensor and it's holding a zero and the d type the data type", "tokens": [50885, 370, 741, 478, 4084, 257, 707, 472, 538, 472, 40863, 293, 309, 311, 5061, 257, 4018, 293, 264, 274, 2010, 264, 1412, 2010, 51181], "temperature": 0.0, "avg_logprob": -0.056819189162481396, "compression_ratio": 1.8385650224215246, "no_speech_prob": 0.00021904661844018847}, {"id": 437, "seek": 193920, "start": 1955.52, "end": 1962.72, "text": " is uh integer so zero is going to be how we kick off the generation and remember that zero is uh", "tokens": [51181, 307, 2232, 24922, 370, 4018, 307, 516, 281, 312, 577, 321, 4437, 766, 264, 5125, 293, 1604, 300, 4018, 307, 2232, 51541], "temperature": 0.0, "avg_logprob": -0.056819189162481396, "compression_ratio": 1.8385650224215246, "no_speech_prob": 0.00021904661844018847}, {"id": 438, "seek": 193920, "start": 1962.72, "end": 1967.3600000000001, "text": " is the element standing for a new line character so it's kind of like a reasonable thing to", "tokens": [51541, 307, 264, 4478, 4877, 337, 257, 777, 1622, 2517, 370, 309, 311, 733, 295, 411, 257, 10585, 551, 281, 51773], "temperature": 0.0, "avg_logprob": -0.056819189162481396, "compression_ratio": 1.8385650224215246, "no_speech_prob": 0.00021904661844018847}, {"id": 439, "seek": 193920, "start": 1967.3600000000001, "end": 1968.96, "text": " to feed in as the very first character", "tokens": [51773, 281, 3154, 294, 382, 264, 588, 700, 2517, 51853], "temperature": 0.0, "avg_logprob": -0.056819189162481396, "compression_ratio": 1.8385650224215246, "no_speech_prob": 0.00021904661844018847}, {"id": 440, "seek": 196920, "start": 1969.2, "end": 1976.0, "text": " sequence to be the new line um so it's going to be idx which we're going to feed in here", "tokens": [50365, 8310, 281, 312, 264, 777, 1622, 1105, 370, 309, 311, 516, 281, 312, 4496, 87, 597, 321, 434, 516, 281, 3154, 294, 510, 50705], "temperature": 0.0, "avg_logprob": -0.10556791093614366, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0006314865895546973}, {"id": 441, "seek": 196920, "start": 1976.0, "end": 1980.8, "text": " then we're going to ask for 100 tokens and then end that generate will continue that", "tokens": [50705, 550, 321, 434, 516, 281, 1029, 337, 2319, 22667, 293, 550, 917, 300, 8460, 486, 2354, 300, 50945], "temperature": 0.0, "avg_logprob": -0.10556791093614366, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0006314865895546973}, {"id": 442, "seek": 196920, "start": 1981.68, "end": 1987.8400000000001, "text": " now because uh generate works on the level of batches we then have to index into the", "tokens": [50989, 586, 570, 2232, 8460, 1985, 322, 264, 1496, 295, 15245, 279, 321, 550, 362, 281, 8186, 666, 264, 51297], "temperature": 0.0, "avg_logprob": -0.10556791093614366, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0006314865895546973}, {"id": 443, "seek": 196920, "start": 1987.8400000000001, "end": 1996.8, "text": " zero throw to basically unplug the um the single batch dimension that exists and then that gives us", "tokens": [51297, 4018, 3507, 281, 1936, 39456, 264, 1105, 264, 2167, 15245, 10139, 300, 8198, 293, 550, 300, 2709, 505, 51745], "temperature": 0.0, "avg_logprob": -0.10556791093614366, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0006314865895546973}, {"id": 444, "seek": 196920, "start": 1996.8, "end": 1997.6000000000001, "text": " a um", "tokens": [51745, 257, 1105, 51785], "temperature": 0.0, "avg_logprob": -0.10556791093614366, "compression_ratio": 1.7621359223300972, "no_speech_prob": 0.0006314865895546973}, {"id": 445, "seek": 199760, "start": 1997.6, "end": 2004.32, "text": " time steps is just a one-dimensional array of all the indices which we will convert to simple python", "tokens": [50365, 565, 4439, 307, 445, 257, 472, 12, 18759, 10225, 295, 439, 264, 43840, 597, 321, 486, 7620, 281, 2199, 38797, 50701], "temperature": 0.0, "avg_logprob": -0.1705815709870437, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00016982419765554368}, {"id": 446, "seek": 199760, "start": 2004.32, "end": 2012.7199999999998, "text": " list from pytorch tensor so that that can feed into our decode function and convert those integers", "tokens": [50701, 1329, 490, 25878, 284, 339, 40863, 370, 300, 300, 393, 3154, 666, 527, 979, 1429, 2445, 293, 7620, 729, 41674, 51121], "temperature": 0.0, "avg_logprob": -0.1705815709870437, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00016982419765554368}, {"id": 447, "seek": 199760, "start": 2012.7199999999998, "end": 2020.08, "text": " into text so let me bring this back and we're generating 100 tokens let's run and uh here's", "tokens": [51121, 666, 2487, 370, 718, 385, 1565, 341, 646, 293, 321, 434, 17746, 2319, 22667, 718, 311, 1190, 293, 2232, 510, 311, 51489], "temperature": 0.0, "avg_logprob": -0.1705815709870437, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00016982419765554368}, {"id": 448, "seek": 199760, "start": 2020.08, "end": 2024.56, "text": " the generation that we achieved so obviously it's garbage and the reason it's garbage is because", "tokens": [51489, 264, 5125, 300, 321, 11042, 370, 2745, 309, 311, 14150, 293, 264, 1778, 309, 311, 14150, 307, 570, 51713], "temperature": 0.0, "avg_logprob": -0.1705815709870437, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00016982419765554368}, {"id": 449, "seek": 199760, "start": 2024.56, "end": 2027.12, "text": " this is a totally random model so next up we're going to want to do is we're going to want to do a", "tokens": [51713, 341, 307, 257, 3879, 4974, 2316, 370, 958, 493, 321, 434, 516, 281, 528, 281, 360, 307, 321, 434, 516, 281, 528, 281, 360, 257, 51841], "temperature": 0.0, "avg_logprob": -0.1705815709870437, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.00016982419765554368}, {"id": 450, "seek": 202712, "start": 2027.12, "end": 2030.8799999999999, "text": " we're going to want to train this model now one more thing i wanted to point out here is", "tokens": [50365, 321, 434, 516, 281, 528, 281, 3847, 341, 2316, 586, 472, 544, 551, 741, 1415, 281, 935, 484, 510, 307, 50553], "temperature": 0.0, "avg_logprob": -0.07040940261468655, "compression_ratio": 1.9338235294117647, "no_speech_prob": 0.00017057459626812488}, {"id": 451, "seek": 202712, "start": 2032.08, "end": 2036.3999999999999, "text": " this function is written to be general but it's kind of like ridiculous right now because", "tokens": [50613, 341, 2445, 307, 3720, 281, 312, 2674, 457, 309, 311, 733, 295, 411, 11083, 558, 586, 570, 50829], "temperature": 0.0, "avg_logprob": -0.07040940261468655, "compression_ratio": 1.9338235294117647, "no_speech_prob": 0.00017057459626812488}, {"id": 452, "seek": 202712, "start": 2037.9199999999998, "end": 2042.7199999999998, "text": " we're feeding in all this we're building out this context and we're concatenating it all", "tokens": [50905, 321, 434, 12919, 294, 439, 341, 321, 434, 2390, 484, 341, 4319, 293, 321, 434, 1588, 7186, 990, 309, 439, 51145], "temperature": 0.0, "avg_logprob": -0.07040940261468655, "compression_ratio": 1.9338235294117647, "no_speech_prob": 0.00017057459626812488}, {"id": 453, "seek": 202712, "start": 2042.7199999999998, "end": 2048.48, "text": " and we're always feeding it all into the model but that's kind of ridiculous because this is", "tokens": [51145, 293, 321, 434, 1009, 12919, 309, 439, 666, 264, 2316, 457, 300, 311, 733, 295, 11083, 570, 341, 307, 51433], "temperature": 0.0, "avg_logprob": -0.07040940261468655, "compression_ratio": 1.9338235294117647, "no_speech_prob": 0.00017057459626812488}, {"id": 454, "seek": 202712, "start": 2048.48, "end": 2054.24, "text": " just a simple bigram model so to make for example this prediction about k we only needed this w", "tokens": [51433, 445, 257, 2199, 955, 2356, 2316, 370, 281, 652, 337, 1365, 341, 17630, 466, 350, 321, 787, 2978, 341, 261, 51721], "temperature": 0.0, "avg_logprob": -0.07040940261468655, "compression_ratio": 1.9338235294117647, "no_speech_prob": 0.00017057459626812488}, {"id": 455, "seek": 202712, "start": 2054.24, "end": 2057.04, "text": " but actually what we fed into the model is we fed the entire sequence", "tokens": [51721, 457, 767, 437, 321, 4636, 666, 264, 2316, 307, 321, 4636, 264, 2302, 8310, 51861], "temperature": 0.0, "avg_logprob": -0.07040940261468655, "compression_ratio": 1.9338235294117647, "no_speech_prob": 0.00017057459626812488}, {"id": 456, "seek": 205712, "start": 2057.52, "end": 2063.44, "text": " and then we only looked at the very last piece and predicted k so the only reason i'm writing", "tokens": [50385, 293, 550, 321, 787, 2956, 412, 264, 588, 1036, 2522, 293, 19147, 350, 370, 264, 787, 1778, 741, 478, 3579, 50681], "temperature": 0.0, "avg_logprob": -0.037788300900845916, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0008590870420448482}, {"id": 457, "seek": 205712, "start": 2063.44, "end": 2068.3199999999997, "text": " it in this way is because right now this is a bigram model but i'd like to keep this function", "tokens": [50681, 309, 294, 341, 636, 307, 570, 558, 586, 341, 307, 257, 955, 2356, 2316, 457, 741, 1116, 411, 281, 1066, 341, 2445, 50925], "temperature": 0.0, "avg_logprob": -0.037788300900845916, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0008590870420448482}, {"id": 458, "seek": 205712, "start": 2068.3199999999997, "end": 2076.0, "text": " fixed and i'd like it to work later when our characters actually basically look further in", "tokens": [50925, 6806, 293, 741, 1116, 411, 309, 281, 589, 1780, 562, 527, 4342, 767, 1936, 574, 3052, 294, 51309], "temperature": 0.0, "avg_logprob": -0.037788300900845916, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0008590870420448482}, {"id": 459, "seek": 205712, "start": 2076.0, "end": 2081.04, "text": " the history and so right now the history is not used so this looks silly but eventually", "tokens": [51309, 264, 2503, 293, 370, 558, 586, 264, 2503, 307, 406, 1143, 370, 341, 1542, 11774, 457, 4728, 51561], "temperature": 0.0, "avg_logprob": -0.037788300900845916, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0008590870420448482}, {"id": 460, "seek": 205712, "start": 2081.04, "end": 2086.72, "text": " the history will be used and so that's why we want to do it this way so just a quick comment on that", "tokens": [51561, 264, 2503, 486, 312, 1143, 293, 370, 300, 311, 983, 321, 528, 281, 360, 309, 341, 636, 370, 445, 257, 1702, 2871, 322, 300, 51845], "temperature": 0.0, "avg_logprob": -0.037788300900845916, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0008590870420448482}, {"id": 461, "seek": 208712, "start": 2087.52, "end": 2093.6, "text": " so now we see that this is random so let's train the model so it becomes a bit less random okay", "tokens": [50385, 370, 586, 321, 536, 300, 341, 307, 4974, 370, 718, 311, 3847, 264, 2316, 370, 309, 3643, 257, 857, 1570, 4974, 1392, 50689], "temperature": 0.0, "avg_logprob": -0.1667809644282259, "compression_ratio": 2.1986062717770034, "no_speech_prob": 0.0005101456772536039}, {"id": 462, "seek": 208712, "start": 2093.6, "end": 2098.48, "text": " let's now train the model so first what i'm going to do is i'm going to create a pytorch optimization", "tokens": [50689, 718, 311, 586, 3847, 264, 2316, 370, 700, 437, 741, 478, 516, 281, 360, 307, 741, 478, 516, 281, 1884, 257, 25878, 284, 339, 19618, 50933], "temperature": 0.0, "avg_logprob": -0.1667809644282259, "compression_ratio": 2.1986062717770034, "no_speech_prob": 0.0005101456772536039}, {"id": 463, "seek": 208712, "start": 2098.48, "end": 2106.24, "text": " object so here we are using the optimizer adam w now in the make more series we've only ever used", "tokens": [50933, 2657, 370, 510, 321, 366, 1228, 264, 5028, 6545, 16368, 261, 586, 294, 264, 652, 544, 2638, 321, 600, 787, 1562, 1143, 51321], "temperature": 0.0, "avg_logprob": -0.1667809644282259, "compression_ratio": 2.1986062717770034, "no_speech_prob": 0.0005101456772536039}, {"id": 464, "seek": 208712, "start": 2106.24, "end": 2111.44, "text": " stochastic gradient descent the simplest possible optimizer which you can get using the sgd instead", "tokens": [51321, 342, 8997, 2750, 16235, 23475, 264, 22811, 1944, 5028, 6545, 597, 291, 393, 483, 1228, 264, 262, 70, 67, 2602, 51581], "temperature": 0.0, "avg_logprob": -0.1667809644282259, "compression_ratio": 2.1986062717770034, "no_speech_prob": 0.0005101456772536039}, {"id": 465, "seek": 208712, "start": 2111.44, "end": 2115.44, "text": " but i want to use adam which is a much more advanced and popular optimizer and it works", "tokens": [51581, 457, 741, 528, 281, 764, 16368, 597, 307, 257, 709, 544, 7339, 293, 3743, 5028, 6545, 293, 309, 1985, 51781], "temperature": 0.0, "avg_logprob": -0.1667809644282259, "compression_ratio": 2.1986062717770034, "no_speech_prob": 0.0005101456772536039}, {"id": 466, "seek": 208712, "start": 2115.44, "end": 2116.0, "text": " extremely well for a lot of other optimizers but i want to use adam which is a much more advanced and popular optimizer and it works extremely well", "tokens": [51781, 4664, 731, 337, 257, 688, 295, 661, 5028, 22525, 457, 741, 528, 281, 764, 16368, 597, 307, 257, 709, 544, 7339, 293, 3743, 5028, 6545, 293, 309, 1985, 4664, 731, 51809], "temperature": 0.0, "avg_logprob": -0.1667809644282259, "compression_ratio": 2.1986062717770034, "no_speech_prob": 0.0005101456772536039}, {"id": 467, "seek": 211712, "start": 2117.68, "end": 2123.04, "text": " typical good setting for the learning rate is roughly 3e negative 4 but for very very small", "tokens": [50393, 7476, 665, 3287, 337, 264, 2539, 3314, 307, 9810, 805, 68, 3671, 1017, 457, 337, 588, 588, 1359, 50661], "temperature": 0.0, "avg_logprob": -0.08496651422409784, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0016516615869477391}, {"id": 468, "seek": 211712, "start": 2123.04, "end": 2126.72, "text": " networks like it's the case here you can get away with much much higher learning rates", "tokens": [50661, 9590, 411, 309, 311, 264, 1389, 510, 291, 393, 483, 1314, 365, 709, 709, 2946, 2539, 6846, 50845], "temperature": 0.0, "avg_logprob": -0.08496651422409784, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0016516615869477391}, {"id": 469, "seek": 211712, "start": 2126.72, "end": 2133.2799999999997, "text": " 1-3 or even higher probably but let me create the optimizer object which will basically take", "tokens": [50845, 502, 12, 18, 420, 754, 2946, 1391, 457, 718, 385, 1884, 264, 5028, 6545, 2657, 597, 486, 1936, 747, 51173], "temperature": 0.0, "avg_logprob": -0.08496651422409784, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0016516615869477391}, {"id": 470, "seek": 211712, "start": 2133.2799999999997, "end": 2140.48, "text": " the gradients and update the parameters using the gradients and then here our batch size up above", "tokens": [51173, 264, 2771, 2448, 293, 5623, 264, 9834, 1228, 264, 2771, 2448, 293, 550, 510, 527, 15245, 2744, 493, 3673, 51533], "temperature": 0.0, "avg_logprob": -0.08496651422409784, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0016516615869477391}, {"id": 471, "seek": 211712, "start": 2140.48, "end": 2145.3599999999997, "text": " was only 4 so let me actually use something bigger let's say 32 and then for some number of steps", "tokens": [51533, 390, 787, 1017, 370, 718, 385, 767, 764, 746, 3801, 718, 311, 584, 8858, 293, 550, 337, 512, 1230, 295, 4439, 51777], "temperature": 0.0, "avg_logprob": -0.08496651422409784, "compression_ratio": 1.742537313432836, "no_speech_prob": 0.0016516615869477391}, {"id": 472, "seek": 214712, "start": 2147.12, "end": 2153.6, "text": " we're sampling a new batch of data we're evaluating the loss we're zeroing out all the gradients from", "tokens": [50365, 321, 434, 21179, 257, 777, 15245, 295, 1412, 321, 434, 27479, 264, 4470, 321, 434, 4018, 278, 484, 439, 264, 2771, 2448, 490, 50689], "temperature": 0.4, "avg_logprob": -0.20550342906605112, "compression_ratio": 1.8008130081300813, "no_speech_prob": 0.0013036091113463044}, {"id": 473, "seek": 214712, "start": 2153.6, "end": 2158.24, "text": " the previous step getting the gradients for all the parameters and then using those gradients to", "tokens": [50689, 264, 3894, 1823, 1242, 264, 2771, 2448, 337, 439, 264, 9834, 293, 550, 1228, 729, 2771, 2448, 281, 50921], "temperature": 0.4, "avg_logprob": -0.20550342906605112, "compression_ratio": 1.8008130081300813, "no_speech_prob": 0.0013036091113463044}, {"id": 474, "seek": 214712, "start": 2158.24, "end": 2164.24, "text": " update our parameters so typical training loop as we saw in the make more series so let me now", "tokens": [50921, 5623, 527, 9834, 370, 7476, 3097, 6367, 382, 321, 1866, 294, 264, 652, 544, 2638, 370, 718, 385, 586, 51221], "temperature": 0.4, "avg_logprob": -0.20550342906605112, "compression_ratio": 1.8008130081300813, "no_speech_prob": 0.0013036091113463044}, {"id": 475, "seek": 214712, "start": 2164.24, "end": 2169.2, "text": " run this for say 100 iterations and let's see what kind of loss is we're going to get", "tokens": [51221, 1190, 341, 337, 584, 2319, 36540, 293, 718, 311, 536, 437, 733, 295, 4470, 307, 321, 434, 516, 281, 483, 51469], "temperature": 0.4, "avg_logprob": -0.20550342906605112, "compression_ratio": 1.8008130081300813, "no_speech_prob": 0.0013036091113463044}, {"id": 476, "seek": 214712, "start": 2171.44, "end": 2176.72, "text": " so we started around 4.7 and now we're getting down to like 4.6", "tokens": [51581, 370, 321, 1409, 926, 1017, 13, 22, 293, 586, 321, 434, 1242, 760, 281, 411, 1017, 13, 21, 51845], "temperature": 0.4, "avg_logprob": -0.20550342906605112, "compression_ratio": 1.8008130081300813, "no_speech_prob": 0.0013036091113463044}, {"id": 477, "seek": 217672, "start": 2177.04, "end": 2183.68, "text": " so the optimization is definitely happening but let's sort of try to increase the number", "tokens": [50381, 370, 264, 19618, 307, 2138, 2737, 457, 718, 311, 1333, 295, 853, 281, 3488, 264, 1230, 50713], "temperature": 0.0, "avg_logprob": -0.13085286067082333, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0001751354429870844}, {"id": 478, "seek": 217672, "start": 2183.68, "end": 2188.3199999999997, "text": " of iterations and only print at the end because we probably will not train for longer", "tokens": [50713, 295, 36540, 293, 787, 4482, 412, 264, 917, 570, 321, 1391, 486, 406, 3847, 337, 2854, 50945], "temperature": 0.0, "avg_logprob": -0.13085286067082333, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0001751354429870844}, {"id": 479, "seek": 217672, "start": 2190.24, "end": 2192.24, "text": " okay so we're down to 3.6 roughly", "tokens": [51041, 1392, 370, 321, 434, 760, 281, 805, 13, 21, 9810, 51141], "temperature": 0.0, "avg_logprob": -0.13085286067082333, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0001751354429870844}, {"id": 480, "seek": 217672, "start": 2195.52, "end": 2196.48, "text": " roughly down to three", "tokens": [51305, 9810, 760, 281, 1045, 51353], "temperature": 0.0, "avg_logprob": -0.13085286067082333, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0001751354429870844}, {"id": 481, "seek": 217672, "start": 2201.4399999999996, "end": 2203.04, "text": " this is the most janky optimization", "tokens": [51601, 341, 307, 264, 881, 361, 657, 88, 19618, 51681], "temperature": 0.0, "avg_logprob": -0.13085286067082333, "compression_ratio": 1.5647058823529412, "no_speech_prob": 0.0001751354429870844}, {"id": 482, "seek": 220672, "start": 2207.68, "end": 2212.72, "text": " if we do that and clean those up we get six hours of telly in in mobile", "tokens": [50413, 498, 321, 360, 300, 293, 2541, 729, 493, 321, 483, 2309, 2496, 295, 980, 88, 294, 294, 6013, 50665], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 483, "seek": 220672, "start": 2214.3199999999997, "end": 2216.08, "text": " okay it's working let's just do 10 000", "tokens": [50745, 1392, 309, 311, 1364, 718, 311, 445, 360, 1266, 13711, 50833], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 484, "seek": 220672, "start": 2217.52, "end": 2220.64, "text": " and then from here we want to copy this", "tokens": [50905, 293, 550, 490, 510, 321, 528, 281, 5055, 341, 51061], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 485, "seek": 220672, "start": 2221.7599999999998, "end": 2224.64, "text": " and hopefully we're going to get something reasonable and of course it's not going to", "tokens": [51117, 293, 4696, 321, 434, 516, 281, 483, 746, 10585, 293, 295, 1164, 309, 311, 406, 516, 281, 51261], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 486, "seek": 220672, "start": 2224.64, "end": 2228.3199999999997, "text": " be shakespeare from a bigger model but at least we see that the loss is improving", "tokens": [51261, 312, 37891, 22446, 490, 257, 3801, 2316, 457, 412, 1935, 321, 536, 300, 264, 4470, 307, 11470, 51445], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 487, "seek": 220672, "start": 2228.8799999999997, "end": 2231.68, "text": " and hopefully we're expecting something a bit more reasonable", "tokens": [51473, 293, 4696, 321, 434, 9650, 746, 257, 857, 544, 10585, 51613], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 488, "seek": 220672, "start": 2232.9599999999996, "end": 2235.6, "text": " so we're down in about 2.5-ish let's see what we get", "tokens": [51677, 370, 321, 434, 760, 294, 466, 568, 13, 20, 12, 742, 718, 311, 536, 437, 321, 483, 51809], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 489, "seek": 220672, "start": 2235.6, "end": 2236.24, "text": " okay", "tokens": [51809, 1392, 51841], "temperature": 1.0, "avg_logprob": -1.1975261981670673, "compression_ratio": 1.752, "no_speech_prob": 0.00037472438998520374}, {"id": 490, "seek": 223624, "start": 2236.24, "end": 2238.22, "text": " Let me just increase the number of tokens.", "tokens": [50365, 961, 385, 445, 3488, 264, 1230, 295, 22667, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18937130632071658, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.06365711241960526}, {"id": 491, "seek": 223624, "start": 2239.16, "end": 2243.74, "text": " Okay, so we see that we're starting to get something at least like reasonable-ish.", "tokens": [50511, 1033, 11, 370, 321, 536, 300, 321, 434, 2891, 281, 483, 746, 412, 1935, 411, 10585, 12, 742, 13, 50740], "temperature": 0.0, "avg_logprob": -0.18937130632071658, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.06365711241960526}, {"id": 492, "seek": 223624, "start": 2246.54, "end": 2250.2799999999997, "text": " Certainly not Shakespeare, but the model is making progress.", "tokens": [50880, 16628, 406, 22825, 11, 457, 264, 2316, 307, 1455, 4205, 13, 51067], "temperature": 0.0, "avg_logprob": -0.18937130632071658, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.06365711241960526}, {"id": 493, "seek": 223624, "start": 2250.64, "end": 2252.64, "text": " So that is the simplest possible model.", "tokens": [51085, 407, 300, 307, 264, 22811, 1944, 2316, 13, 51185], "temperature": 0.0, "avg_logprob": -0.18937130632071658, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.06365711241960526}, {"id": 494, "seek": 223624, "start": 2253.8599999999997, "end": 2261.3599999999997, "text": " So now what I'd like to do is, obviously, this is a very simple model because the tokens are not talking to each other.", "tokens": [51246, 407, 586, 437, 286, 1116, 411, 281, 360, 307, 11, 2745, 11, 341, 307, 257, 588, 2199, 2316, 570, 264, 22667, 366, 406, 1417, 281, 1184, 661, 13, 51621], "temperature": 0.0, "avg_logprob": -0.18937130632071658, "compression_ratio": 1.5656108597285068, "no_speech_prob": 0.06365711241960526}, {"id": 495, "seek": 226136, "start": 2261.36, "end": 2268.26, "text": " So given the previous context of whatever was generated, we're only looking at the very last character to make the predictions about what comes next.", "tokens": [50365, 407, 2212, 264, 3894, 4319, 295, 2035, 390, 10833, 11, 321, 434, 787, 1237, 412, 264, 588, 1036, 2517, 281, 652, 264, 21264, 466, 437, 1487, 958, 13, 50710], "temperature": 0.0, "avg_logprob": -0.07368361155192057, "compression_ratio": 1.7807807807807807, "no_speech_prob": 0.0001315716071985662}, {"id": 496, "seek": 226136, "start": 2268.88, "end": 2277.1400000000003, "text": " So now these tokens have to start talking to each other and figuring out what is in the context so that they can make better predictions for what comes next.", "tokens": [50741, 407, 586, 613, 22667, 362, 281, 722, 1417, 281, 1184, 661, 293, 15213, 484, 437, 307, 294, 264, 4319, 370, 300, 436, 393, 652, 1101, 21264, 337, 437, 1487, 958, 13, 51154], "temperature": 0.0, "avg_logprob": -0.07368361155192057, "compression_ratio": 1.7807807807807807, "no_speech_prob": 0.0001315716071985662}, {"id": 497, "seek": 226136, "start": 2277.52, "end": 2279.84, "text": " And this is how we're going to kick off the transformer.", "tokens": [51173, 400, 341, 307, 577, 321, 434, 516, 281, 4437, 766, 264, 31782, 13, 51289], "temperature": 0.0, "avg_logprob": -0.07368361155192057, "compression_ratio": 1.7807807807807807, "no_speech_prob": 0.0001315716071985662}, {"id": 498, "seek": 226136, "start": 2280.5, "end": 2284.88, "text": " Okay, so next, I took the code that we developed in this Jupyter notebook and I converted it to be a script.", "tokens": [51322, 1033, 11, 370, 958, 11, 286, 1890, 264, 3089, 300, 321, 4743, 294, 341, 22125, 88, 391, 21060, 293, 286, 16424, 309, 281, 312, 257, 5755, 13, 51541], "temperature": 0.0, "avg_logprob": -0.07368361155192057, "compression_ratio": 1.7807807807807807, "no_speech_prob": 0.0001315716071985662}, {"id": 499, "seek": 226136, "start": 2285.34, "end": 2291.32, "text": " And I'm doing this because I just want to simplify our intermediate work, which is just the final product that we have.", "tokens": [51564, 400, 286, 478, 884, 341, 570, 286, 445, 528, 281, 20460, 527, 19376, 589, 11, 597, 307, 445, 264, 2572, 1674, 300, 321, 362, 13, 51863], "temperature": 0.0, "avg_logprob": -0.07368361155192057, "compression_ratio": 1.7807807807807807, "no_speech_prob": 0.0001315716071985662}, {"id": 500, "seek": 229136, "start": 2291.36, "end": 2296.52, "text": " At this point, so in the top here, I put all the hyperparameters that we've defined.", "tokens": [50365, 1711, 341, 935, 11, 370, 294, 264, 1192, 510, 11, 286, 829, 439, 264, 9848, 2181, 335, 6202, 300, 321, 600, 7642, 13, 50623], "temperature": 0.0, "avg_logprob": -0.20108488202095032, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0010643680579960346}, {"id": 501, "seek": 229136, "start": 2296.76, "end": 2299.52, "text": " I introduced a few and I'm going to speak to that in a little bit.", "tokens": [50635, 286, 7268, 257, 1326, 293, 286, 478, 516, 281, 1710, 281, 300, 294, 257, 707, 857, 13, 50773], "temperature": 0.0, "avg_logprob": -0.20108488202095032, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0010643680579960346}, {"id": 502, "seek": 229136, "start": 2300.1200000000003, "end": 2314.88, "text": " Otherwise, a lot of this should be recognizable, reproducibility, read data, get the encoder and decoder, create the train and test splits, use the kind of like data loader that gets a batch of the inputs and targets.", "tokens": [50803, 10328, 11, 257, 688, 295, 341, 820, 312, 40757, 11, 11408, 537, 39802, 11, 1401, 1412, 11, 483, 264, 2058, 19866, 293, 979, 19866, 11, 1884, 264, 3847, 293, 1500, 37741, 11, 764, 264, 733, 295, 411, 1412, 3677, 260, 300, 2170, 257, 15245, 295, 264, 15743, 293, 12911, 13, 51541], "temperature": 0.0, "avg_logprob": -0.20108488202095032, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0010643680579960346}, {"id": 503, "seek": 229136, "start": 2315.84, "end": 2317.96, "text": " This is new, and I'll talk about it in a second.", "tokens": [51589, 639, 307, 777, 11, 293, 286, 603, 751, 466, 309, 294, 257, 1150, 13, 51695], "temperature": 0.0, "avg_logprob": -0.20108488202095032, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0010643680579960346}, {"id": 504, "seek": 229136, "start": 2319.02, "end": 2321.0, "text": " Now, this is the bigram language model that we developed.", "tokens": [51748, 823, 11, 341, 307, 264, 955, 2356, 2856, 2316, 300, 321, 4743, 13, 51847], "temperature": 0.0, "avg_logprob": -0.20108488202095032, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.0010643680579960346}, {"id": 505, "seek": 232136, "start": 2321.7200000000003, "end": 2324.9, "text": " And it can forward and give us a logits and loss and it can generate.", "tokens": [50383, 400, 309, 393, 2128, 293, 976, 505, 257, 3565, 1208, 293, 4470, 293, 309, 393, 8460, 13, 50542], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 506, "seek": 232136, "start": 2326.8, "end": 2329.98, "text": " And then here we are creating the optimizer and this is the training loop.", "tokens": [50637, 400, 550, 510, 321, 366, 4084, 264, 5028, 6545, 293, 341, 307, 264, 3097, 6367, 13, 50796], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 507, "seek": 232136, "start": 2331.96, "end": 2334.04, "text": " So everything here should look pretty familiar.", "tokens": [50895, 407, 1203, 510, 820, 574, 1238, 4963, 13, 50999], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 508, "seek": 232136, "start": 2334.1600000000003, "end": 2336.08, "text": " Now, some of the small things that I added.", "tokens": [51005, 823, 11, 512, 295, 264, 1359, 721, 300, 286, 3869, 13, 51101], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 509, "seek": 232136, "start": 2336.2000000000003, "end": 2340.28, "text": " Number one, I added the ability to run on a GPU if you have it.", "tokens": [51107, 5118, 472, 11, 286, 3869, 264, 3485, 281, 1190, 322, 257, 18407, 498, 291, 362, 309, 13, 51311], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 510, "seek": 232136, "start": 2340.76, "end": 2346.6, "text": " So if you have a GPU, then you can, this will use CUDA instead of just CPU and everything will be a lot more faster.", "tokens": [51335, 407, 498, 291, 362, 257, 18407, 11, 550, 291, 393, 11, 341, 486, 764, 29777, 7509, 2602, 295, 445, 13199, 293, 1203, 486, 312, 257, 688, 544, 4663, 13, 51627], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 511, "seek": 232136, "start": 2347.2200000000003, "end": 2351.34, "text": " Now, when device becomes CUDA, then we need to make sure that when we load the data.", "tokens": [51658, 823, 11, 562, 4302, 3643, 29777, 7509, 11, 550, 321, 643, 281, 652, 988, 300, 562, 321, 3677, 264, 1412, 13, 51864], "temperature": 0.0, "avg_logprob": -0.1836221609542619, "compression_ratio": 1.725085910652921, "no_speech_prob": 0.001263097976334393}, {"id": 512, "seek": 235136, "start": 2351.46, "end": 2352.96, "text": " We move it to device.", "tokens": [50370, 492, 1286, 309, 281, 4302, 13, 50445], "temperature": 0.0, "avg_logprob": -0.21395837059316708, "compression_ratio": 1.7535211267605635, "no_speech_prob": 0.003494762582704425}, {"id": 513, "seek": 235136, "start": 2353.96, "end": 2358.46, "text": " When we create the model, we want to move the model parameters to device.", "tokens": [50495, 1133, 321, 1884, 264, 2316, 11, 321, 528, 281, 1286, 264, 2316, 9834, 281, 4302, 13, 50720], "temperature": 0.0, "avg_logprob": -0.21395837059316708, "compression_ratio": 1.7535211267605635, "no_speech_prob": 0.003494762582704425}, {"id": 514, "seek": 235136, "start": 2358.96, "end": 2366.96, "text": " So as an example, here we have the in an embedding table and it's got a dot weight inside it, which stores the sort of lookup table.", "tokens": [50745, 407, 382, 364, 1365, 11, 510, 321, 362, 264, 294, 364, 12240, 3584, 3199, 293, 309, 311, 658, 257, 5893, 3364, 1854, 309, 11, 597, 9512, 264, 1333, 295, 574, 1010, 3199, 13, 51145], "temperature": 0.0, "avg_logprob": -0.21395837059316708, "compression_ratio": 1.7535211267605635, "no_speech_prob": 0.003494762582704425}, {"id": 515, "seek": 235136, "start": 2367.1600000000003, "end": 2373.1600000000003, "text": " So that would be moved to the GPU so that all the calculations here happen on the GPU and they can be a lot faster.", "tokens": [51155, 407, 300, 576, 312, 4259, 281, 264, 18407, 370, 300, 439, 264, 20448, 510, 1051, 322, 264, 18407, 293, 436, 393, 312, 257, 688, 4663, 13, 51455], "temperature": 0.0, "avg_logprob": -0.21395837059316708, "compression_ratio": 1.7535211267605635, "no_speech_prob": 0.003494762582704425}, {"id": 516, "seek": 235136, "start": 2373.96, "end": 2379.46, "text": " And then finally here, when I'm creating the context that feeds into generate, I have to make sure that I create on the device.", "tokens": [51495, 400, 550, 2721, 510, 11, 562, 286, 478, 4084, 264, 4319, 300, 23712, 666, 8460, 11, 286, 362, 281, 652, 988, 300, 286, 1884, 322, 264, 4302, 13, 51770], "temperature": 0.0, "avg_logprob": -0.21395837059316708, "compression_ratio": 1.7535211267605635, "no_speech_prob": 0.003494762582704425}, {"id": 517, "seek": 235136, "start": 2380.36, "end": 2381.26, "text": " Number two, when I enter.", "tokens": [51815, 5118, 732, 11, 562, 286, 3242, 13, 51860], "temperature": 0.0, "avg_logprob": -0.21395837059316708, "compression_ratio": 1.7535211267605635, "no_speech_prob": 0.003494762582704425}, {"id": 518, "seek": 238136, "start": 2381.46, "end": 2385.96, "text": " Introduced is the fact that here in the training loop.", "tokens": [50370, 27193, 1232, 307, 264, 1186, 300, 510, 294, 264, 3097, 6367, 13, 50595], "temperature": 0.0, "avg_logprob": -0.1904992509162289, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0019991809967905283}, {"id": 519, "seek": 238136, "start": 2387.6600000000003, "end": 2392.96, "text": " Here, I was just printing the loss dot item inside the training loop.", "tokens": [50680, 1692, 11, 286, 390, 445, 14699, 264, 4470, 5893, 3174, 1854, 264, 3097, 6367, 13, 50945], "temperature": 0.0, "avg_logprob": -0.1904992509162289, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0019991809967905283}, {"id": 520, "seek": 238136, "start": 2393.1600000000003, "end": 2397.96, "text": " But this is a very noisy measurement of the current loss because every batch will be more or less lucky.", "tokens": [50955, 583, 341, 307, 257, 588, 24518, 13160, 295, 264, 2190, 4470, 570, 633, 15245, 486, 312, 544, 420, 1570, 6356, 13, 51195], "temperature": 0.0, "avg_logprob": -0.1904992509162289, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0019991809967905283}, {"id": 521, "seek": 238136, "start": 2398.6600000000003, "end": 2411.1600000000003, "text": " And so what I want to do usually is I have an estimate loss function and the estimate loss basically then goes up here and it averages up.", "tokens": [51230, 400, 370, 437, 286, 528, 281, 360, 2673, 307, 286, 362, 364, 12539, 4470, 2445, 293, 264, 12539, 4470, 1936, 550, 1709, 493, 510, 293, 309, 42257, 493, 13, 51855], "temperature": 0.0, "avg_logprob": -0.1904992509162289, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.0019991809967905283}, {"id": 522, "seek": 241116, "start": 2411.16, "end": 2413.06, "text": " The loss over multiple batches.", "tokens": [50365, 440, 4470, 670, 3866, 15245, 279, 13, 50460], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 523, "seek": 241116, "start": 2413.56, "end": 2422.16, "text": " So in particular, we're going to iterate eval, either times and we're going to basically get our loss and then we're going to get the average loss for both splits.", "tokens": [50485, 407, 294, 1729, 11, 321, 434, 516, 281, 44497, 1073, 304, 11, 2139, 1413, 293, 321, 434, 516, 281, 1936, 483, 527, 4470, 293, 550, 321, 434, 516, 281, 483, 264, 4274, 4470, 337, 1293, 37741, 13, 50915], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 524, "seek": 241116, "start": 2422.56, "end": 2424.16, "text": " And so this will be a lot less noisy.", "tokens": [50935, 400, 370, 341, 486, 312, 257, 688, 1570, 24518, 13, 51015], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 525, "seek": 241116, "start": 2425.06, "end": 2430.7599999999998, "text": " So here when we call the estimate loss, we're going to report the pretty accurate train and validation loss.", "tokens": [51060, 407, 510, 562, 321, 818, 264, 12539, 4470, 11, 321, 434, 516, 281, 2275, 264, 1238, 8559, 3847, 293, 24071, 4470, 13, 51345], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 526, "seek": 241116, "start": 2431.96, "end": 2434.56, "text": " Now when we come back up, you'll notice a few things here.", "tokens": [51405, 823, 562, 321, 808, 646, 493, 11, 291, 603, 3449, 257, 1326, 721, 510, 13, 51535], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 527, "seek": 241116, "start": 2434.7599999999998, "end": 2438.2599999999998, "text": " I'm setting the model to evaluation phase and down here.", "tokens": [51545, 286, 478, 3287, 264, 2316, 281, 13344, 5574, 293, 760, 510, 13, 51720], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 528, "seek": 241116, "start": 2438.2599999999998, "end": 2440.2599999999998, "text": " I'm resetting it back to training phase.", "tokens": [51720, 286, 478, 14322, 783, 309, 646, 281, 3097, 5574, 13, 51820], "temperature": 0.0, "avg_logprob": -0.15096806370934776, "compression_ratio": 1.8345588235294117, "no_speech_prob": 0.00032874857424758375}, {"id": 529, "seek": 244026, "start": 2440.26, "end": 2456.0600000000004, "text": " Now right now for our model as is, this doesn't actually do anything because the only thing inside this model is this nn.embedding and this network would behave the same in both evaluation mode and training mode.", "tokens": [50365, 823, 558, 586, 337, 527, 2316, 382, 307, 11, 341, 1177, 380, 767, 360, 1340, 570, 264, 787, 551, 1854, 341, 2316, 307, 341, 297, 77, 13, 443, 2883, 3584, 293, 341, 3209, 576, 15158, 264, 912, 294, 1293, 13344, 4391, 293, 3097, 4391, 13, 51155], "temperature": 0.0, "avg_logprob": -0.2037902303261332, "compression_ratio": 1.752, "no_speech_prob": 0.0004273691156413406}, {"id": 530, "seek": 244026, "start": 2456.46, "end": 2457.6600000000003, "text": " We have no dropout layers.", "tokens": [51175, 492, 362, 572, 3270, 346, 7914, 13, 51235], "temperature": 0.0, "avg_logprob": -0.2037902303261332, "compression_ratio": 1.752, "no_speech_prob": 0.0004273691156413406}, {"id": 531, "seek": 244026, "start": 2457.6600000000003, "end": 2459.1600000000003, "text": " We have no batch drum layers, etc.", "tokens": [51235, 492, 362, 572, 15245, 10206, 7914, 11, 5183, 13, 51310], "temperature": 0.0, "avg_logprob": -0.2037902303261332, "compression_ratio": 1.752, "no_speech_prob": 0.0004273691156413406}, {"id": 532, "seek": 244026, "start": 2459.6600000000003, "end": 2468.46, "text": " But it is a good practice to think through what mode your neural network is in because some layers will have different behavior at inference time or training time.", "tokens": [51335, 583, 309, 307, 257, 665, 3124, 281, 519, 807, 437, 4391, 428, 18161, 3209, 307, 294, 570, 512, 7914, 486, 362, 819, 5223, 412, 38253, 565, 420, 3097, 565, 13, 51775], "temperature": 0.0, "avg_logprob": -0.2037902303261332, "compression_ratio": 1.752, "no_speech_prob": 0.0004273691156413406}, {"id": 533, "seek": 246846, "start": 2468.46, "end": 2479.36, "text": " And there's also this context manager, torch.nograd, and this is just telling PyTorch that everything that happens inside this function, we will not call .backward on.", "tokens": [50365, 400, 456, 311, 611, 341, 4319, 6598, 11, 27822, 13, 77, 664, 6206, 11, 293, 341, 307, 445, 3585, 9953, 51, 284, 339, 300, 1203, 300, 2314, 1854, 341, 2445, 11, 321, 486, 406, 818, 2411, 3207, 1007, 322, 13, 50910], "temperature": 0.0, "avg_logprob": -0.13929966517857142, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0003046590427402407}, {"id": 534, "seek": 246846, "start": 2480.06, "end": 2488.16, "text": " And so PyTorch can be a lot more efficient with its memory use because it doesn't have to store all the intermediate variables because we're never going to call backward.", "tokens": [50945, 400, 370, 9953, 51, 284, 339, 393, 312, 257, 688, 544, 7148, 365, 1080, 4675, 764, 570, 309, 1177, 380, 362, 281, 3531, 439, 264, 19376, 9102, 570, 321, 434, 1128, 516, 281, 818, 23897, 13, 51350], "temperature": 0.0, "avg_logprob": -0.13929966517857142, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0003046590427402407}, {"id": 535, "seek": 246846, "start": 2488.66, "end": 2491.56, "text": " And so it can be a lot more efficient in that way.", "tokens": [51375, 400, 370, 309, 393, 312, 257, 688, 544, 7148, 294, 300, 636, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13929966517857142, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0003046590427402407}, {"id": 536, "seek": 246846, "start": 2491.86, "end": 2496.56, "text": " So also a good practice to tell PyTorch when we don't intend to do backpropagation.", "tokens": [51535, 407, 611, 257, 665, 3124, 281, 980, 9953, 51, 284, 339, 562, 321, 500, 380, 19759, 281, 360, 646, 79, 1513, 559, 399, 13, 51770], "temperature": 0.0, "avg_logprob": -0.13929966517857142, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0003046590427402407}, {"id": 537, "seek": 246846, "start": 2497.66, "end": 2498.16, "text": " So,", "tokens": [51825, 407, 11, 51850], "temperature": 0.0, "avg_logprob": -0.13929966517857142, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0003046590427402407}, {"id": 538, "seek": 249846, "start": 2498.46, "end": 2498.96, "text": " right now,", "tokens": [50365, 558, 586, 11, 50390], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 539, "seek": 249846, "start": 2498.96, "end": 2504.56, "text": " this script is about 120 lines of code of and that's kind of our starter code.", "tokens": [50390, 341, 5755, 307, 466, 10411, 3876, 295, 3089, 295, 293, 300, 311, 733, 295, 527, 22465, 3089, 13, 50670], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 540, "seek": 249846, "start": 2505.36, "end": 2508.36, "text": " I'm calling it bigram.py and I'm going to release it later.", "tokens": [50710, 286, 478, 5141, 309, 955, 2356, 13, 8200, 293, 286, 478, 516, 281, 4374, 309, 1780, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 541, "seek": 249846, "start": 2508.96, "end": 2513.86, "text": " Now running this script gives us output in the terminal and it looks something like this.", "tokens": [50890, 823, 2614, 341, 5755, 2709, 505, 5598, 294, 264, 14709, 293, 309, 1542, 746, 411, 341, 13, 51135], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 542, "seek": 249846, "start": 2514.86, "end": 2519.56, "text": " It basically, as I ran this code, it was giving me the train loss and val loss.", "tokens": [51185, 467, 1936, 11, 382, 286, 5872, 341, 3089, 11, 309, 390, 2902, 385, 264, 3847, 4470, 293, 1323, 4470, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 543, "seek": 249846, "start": 2519.76, "end": 2523.96, "text": " And we see that we convert to somewhere around 2.5 with the bigram model.", "tokens": [51430, 400, 321, 536, 300, 321, 7620, 281, 4079, 926, 568, 13, 20, 365, 264, 955, 2356, 2316, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 544, "seek": 249846, "start": 2524.46, "end": 2526.96, "text": " And then here's the sample that we produced at the end.", "tokens": [51665, 400, 550, 510, 311, 264, 6889, 300, 321, 7126, 412, 264, 917, 13, 51790], "temperature": 0.0, "avg_logprob": -0.1318007067215344, "compression_ratio": 1.6879699248120301, "no_speech_prob": 0.0009523520129732788}, {"id": 545, "seek": 252846, "start": 2528.46, "end": 2533.06, "text": " And so we have everything packaged up in the script and we're in a good position now to iterate on this.", "tokens": [50365, 400, 370, 321, 362, 1203, 38162, 493, 294, 264, 5755, 293, 321, 434, 294, 257, 665, 2535, 586, 281, 44497, 322, 341, 13, 50595], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 546, "seek": 252846, "start": 2533.46, "end": 2533.66, "text": " Okay,", "tokens": [50615, 1033, 11, 50625], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 547, "seek": 252846, "start": 2533.66, "end": 2540.46, "text": " so we are almost ready to start writing our very first self-attention block for processing these tokens.", "tokens": [50625, 370, 321, 366, 1920, 1919, 281, 722, 3579, 527, 588, 700, 2698, 12, 1591, 1251, 3461, 337, 9007, 613, 22667, 13, 50965], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 548, "seek": 252846, "start": 2541.16, "end": 2541.66, "text": " Now,", "tokens": [51000, 823, 11, 51025], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 549, "seek": 252846, "start": 2542.26, "end": 2543.36, "text": " before we actually get there,", "tokens": [51055, 949, 321, 767, 483, 456, 11, 51110], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 550, "seek": 252846, "start": 2543.56, "end": 2553.96, "text": " I want to get you used to a mathematical trick that is used in the self-attention inside a transformer and is really just like at the heart of an efficient implementation of self-attention.", "tokens": [51120, 286, 528, 281, 483, 291, 1143, 281, 257, 18894, 4282, 300, 307, 1143, 294, 264, 2698, 12, 1591, 1251, 1854, 257, 31782, 293, 307, 534, 445, 411, 412, 264, 1917, 295, 364, 7148, 11420, 295, 2698, 12, 1591, 1251, 13, 51640], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 551, "seek": 252846, "start": 2554.66, "end": 2558.16, "text": " And so I want to work with this toy example to just get you used to this operation.", "tokens": [51675, 400, 370, 286, 528, 281, 589, 365, 341, 12058, 1365, 281, 445, 483, 291, 1143, 281, 341, 6916, 13, 51850], "temperature": 0.0, "avg_logprob": -0.09630594402551651, "compression_ratio": 1.7643097643097643, "no_speech_prob": 9.090992534765974e-05}, {"id": 552, "seek": 255846, "start": 2558.46, "end": 2564.16, "text": " And then it's going to make it much more clear once we actually get to it in the script again.", "tokens": [50365, 400, 550, 309, 311, 516, 281, 652, 309, 709, 544, 1850, 1564, 321, 767, 483, 281, 309, 294, 264, 5755, 797, 13, 50650], "temperature": 0.0, "avg_logprob": -0.13590234120686848, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0003057437133975327}, {"id": 553, "seek": 255846, "start": 2565.46, "end": 2570.46, "text": " So let's create a B by T by C where B, T and C are just 4, 8 and 2 in this toy example.", "tokens": [50715, 407, 718, 311, 1884, 257, 363, 538, 314, 538, 383, 689, 363, 11, 314, 293, 383, 366, 445, 1017, 11, 1649, 293, 568, 294, 341, 12058, 1365, 13, 50965], "temperature": 0.0, "avg_logprob": -0.13590234120686848, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0003057437133975327}, {"id": 554, "seek": 255846, "start": 2571.26, "end": 2579.66, "text": " And these are basically channels and we have batches and we have the time component and we have some information at each point in the sequence.", "tokens": [51005, 400, 613, 366, 1936, 9235, 293, 321, 362, 15245, 279, 293, 321, 362, 264, 565, 6542, 293, 321, 362, 512, 1589, 412, 1184, 935, 294, 264, 8310, 13, 51425], "temperature": 0.0, "avg_logprob": -0.13590234120686848, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0003057437133975327}, {"id": 555, "seek": 255846, "start": 2579.96, "end": 2580.56, "text": " So C.", "tokens": [51440, 407, 383, 13, 51470], "temperature": 0.0, "avg_logprob": -0.13590234120686848, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0003057437133975327}, {"id": 556, "seek": 255846, "start": 2582.06, "end": 2585.66, "text": " Now what we would like to do is we would like these tokens.", "tokens": [51545, 823, 437, 321, 576, 411, 281, 360, 307, 321, 576, 411, 613, 22667, 13, 51725], "temperature": 0.0, "avg_logprob": -0.13590234120686848, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0003057437133975327}, {"id": 557, "seek": 255846, "start": 2585.76, "end": 2588.36, "text": " So we have up to eight tokens here in a batch.", "tokens": [51730, 407, 321, 362, 493, 281, 3180, 22667, 510, 294, 257, 15245, 13, 51860], "temperature": 0.0, "avg_logprob": -0.13590234120686848, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0003057437133975327}, {"id": 558, "seek": 258846, "start": 2588.66, "end": 2592.66, "text": " And these eight tokens are currently not talking to each other and we would like them to talk to each other.", "tokens": [50375, 400, 613, 3180, 22667, 366, 4362, 406, 1417, 281, 1184, 661, 293, 321, 576, 411, 552, 281, 751, 281, 1184, 661, 13, 50575], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 559, "seek": 258846, "start": 2592.66, "end": 2593.66, "text": " We'd like to couple them.", "tokens": [50575, 492, 1116, 411, 281, 1916, 552, 13, 50625], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 560, "seek": 258846, "start": 2594.86, "end": 2596.26, "text": " And in particular,", "tokens": [50685, 400, 294, 1729, 11, 50755], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 561, "seek": 258846, "start": 2596.66, "end": 2599.46, "text": " we don't we want to couple them in this very specific way.", "tokens": [50775, 321, 500, 380, 321, 528, 281, 1916, 552, 294, 341, 588, 2685, 636, 13, 50915], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 562, "seek": 258846, "start": 2599.96, "end": 2600.96, "text": " So the token,", "tokens": [50940, 407, 264, 14862, 11, 50990], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 563, "seek": 258846, "start": 2600.96, "end": 2601.36, "text": " for example,", "tokens": [50990, 337, 1365, 11, 51010], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 564, "seek": 258846, "start": 2601.36, "end": 2602.56, "text": " at the fifth location,", "tokens": [51010, 412, 264, 9266, 4914, 11, 51070], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 565, "seek": 258846, "start": 2603.06, "end": 2610.46, "text": " it should not communicate with tokens in the sixth seventh and eighth location because those are future tokens in the sequence.", "tokens": [51095, 309, 820, 406, 7890, 365, 22667, 294, 264, 15102, 17875, 293, 19495, 4914, 570, 729, 366, 2027, 22667, 294, 264, 8310, 13, 51465], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 566, "seek": 258846, "start": 2611.06, "end": 2615.56, "text": " The token on the fifth location should only talk to the one in the fourth third second and first.", "tokens": [51495, 440, 14862, 322, 264, 9266, 4914, 820, 787, 751, 281, 264, 472, 294, 264, 6409, 2636, 1150, 293, 700, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 567, "seek": 258846, "start": 2616.06, "end": 2618.46, "text": " So it's only so information only flows.", "tokens": [51745, 407, 309, 311, 787, 370, 1589, 787, 12867, 13, 51865], "temperature": 0.0, "avg_logprob": -0.13338857752676228, "compression_ratio": 1.8790035587188612, "no_speech_prob": 0.00012224598322063684}, {"id": 568, "seek": 261846, "start": 2618.56, "end": 2625.26, "text": " From previous context to the current time step and we cannot get any information from the future because we are about to try to predict the future.", "tokens": [50370, 3358, 3894, 4319, 281, 264, 2190, 565, 1823, 293, 321, 2644, 483, 604, 1589, 490, 264, 2027, 570, 321, 366, 466, 281, 853, 281, 6069, 264, 2027, 13, 50705], "temperature": 0.0, "avg_logprob": -0.12455918712000694, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.00017906524590216577}, {"id": 569, "seek": 261846, "start": 2626.46, "end": 2630.56, "text": " So what is the easiest way for tokens to communicate?", "tokens": [50765, 407, 437, 307, 264, 12889, 636, 337, 22667, 281, 7890, 30, 50970], "temperature": 0.0, "avg_logprob": -0.12455918712000694, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.00017906524590216577}, {"id": 570, "seek": 261846, "start": 2630.96, "end": 2633.86, "text": " Okay, the easiest way I would say is okay.", "tokens": [50990, 1033, 11, 264, 12889, 636, 286, 576, 584, 307, 1392, 13, 51135], "temperature": 0.0, "avg_logprob": -0.12455918712000694, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.00017906524590216577}, {"id": 571, "seek": 261846, "start": 2633.86, "end": 2645.86, "text": " If we are up to if we're a fifth token and I'd like to communicate with my past the simplest way we can do that is to just do a weight is to just do an average of all the of all the preceding elements.", "tokens": [51135, 759, 321, 366, 493, 281, 498, 321, 434, 257, 9266, 14862, 293, 286, 1116, 411, 281, 7890, 365, 452, 1791, 264, 22811, 636, 321, 393, 360, 300, 307, 281, 445, 360, 257, 3364, 307, 281, 445, 360, 364, 4274, 295, 439, 264, 295, 439, 264, 16969, 278, 4959, 13, 51735], "temperature": 0.0, "avg_logprob": -0.12455918712000694, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.00017906524590216577}, {"id": 572, "seek": 261846, "start": 2646.16, "end": 2646.76, "text": " So for example,", "tokens": [51750, 407, 337, 1365, 11, 51780], "temperature": 0.0, "avg_logprob": -0.12455918712000694, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.00017906524590216577}, {"id": 573, "seek": 261846, "start": 2646.76, "end": 2647.66, "text": " if I'm the fifth token,", "tokens": [51780, 498, 286, 478, 264, 9266, 14862, 11, 51825], "temperature": 0.0, "avg_logprob": -0.12455918712000694, "compression_ratio": 1.8134328358208955, "no_speech_prob": 0.00017906524590216577}, {"id": 574, "seek": 264766, "start": 2647.7599999999998, "end": 2653.2599999999998, "text": " I would like to take the channels that make up that are information at my step,", "tokens": [50370, 286, 576, 411, 281, 747, 264, 9235, 300, 652, 493, 300, 366, 1589, 412, 452, 1823, 11, 50645], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 575, "seek": 264766, "start": 2653.66, "end": 2657.46, "text": " but then also the channels from the fourth step third step second step in the first step.", "tokens": [50665, 457, 550, 611, 264, 9235, 490, 264, 6409, 1823, 2636, 1823, 1150, 1823, 294, 264, 700, 1823, 13, 50855], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 576, "seek": 264766, "start": 2657.66, "end": 2664.96, "text": " I'd like to average those up and then that would become sort of like a feature vector that summarizes me in the context of my history.", "tokens": [50865, 286, 1116, 411, 281, 4274, 729, 493, 293, 550, 300, 576, 1813, 1333, 295, 411, 257, 4111, 8062, 300, 14611, 5660, 385, 294, 264, 4319, 295, 452, 2503, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 577, "seek": 264766, "start": 2665.66, "end": 2665.8599999999997, "text": " Now,", "tokens": [51265, 823, 11, 51275], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 578, "seek": 264766, "start": 2665.8599999999997, "end": 2666.16, "text": " of course,", "tokens": [51275, 295, 1164, 11, 51290], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 579, "seek": 264766, "start": 2666.16, "end": 2670.2599999999998, "text": " just doing a sum or like an average is an extremely weak form of interaction.", "tokens": [51290, 445, 884, 257, 2408, 420, 411, 364, 4274, 307, 364, 4664, 5336, 1254, 295, 9285, 13, 51495], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 580, "seek": 264766, "start": 2670.2599999999998, "end": 2672.46, "text": " Like this communication is extremely lossy.", "tokens": [51495, 1743, 341, 6101, 307, 4664, 4470, 88, 13, 51605], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 581, "seek": 264766, "start": 2672.66, "end": 2676.16, "text": " We've lost a ton of information about spatial arrangements of all those tokens,", "tokens": [51615, 492, 600, 2731, 257, 2952, 295, 1589, 466, 23598, 22435, 295, 439, 729, 22667, 11, 51790], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 582, "seek": 264766, "start": 2676.96, "end": 2677.56, "text": " but that's okay.", "tokens": [51830, 457, 300, 311, 1392, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1309983389718192, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00016591831808909774}, {"id": 583, "seek": 267756, "start": 2677.66, "end": 2678.16, "text": " For now,", "tokens": [50370, 1171, 586, 11, 50395], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 584, "seek": 267756, "start": 2678.16, "end": 2680.2599999999998, "text": " we'll see how we can bring that information back later.", "tokens": [50395, 321, 603, 536, 577, 321, 393, 1565, 300, 1589, 646, 1780, 13, 50500], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 585, "seek": 267756, "start": 2681.06, "end": 2681.36, "text": " For now,", "tokens": [50540, 1171, 586, 11, 50555], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 586, "seek": 267756, "start": 2681.36, "end": 2688.7599999999998, "text": " what we would like to do is for every single batch element independently for every teeth token in that sequence.", "tokens": [50555, 437, 321, 576, 411, 281, 360, 307, 337, 633, 2167, 15245, 4478, 21761, 337, 633, 7798, 14862, 294, 300, 8310, 13, 50925], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 587, "seek": 267756, "start": 2689.16, "end": 2696.56, "text": " We'd like to now calculate the average of all the vectors in all the previous tokens and also at this token.", "tokens": [50945, 492, 1116, 411, 281, 586, 8873, 264, 4274, 295, 439, 264, 18875, 294, 439, 264, 3894, 22667, 293, 611, 412, 341, 14862, 13, 51315], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 588, "seek": 267756, "start": 2697.46, "end": 2698.46, "text": " So let's write that out.", "tokens": [51360, 407, 718, 311, 2464, 300, 484, 13, 51410], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 589, "seek": 267756, "start": 2699.96, "end": 2702.96, "text": " I have a small snippet here and instead of just fumbling around,", "tokens": [51485, 286, 362, 257, 1359, 35623, 302, 510, 293, 2602, 295, 445, 283, 14188, 926, 11, 51635], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 590, "seek": 267756, "start": 2703.56, "end": 2705.16, "text": " let me just copy paste it and talk to it.", "tokens": [51665, 718, 385, 445, 5055, 9163, 309, 293, 751, 281, 309, 13, 51745], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 591, "seek": 267756, "start": 2706.56, "end": 2707.46, "text": " So in other words,", "tokens": [51815, 407, 294, 661, 2283, 11, 51860], "temperature": 0.0, "avg_logprob": -0.11498554614411682, "compression_ratio": 1.664179104477612, "no_speech_prob": 1.5086166058608796e-05}, {"id": 592, "seek": 270756, "start": 2708.16, "end": 2718.96, "text": " we're going to create X and B O W is short for bag of words because bag of words is is kind of like a term that people use when you are just averaging up things.", "tokens": [50395, 321, 434, 516, 281, 1884, 1783, 293, 363, 422, 343, 307, 2099, 337, 3411, 295, 2283, 570, 3411, 295, 2283, 307, 307, 733, 295, 411, 257, 1433, 300, 561, 764, 562, 291, 366, 445, 47308, 493, 721, 13, 50935], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 593, "seek": 270756, "start": 2718.96, "end": 2720.36, "text": " So this is just a bag of words.", "tokens": [50935, 407, 341, 307, 445, 257, 3411, 295, 2283, 13, 51005], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 594, "seek": 270756, "start": 2720.66, "end": 2720.96, "text": " Basically,", "tokens": [51020, 8537, 11, 51035], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 595, "seek": 270756, "start": 2720.96, "end": 2726.2599999999998, "text": " there's a word stored on every one of these eight locations and we're doing a bag of words for just averaging.", "tokens": [51035, 456, 311, 257, 1349, 12187, 322, 633, 472, 295, 613, 3180, 9253, 293, 321, 434, 884, 257, 3411, 295, 2283, 337, 445, 47308, 13, 51300], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 596, "seek": 270756, "start": 2727.46, "end": 2728.2599999999998, "text": " So in the beginning,", "tokens": [51360, 407, 294, 264, 2863, 11, 51400], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 597, "seek": 270756, "start": 2728.2599999999998, "end": 2731.96, "text": " we're going to say that it's just initialized at zero and then I'm doing a for loop here.", "tokens": [51400, 321, 434, 516, 281, 584, 300, 309, 311, 445, 5883, 1602, 412, 4018, 293, 550, 286, 478, 884, 257, 337, 6367, 510, 13, 51585], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 598, "seek": 270756, "start": 2731.96, "end": 2733.2599999999998, "text": " So we're not being efficient yet.", "tokens": [51585, 407, 321, 434, 406, 885, 7148, 1939, 13, 51650], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 599, "seek": 270756, "start": 2733.2599999999998, "end": 2733.96, "text": " That's coming.", "tokens": [51650, 663, 311, 1348, 13, 51685], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 600, "seek": 270756, "start": 2734.56, "end": 2734.96, "text": " But for now,", "tokens": [51715, 583, 337, 586, 11, 51735], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 601, "seek": 270756, "start": 2734.96, "end": 2737.46, "text": " we're just iterating over all the batch dimensions independently.", "tokens": [51735, 321, 434, 445, 17138, 990, 670, 439, 264, 15245, 12819, 21761, 13, 51860], "temperature": 0.0, "avg_logprob": -0.14809102376302083, "compression_ratio": 1.8344370860927153, "no_speech_prob": 0.0001421497145202011}, {"id": 602, "seek": 273756, "start": 2738.06, "end": 2739.46, "text": " Iterating over time", "tokens": [50390, 286, 391, 990, 670, 565, 50460], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 603, "seek": 273756, "start": 2740.16, "end": 2745.86, "text": " and then the previous tokens are at this batch dimension", "tokens": [50495, 293, 550, 264, 3894, 22667, 366, 412, 341, 15245, 10139, 50780], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 604, "seek": 273756, "start": 2746.36, "end": 2749.36, "text": " and then everything up to and including the teeth token.", "tokens": [50805, 293, 550, 1203, 493, 281, 293, 3009, 264, 7798, 14862, 13, 50955], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 605, "seek": 273756, "start": 2749.86, "end": 2750.2599999999998, "text": " Okay.", "tokens": [50980, 1033, 13, 51000], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 606, "seek": 273756, "start": 2750.96, "end": 2753.06, "text": " So when we slice out X in this way,", "tokens": [51035, 407, 562, 321, 13153, 484, 1783, 294, 341, 636, 11, 51140], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 607, "seek": 273756, "start": 2753.56, "end": 2755.66, "text": " Xprev becomes of shape,", "tokens": [51165, 1783, 3712, 85, 3643, 295, 3909, 11, 51270], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 608, "seek": 273756, "start": 2756.86, "end": 2760.96, "text": " how many T elements there were in the past and then of course C.", "tokens": [51330, 577, 867, 314, 4959, 456, 645, 294, 264, 1791, 293, 550, 295, 1164, 383, 13, 51535], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 609, "seek": 273756, "start": 2760.96, "end": 2764.2599999999998, "text": " So all the two-dimensional information from these little tokens.", "tokens": [51535, 407, 439, 264, 732, 12, 18759, 1589, 490, 613, 707, 22667, 13, 51700], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 610, "seek": 273756, "start": 2765.2599999999998, "end": 2767.46, "text": " So that's the previous sort of chunk of", "tokens": [51750, 407, 300, 311, 264, 3894, 1333, 295, 16635, 295, 51860], "temperature": 0.0, "avg_logprob": -0.18366655349731445, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00035031422157771885}, {"id": 611, "seek": 276756, "start": 2767.56, "end": 2771.2599999999998, "text": " tokens from my current sequence.", "tokens": [50365, 22667, 490, 452, 2190, 8310, 13, 50550], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 612, "seek": 276756, "start": 2771.96, "end": 2775.56, "text": " And then I'm just doing the average or the mean over the zero dimension.", "tokens": [50585, 400, 550, 286, 478, 445, 884, 264, 4274, 420, 264, 914, 670, 264, 4018, 10139, 13, 50765], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 613, "seek": 276756, "start": 2775.56, "end": 2781.66, "text": " So I'm averaging out the time here and I'm just going to get a little C one-dimensional vector,", "tokens": [50765, 407, 286, 478, 47308, 484, 264, 565, 510, 293, 286, 478, 445, 516, 281, 483, 257, 707, 383, 472, 12, 18759, 8062, 11, 51070], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 614, "seek": 276756, "start": 2781.66, "end": 2784.36, "text": " which I'm going to store in X bag of words.", "tokens": [51070, 597, 286, 478, 516, 281, 3531, 294, 1783, 3411, 295, 2283, 13, 51205], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 615, "seek": 276756, "start": 2785.16, "end": 2791.56, "text": " So I can run this and this is not going to be very informative because let's see.", "tokens": [51245, 407, 286, 393, 1190, 341, 293, 341, 307, 406, 516, 281, 312, 588, 27759, 570, 718, 311, 536, 13, 51565], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 616, "seek": 276756, "start": 2791.56, "end": 2792.56, "text": " So this is X of zero.", "tokens": [51565, 407, 341, 307, 1783, 295, 4018, 13, 51615], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 617, "seek": 276756, "start": 2792.56, "end": 2796.66, "text": " So this is the zeroth batch element and then expo at zero.", "tokens": [51615, 407, 341, 307, 264, 44746, 900, 15245, 4478, 293, 550, 1278, 78, 412, 4018, 13, 51820], "temperature": 0.0, "avg_logprob": -0.16837869610702783, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.0003600776253733784}, {"id": 618, "seek": 279666, "start": 2797.16, "end": 2797.66, "text": " Now,", "tokens": [50390, 823, 11, 50415], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 619, "seek": 279666, "start": 2798.56, "end": 2799.46, "text": " you see how the", "tokens": [50460, 291, 536, 577, 264, 50505], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 620, "seek": 279666, "start": 2799.96, "end": 2801.2599999999998, "text": " at the first location here,", "tokens": [50530, 412, 264, 700, 4914, 510, 11, 50595], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 621, "seek": 279666, "start": 2801.66, "end": 2803.3599999999997, "text": " you see that the two are equal", "tokens": [50615, 291, 536, 300, 264, 732, 366, 2681, 50700], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 622, "seek": 279666, "start": 2803.8599999999997, "end": 2806.8599999999997, "text": " and that's because it's we're just doing an average of this one token.", "tokens": [50725, 293, 300, 311, 570, 309, 311, 321, 434, 445, 884, 364, 4274, 295, 341, 472, 14862, 13, 50875], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 623, "seek": 279666, "start": 2807.66, "end": 2810.66, "text": " But here this one is now an average of these two.", "tokens": [50915, 583, 510, 341, 472, 307, 586, 364, 4274, 295, 613, 732, 13, 51065], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 624, "seek": 279666, "start": 2811.8599999999997, "end": 2814.8599999999997, "text": " And now this one is an average of these three.", "tokens": [51125, 400, 586, 341, 472, 307, 364, 4274, 295, 613, 1045, 13, 51275], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 625, "seek": 279666, "start": 2815.96, "end": 2816.56, "text": " And so on.", "tokens": [51330, 400, 370, 322, 13, 51360], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 626, "seek": 279666, "start": 2817.8599999999997, "end": 2822.46, "text": " So and this last one is the average of all of these elements.", "tokens": [51425, 407, 293, 341, 1036, 472, 307, 264, 4274, 295, 439, 295, 613, 4959, 13, 51655], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 627, "seek": 279666, "start": 2822.46, "end": 2826.3599999999997, "text": " So vertical average just averaging up all the tokens now gives this outcome.", "tokens": [51655, 407, 9429, 4274, 445, 47308, 493, 439, 264, 22667, 586, 2709, 341, 9700, 13, 51850], "temperature": 0.0, "avg_logprob": -0.13292620011738368, "compression_ratio": 1.9365853658536585, "no_speech_prob": 0.00030367879662662745}, {"id": 628, "seek": 282666, "start": 2826.66, "end": 2827.16, "text": " Here.", "tokens": [50365, 1692, 13, 50390], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 629, "seek": 282666, "start": 2828.3599999999997, "end": 2829.7599999999998, "text": " So this is all well and good,", "tokens": [50450, 407, 341, 307, 439, 731, 293, 665, 11, 50520], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 630, "seek": 282666, "start": 2830.16, "end": 2831.56, "text": " but this is very inefficient.", "tokens": [50540, 457, 341, 307, 588, 43495, 13, 50610], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 631, "seek": 282666, "start": 2831.96, "end": 2832.16, "text": " Now.", "tokens": [50630, 823, 13, 50640], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 632, "seek": 282666, "start": 2832.16, "end": 2836.66, "text": " The trick is that we can be very very efficient about doing this using matrix multiplication.", "tokens": [50640, 440, 4282, 307, 300, 321, 393, 312, 588, 588, 7148, 466, 884, 341, 1228, 8141, 27290, 13, 50865], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 633, "seek": 282666, "start": 2837.3599999999997, "end": 2839.06, "text": " So that's the mathematical trick.", "tokens": [50900, 407, 300, 311, 264, 18894, 4282, 13, 50985], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 634, "seek": 282666, "start": 2839.06, "end": 2840.16, "text": " And let me show you what I mean.", "tokens": [50985, 400, 718, 385, 855, 291, 437, 286, 914, 13, 51040], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 635, "seek": 282666, "start": 2840.56, "end": 2842.16, "text": " Let's work with the toy example here.", "tokens": [51060, 961, 311, 589, 365, 264, 12058, 1365, 510, 13, 51140], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 636, "seek": 282666, "start": 2843.06, "end": 2844.2599999999998, "text": " You run it and I'll explain.", "tokens": [51185, 509, 1190, 309, 293, 286, 603, 2903, 13, 51245], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 637, "seek": 282666, "start": 2845.46, "end": 2847.56, "text": " I have a simple matrix here.", "tokens": [51305, 286, 362, 257, 2199, 8141, 510, 13, 51410], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 638, "seek": 282666, "start": 2847.56, "end": 2854.16, "text": " That is three by three of all ones a matrix B of just random numbers and it's a three by two and a matrix C,", "tokens": [51410, 663, 307, 1045, 538, 1045, 295, 439, 2306, 257, 8141, 363, 295, 445, 4974, 3547, 293, 309, 311, 257, 1045, 538, 732, 293, 257, 8141, 383, 11, 51740], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 639, "seek": 282666, "start": 2854.16, "end": 2856.56, "text": " which will be three by three multiply three by two.", "tokens": [51740, 597, 486, 312, 1045, 538, 1045, 12972, 1045, 538, 732, 13, 51860], "temperature": 0.0, "avg_logprob": -0.18015427520309668, "compression_ratio": 1.7681159420289856, "no_speech_prob": 0.000733902386855334}, {"id": 640, "seek": 285666, "start": 2856.8599999999997, "end": 2858.56, "text": " Which will give out a three by two.", "tokens": [50375, 3013, 486, 976, 484, 257, 1045, 538, 732, 13, 50460], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 641, "seek": 285666, "start": 2859.46, "end": 2860.56, "text": " So here we're just using", "tokens": [50505, 407, 510, 321, 434, 445, 1228, 50560], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 642, "seek": 285666, "start": 2861.8599999999997, "end": 2862.8599999999997, "text": " matrix multiplication.", "tokens": [50625, 8141, 27290, 13, 50675], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 643, "seek": 285666, "start": 2863.46, "end": 2865.2599999999998, "text": " So a multiply B gives us C.", "tokens": [50705, 407, 257, 12972, 363, 2709, 505, 383, 13, 50795], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 644, "seek": 285666, "start": 2867.06, "end": 2871.8599999999997, "text": " Okay, so how are these numbers in C achieved?", "tokens": [50885, 1033, 11, 370, 577, 366, 613, 3547, 294, 383, 11042, 30, 51125], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 645, "seek": 285666, "start": 2871.8599999999997, "end": 2872.16, "text": " Right?", "tokens": [51125, 1779, 30, 51140], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 646, "seek": 285666, "start": 2872.16, "end": 2879.46, "text": " So this number in the top left is the first row of a dot product with the first column of B.", "tokens": [51140, 407, 341, 1230, 294, 264, 1192, 1411, 307, 264, 700, 5386, 295, 257, 5893, 1674, 365, 264, 700, 7738, 295, 363, 13, 51505], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 647, "seek": 285666, "start": 2880.16, "end": 2886.46, "text": " And since all the row of a right now is all just once then the dot product here with with this column of", "tokens": [51540, 400, 1670, 439, 264, 5386, 295, 257, 558, 586, 307, 439, 445, 1564, 550, 264, 5893, 1674, 510, 365, 365, 341, 7738, 295, 51855], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 648, "seek": 285666, "start": 2886.46, "end": 2886.56, "text": " B.", "tokens": [51855, 363, 13, 51860], "temperature": 0.0, "avg_logprob": -0.17682972862606958, "compression_ratio": 1.6441441441441442, "no_speech_prob": 6.41379738226533e-05}, {"id": 649, "seek": 288666, "start": 2886.8599999999997, "end": 2889.8599999999997, "text": " Is just going to do a sum of these of this column.", "tokens": [50375, 1119, 445, 516, 281, 360, 257, 2408, 295, 613, 295, 341, 7738, 13, 50525], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 650, "seek": 288666, "start": 2890.06, "end": 2892.46, "text": " So two plus six plus six is 14.", "tokens": [50535, 407, 732, 1804, 2309, 1804, 2309, 307, 3499, 13, 50655], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 651, "seek": 288666, "start": 2893.46, "end": 2897.06, "text": " The element here in the output of C is also the first column here.", "tokens": [50705, 440, 4478, 510, 294, 264, 5598, 295, 383, 307, 611, 264, 700, 7738, 510, 13, 50885], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 652, "seek": 288666, "start": 2897.06, "end": 2901.06, "text": " The first row of a multiplied now with the second column of B.", "tokens": [50885, 440, 700, 5386, 295, 257, 17207, 586, 365, 264, 1150, 7738, 295, 363, 13, 51085], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 653, "seek": 288666, "start": 2901.46, "end": 2903.8599999999997, "text": " So seven plus four plus plus five is 16.", "tokens": [51105, 407, 3407, 1804, 1451, 1804, 1804, 1732, 307, 3165, 13, 51225], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 654, "seek": 288666, "start": 2904.7599999999998, "end": 2906.2599999999998, "text": " Now you see that there's repeating elements here.", "tokens": [51270, 823, 291, 536, 300, 456, 311, 18617, 4959, 510, 13, 51345], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 655, "seek": 288666, "start": 2906.2599999999998, "end": 2911.46, "text": " So this 14 again is because this row is again all once and it's multiplying the first column of B.", "tokens": [51345, 407, 341, 3499, 797, 307, 570, 341, 5386, 307, 797, 439, 1564, 293, 309, 311, 30955, 264, 700, 7738, 295, 363, 13, 51605], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 656, "seek": 288666, "start": 2911.46, "end": 2914.7599999999998, "text": " So we get 14 and this one is and so on.", "tokens": [51605, 407, 321, 483, 3499, 293, 341, 472, 307, 293, 370, 322, 13, 51770], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 657, "seek": 288666, "start": 2914.7599999999998, "end": 2916.56, "text": " So this last number here is the.", "tokens": [51770, 407, 341, 1036, 1230, 510, 307, 264, 13, 51860], "temperature": 0.0, "avg_logprob": -0.14875680750066583, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.00017000023217406124}, {"id": 658, "seek": 291666, "start": 2916.66, "end": 2919.3599999999997, "text": " The last row dot product last column.", "tokens": [50365, 440, 1036, 5386, 5893, 1674, 1036, 7738, 13, 50500], "temperature": 0.0, "avg_logprob": -0.20581122367612778, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0002647907822392881}, {"id": 659, "seek": 291666, "start": 2920.46, "end": 2922.66, "text": " Now the trick here is the following.", "tokens": [50555, 823, 264, 4282, 510, 307, 264, 3480, 13, 50665], "temperature": 0.0, "avg_logprob": -0.20581122367612778, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0002647907822392881}, {"id": 660, "seek": 291666, "start": 2923.3599999999997, "end": 2933.8599999999997, "text": " This is just a boring number of is just a boring array of all once but torch has this function called trill which is short for a triangular.", "tokens": [50700, 639, 307, 445, 257, 9989, 1230, 295, 307, 445, 257, 9989, 10225, 295, 439, 1564, 457, 27822, 575, 341, 2445, 1219, 504, 373, 597, 307, 2099, 337, 257, 38190, 13, 51225], "temperature": 0.0, "avg_logprob": -0.20581122367612778, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0002647907822392881}, {"id": 661, "seek": 291666, "start": 2935.3599999999997, "end": 2941.8599999999997, "text": " Something like that and you can wrap it in torch that once and it will just return the lower triangular portion of this.", "tokens": [51300, 6595, 411, 300, 293, 291, 393, 7019, 309, 294, 27822, 300, 1564, 293, 309, 486, 445, 2736, 264, 3126, 38190, 8044, 295, 341, 13, 51625], "temperature": 0.0, "avg_logprob": -0.20581122367612778, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0002647907822392881}, {"id": 662, "seek": 291666, "start": 2942.66, "end": 2943.06, "text": " Okay.", "tokens": [51665, 1033, 13, 51685], "temperature": 0.0, "avg_logprob": -0.20581122367612778, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0002647907822392881}, {"id": 663, "seek": 291666, "start": 2944.7599999999998, "end": 2946.46, "text": " So now it will basically zero out.", "tokens": [51770, 407, 586, 309, 486, 1936, 4018, 484, 13, 51855], "temperature": 0.0, "avg_logprob": -0.20581122367612778, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.0002647907822392881}, {"id": 664, "seek": 294666, "start": 2946.66, "end": 2947.46, "text": " Of these guys here.", "tokens": [50365, 2720, 613, 1074, 510, 13, 50405], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 665, "seek": 294666, "start": 2947.56, "end": 2949.46, "text": " So we just get the lower triangular part.", "tokens": [50410, 407, 321, 445, 483, 264, 3126, 38190, 644, 13, 50505], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 666, "seek": 294666, "start": 2949.7599999999998, "end": 2951.96, "text": " Well, what happens if we do that?", "tokens": [50520, 1042, 11, 437, 2314, 498, 321, 360, 300, 30, 50630], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 667, "seek": 294666, "start": 2955.16, "end": 2958.16, "text": " So now we'll have a like this and be like this.", "tokens": [50790, 407, 586, 321, 603, 362, 257, 411, 341, 293, 312, 411, 341, 13, 50940], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 668, "seek": 294666, "start": 2958.16, "end": 2959.66, "text": " And now what are we getting here and see?", "tokens": [50940, 400, 586, 437, 366, 321, 1242, 510, 293, 536, 30, 51015], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 669, "seek": 294666, "start": 2960.3599999999997, "end": 2961.56, "text": " Well, what is this number?", "tokens": [51050, 1042, 11, 437, 307, 341, 1230, 30, 51110], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 670, "seek": 294666, "start": 2961.8599999999997, "end": 2966.8599999999997, "text": " Well, this is the first row times the first column and because this is zeros.", "tokens": [51125, 1042, 11, 341, 307, 264, 700, 5386, 1413, 264, 700, 7738, 293, 570, 341, 307, 35193, 13, 51375], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 671, "seek": 294666, "start": 2968.96, "end": 2970.66, "text": " These elements here are now ignored.", "tokens": [51480, 1981, 4959, 510, 366, 586, 19735, 13, 51565], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 672, "seek": 294666, "start": 2970.66, "end": 2976.16, "text": " So we just get a two and then this number here is the first row times the second column.", "tokens": [51565, 407, 321, 445, 483, 257, 732, 293, 550, 341, 1230, 510, 307, 264, 700, 5386, 1413, 264, 1150, 7738, 13, 51840], "temperature": 0.0, "avg_logprob": -0.14329541962722253, "compression_ratio": 1.8738738738738738, "no_speech_prob": 0.00014712108531966805}, {"id": 673, "seek": 297666, "start": 2976.66, "end": 2979.66, "text": " And because these are zeros they get ignored and it's just seven.", "tokens": [50365, 400, 570, 613, 366, 35193, 436, 483, 19735, 293, 309, 311, 445, 3407, 13, 50515], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 674, "seek": 297666, "start": 2980.16, "end": 2981.46, "text": " The seven multiplies this one.", "tokens": [50540, 440, 3407, 12788, 530, 341, 472, 13, 50605], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 675, "seek": 297666, "start": 2982.46, "end": 2985.2599999999998, "text": " But look what happened here because this is one and then zeros.", "tokens": [50655, 583, 574, 437, 2011, 510, 570, 341, 307, 472, 293, 550, 35193, 13, 50795], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 676, "seek": 297666, "start": 2985.56, "end": 2990.8599999999997, "text": " We what ended up happening is we're just plucking out the row of this row of B and that's what we got.", "tokens": [50810, 492, 437, 4590, 493, 2737, 307, 321, 434, 445, 499, 33260, 484, 264, 5386, 295, 341, 5386, 295, 363, 293, 300, 311, 437, 321, 658, 13, 51075], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 677, "seek": 297666, "start": 2992.16, "end": 2994.7599999999998, "text": " Now here we have one one zero.", "tokens": [51140, 823, 510, 321, 362, 472, 472, 4018, 13, 51270], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 678, "seek": 297666, "start": 2995.3599999999997, "end": 3002.66, "text": " So here one one zero dot product with these two columns will now give us two plus six which is eight and seven plus four which is 11.", "tokens": [51300, 407, 510, 472, 472, 4018, 5893, 1674, 365, 613, 732, 13766, 486, 586, 976, 505, 732, 1804, 2309, 597, 307, 3180, 293, 3407, 1804, 1451, 597, 307, 2975, 13, 51665], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 679, "seek": 297666, "start": 3003.3599999999997, "end": 3006.16, "text": " And because this is one one one we ended up with.", "tokens": [51700, 400, 570, 341, 307, 472, 472, 472, 321, 4590, 493, 365, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1582358883273217, "compression_ratio": 1.817490494296578, "no_speech_prob": 0.00017678381118457764}, {"id": 680, "seek": 300666, "start": 3006.66, "end": 3007.8599999999997, "text": " The addition of all of them.", "tokens": [50365, 440, 4500, 295, 439, 295, 552, 13, 50425], "temperature": 0.0, "avg_logprob": -0.1570149194626581, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.00013632039190270007}, {"id": 681, "seek": 300666, "start": 3008.8599999999997, "end": 3011.8599999999997, "text": " And so basically depending on how many ones and zeros we have here.", "tokens": [50475, 400, 370, 1936, 5413, 322, 577, 867, 2306, 293, 35193, 321, 362, 510, 13, 50625], "temperature": 0.0, "avg_logprob": -0.1570149194626581, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.00013632039190270007}, {"id": 682, "seek": 300666, "start": 3012.2599999999998, "end": 3020.2599999999998, "text": " We are basically doing a sum currently of the variable number of these rows and that gets deposited into C.", "tokens": [50645, 492, 366, 1936, 884, 257, 2408, 4362, 295, 264, 7006, 1230, 295, 613, 13241, 293, 300, 2170, 42002, 666, 383, 13, 51045], "temperature": 0.0, "avg_logprob": -0.1570149194626581, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.00013632039190270007}, {"id": 683, "seek": 300666, "start": 3021.7599999999998, "end": 3032.7599999999998, "text": " So currently we're doing sums because these are ones but we can also do average right and you can start to see how we could do average of the rows of B sort of an incremental fashion.", "tokens": [51120, 407, 4362, 321, 434, 884, 34499, 570, 613, 366, 2306, 457, 321, 393, 611, 360, 4274, 558, 293, 291, 393, 722, 281, 536, 577, 321, 727, 360, 4274, 295, 264, 13241, 295, 363, 1333, 295, 364, 35759, 6700, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1570149194626581, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.00013632039190270007}, {"id": 684, "seek": 300666, "start": 3033.56, "end": 3036.3599999999997, "text": " Because we don't have to we can basically normalize.", "tokens": [51710, 1436, 321, 500, 380, 362, 281, 321, 393, 1936, 2710, 1125, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1570149194626581, "compression_ratio": 1.743083003952569, "no_speech_prob": 0.00013632039190270007}, {"id": 685, "seek": 303636, "start": 3036.36, "end": 3039.6600000000003, "text": " These rows so that they sum to one and then we're going to get an average.", "tokens": [50365, 1981, 13241, 370, 300, 436, 2408, 281, 472, 293, 550, 321, 434, 516, 281, 483, 364, 4274, 13, 50530], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 686, "seek": 303636, "start": 3040.36, "end": 3047.06, "text": " So if we took a and then we did a equals a divide a torch dot sum in the.", "tokens": [50565, 407, 498, 321, 1890, 257, 293, 550, 321, 630, 257, 6915, 257, 9845, 257, 27822, 5893, 2408, 294, 264, 13, 50900], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 687, "seek": 303636, "start": 3048.6600000000003, "end": 3050.46, "text": " Of a in the.", "tokens": [50980, 2720, 257, 294, 264, 13, 51070], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 688, "seek": 303636, "start": 3051.36, "end": 3051.76, "text": " One.", "tokens": [51115, 1485, 13, 51135], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 689, "seek": 303636, "start": 3052.86, "end": 3055.86, "text": " Dimension and then let's keep them is true.", "tokens": [51190, 20975, 3378, 293, 550, 718, 311, 1066, 552, 307, 2074, 13, 51340], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 690, "seek": 303636, "start": 3056.46, "end": 3058.26, "text": " So therefore the broadcasting will work out.", "tokens": [51370, 407, 4412, 264, 30024, 486, 589, 484, 13, 51460], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 691, "seek": 303636, "start": 3058.86, "end": 3063.46, "text": " So if I rerun this you see now that these rows now sum to one.", "tokens": [51490, 407, 498, 286, 43819, 409, 341, 291, 536, 586, 300, 613, 13241, 586, 2408, 281, 472, 13, 51720], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 692, "seek": 303636, "start": 3063.76, "end": 3066.1600000000003, "text": " So this row is one this row is point five point five zero.", "tokens": [51735, 407, 341, 5386, 307, 472, 341, 5386, 307, 935, 1732, 935, 1732, 4018, 13, 51855], "temperature": 0.0, "avg_logprob": -0.20673482758658274, "compression_ratio": 1.7699530516431925, "no_speech_prob": 6.29015194135718e-05}, {"id": 693, "seek": 306636, "start": 3066.86, "end": 3068.36, "text": " And here we get one thirds.", "tokens": [50390, 400, 510, 321, 483, 472, 34552, 13, 50465], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 694, "seek": 306636, "start": 3068.96, "end": 3071.76, "text": " And now when we do a multiply be what are we getting.", "tokens": [50495, 400, 586, 562, 321, 360, 257, 12972, 312, 437, 366, 321, 1242, 13, 50635], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 695, "seek": 306636, "start": 3072.56, "end": 3074.86, "text": " Here we are just getting the first row first row.", "tokens": [50675, 1692, 321, 366, 445, 1242, 264, 700, 5386, 700, 5386, 13, 50790], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 696, "seek": 306636, "start": 3075.76, "end": 3079.1600000000003, "text": " Here now we are getting the average of the first two rows.", "tokens": [50835, 1692, 586, 321, 366, 1242, 264, 4274, 295, 264, 700, 732, 13241, 13, 51005], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 697, "seek": 306636, "start": 3081.06, "end": 3085.1600000000003, "text": " Okay so two and six average is four and four and seven averages five point five.", "tokens": [51100, 1033, 370, 732, 293, 2309, 4274, 307, 1451, 293, 1451, 293, 3407, 42257, 1732, 935, 1732, 13, 51305], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 698, "seek": 306636, "start": 3086.06, "end": 3090.76, "text": " And on the bottom here we are now getting the average of these three rows.", "tokens": [51350, 400, 322, 264, 2767, 510, 321, 366, 586, 1242, 264, 4274, 295, 613, 1045, 13241, 13, 51585], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 699, "seek": 306636, "start": 3091.56, "end": 3095.56, "text": " So the average of all of elements of B are now deposited here.", "tokens": [51625, 407, 264, 4274, 295, 439, 295, 4959, 295, 363, 366, 586, 42002, 510, 13, 51825], "temperature": 0.0, "avg_logprob": -0.17434703179125516, "compression_ratio": 2.0147783251231526, "no_speech_prob": 0.0003214519820176065}, {"id": 700, "seek": 309636, "start": 3096.46, "end": 3105.06, "text": " And so you can see that by manipulating these elements of this multiplying matrix and then multiplying it with any given matrix.", "tokens": [50370, 400, 370, 291, 393, 536, 300, 538, 40805, 613, 4959, 295, 341, 30955, 8141, 293, 550, 30955, 309, 365, 604, 2212, 8141, 13, 50800], "temperature": 0.0, "avg_logprob": -0.15917635409631462, "compression_ratio": 1.7293233082706767, "no_speech_prob": 9.831936040427536e-05}, {"id": 701, "seek": 309636, "start": 3105.36, "end": 3109.86, "text": " We can do these averages in this incremental fashion because we just get.", "tokens": [50815, 492, 393, 360, 613, 42257, 294, 341, 35759, 6700, 570, 321, 445, 483, 13, 51040], "temperature": 0.0, "avg_logprob": -0.15917635409631462, "compression_ratio": 1.7293233082706767, "no_speech_prob": 9.831936040427536e-05}, {"id": 702, "seek": 309636, "start": 3111.46, "end": 3114.06, "text": " And we can manipulate that based on the elements of a.", "tokens": [51120, 400, 321, 393, 20459, 300, 2361, 322, 264, 4959, 295, 257, 13, 51250], "temperature": 0.0, "avg_logprob": -0.15917635409631462, "compression_ratio": 1.7293233082706767, "no_speech_prob": 9.831936040427536e-05}, {"id": 703, "seek": 309636, "start": 3114.6600000000003, "end": 3121.46, "text": " Okay so that's very convenient so let's swing back up here and see how we can vectorize this and make it much more efficient using what we've learned.", "tokens": [51280, 1033, 370, 300, 311, 588, 10851, 370, 718, 311, 11173, 646, 493, 510, 293, 536, 577, 321, 393, 8062, 1125, 341, 293, 652, 309, 709, 544, 7148, 1228, 437, 321, 600, 3264, 13, 51620], "temperature": 0.0, "avg_logprob": -0.15917635409631462, "compression_ratio": 1.7293233082706767, "no_speech_prob": 9.831936040427536e-05}, {"id": 704, "seek": 309636, "start": 3122.36, "end": 3123.06, "text": " So in particular.", "tokens": [51665, 407, 294, 1729, 13, 51700], "temperature": 0.0, "avg_logprob": -0.15917635409631462, "compression_ratio": 1.7293233082706767, "no_speech_prob": 9.831936040427536e-05}, {"id": 705, "seek": 309636, "start": 3124.6600000000003, "end": 3126.1600000000003, "text": " We are going to produce an array.", "tokens": [51780, 492, 366, 516, 281, 5258, 364, 10225, 13, 51855], "temperature": 0.0, "avg_logprob": -0.15917635409631462, "compression_ratio": 1.7293233082706767, "no_speech_prob": 9.831936040427536e-05}, {"id": 706, "seek": 312636, "start": 3126.86, "end": 3129.36, "text": " But here I'm going to call it way short for weights.", "tokens": [50390, 583, 510, 286, 478, 516, 281, 818, 309, 636, 2099, 337, 17443, 13, 50515], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 707, "seek": 312636, "start": 3130.1600000000003, "end": 3131.06, "text": " But this is our a.", "tokens": [50555, 583, 341, 307, 527, 257, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 708, "seek": 312636, "start": 3132.6600000000003, "end": 3139.46, "text": " And this is how much of every row we want to average up and it's going to be an average because you can see that these rows sum to one.", "tokens": [50680, 400, 341, 307, 577, 709, 295, 633, 5386, 321, 528, 281, 4274, 493, 293, 309, 311, 516, 281, 312, 364, 4274, 570, 291, 393, 536, 300, 613, 13241, 2408, 281, 472, 13, 51020], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 709, "seek": 312636, "start": 3141.06, "end": 3145.36, "text": " So this is our a and then our B in this example of course is.", "tokens": [51100, 407, 341, 307, 527, 257, 293, 550, 527, 363, 294, 341, 1365, 295, 1164, 307, 13, 51315], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 710, "seek": 312636, "start": 3146.1600000000003, "end": 3146.56, "text": " X.", "tokens": [51355, 1783, 13, 51375], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 711, "seek": 312636, "start": 3147.86, "end": 3150.96, "text": " So it's going to happen here now is that we are going to have an expo to.", "tokens": [51440, 407, 309, 311, 516, 281, 1051, 510, 586, 307, 300, 321, 366, 516, 281, 362, 364, 1278, 78, 281, 13, 51595], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 712, "seek": 312636, "start": 3152.76, "end": 3155.6600000000003, "text": " And this expo to is going to be way.", "tokens": [51685, 400, 341, 1278, 78, 281, 307, 516, 281, 312, 636, 13, 51830], "temperature": 0.0, "avg_logprob": -0.1734870081362517, "compression_ratio": 1.8151658767772512, "no_speech_prob": 0.00022441495093517005}, {"id": 713, "seek": 315636, "start": 3156.46, "end": 3157.1600000000003, "text": " Multiplying.", "tokens": [50370, 31150, 7310, 13, 50405], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 714, "seek": 315636, "start": 3158.06, "end": 3158.56, "text": " Rx.", "tokens": [50450, 497, 87, 13, 50475], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 715, "seek": 315636, "start": 3159.86, "end": 3167.1600000000003, "text": " So let's think this through way is T by T and this is matrix multiplying in PyTorch a B by T by C.", "tokens": [50540, 407, 718, 311, 519, 341, 807, 636, 307, 314, 538, 314, 293, 341, 307, 8141, 30955, 294, 9953, 51, 284, 339, 257, 363, 538, 314, 538, 383, 13, 50905], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 716, "seek": 315636, "start": 3168.6600000000003, "end": 3169.46, "text": " And it's giving us.", "tokens": [50980, 400, 309, 311, 2902, 505, 13, 51020], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 717, "seek": 315636, "start": 3171.06, "end": 3171.56, "text": " What shape.", "tokens": [51100, 708, 3909, 13, 51125], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 718, "seek": 315636, "start": 3172.1600000000003, "end": 3180.26, "text": " So PyTorch will come here and it will see that these shapes are not the same so it will create a bash dimension here and this is a batch matrix multiply.", "tokens": [51155, 407, 9953, 51, 284, 339, 486, 808, 510, 293, 309, 486, 536, 300, 613, 10854, 366, 406, 264, 912, 370, 309, 486, 1884, 257, 46183, 10139, 510, 293, 341, 307, 257, 15245, 8141, 12972, 13, 51560], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 719, "seek": 315636, "start": 3181.46, "end": 3184.86, "text": " And so it will apply this matrix multiplication in all the batch elements.", "tokens": [51620, 400, 370, 309, 486, 3079, 341, 8141, 27290, 294, 439, 264, 15245, 4959, 13, 51790], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 720, "seek": 315636, "start": 3185.46, "end": 3185.96, "text": " In parallel.", "tokens": [51820, 682, 8952, 13, 51845], "temperature": 0.0, "avg_logprob": -0.25785582406180246, "compression_ratio": 1.7762557077625571, "no_speech_prob": 0.00020177203987259418}, {"id": 721, "seek": 318636, "start": 3186.46, "end": 3194.1600000000003, "text": " And individually and then for each batch element there will be a T by T multiplying T by C exactly as we had below.", "tokens": [50370, 400, 16652, 293, 550, 337, 1184, 15245, 4478, 456, 486, 312, 257, 314, 538, 314, 30955, 314, 538, 383, 2293, 382, 321, 632, 2507, 13, 50755], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 722, "seek": 318636, "start": 3196.6600000000003, "end": 3197.86, "text": " So this will now create.", "tokens": [50880, 407, 341, 486, 586, 1884, 13, 50940], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 723, "seek": 318636, "start": 3198.76, "end": 3200.06, "text": " B by T by C.", "tokens": [50985, 363, 538, 314, 538, 383, 13, 51050], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 724, "seek": 318636, "start": 3201.36, "end": 3205.1600000000003, "text": " And expo to will now become identical to expo.", "tokens": [51115, 400, 1278, 78, 281, 486, 586, 1813, 14800, 281, 1278, 78, 13, 51305], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 725, "seek": 318636, "start": 3206.36, "end": 3206.96, "text": " So.", "tokens": [51365, 407, 13, 51395], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 726, "seek": 318636, "start": 3208.96, "end": 3210.96, "text": " We can see that torched out all close.", "tokens": [51495, 492, 393, 536, 300, 3930, 19318, 484, 439, 1998, 13, 51595], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 727, "seek": 318636, "start": 3211.86, "end": 3215.56, "text": " Of expo and expo to should be true now.", "tokens": [51640, 2720, 1278, 78, 293, 1278, 78, 281, 820, 312, 2074, 586, 13, 51825], "temperature": 0.0, "avg_logprob": -0.18537679585543546, "compression_ratio": 1.5297297297297296, "no_speech_prob": 6.305806164164096e-05}, {"id": 728, "seek": 321636, "start": 3217.36, "end": 3221.96, "text": " So this kind of like misses us that these are in fact the same.", "tokens": [50415, 407, 341, 733, 295, 411, 29394, 505, 300, 613, 366, 294, 1186, 264, 912, 13, 50645], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 729, "seek": 321636, "start": 3222.76, "end": 3225.96, "text": " So expo and expo to if I just print them.", "tokens": [50685, 407, 1278, 78, 293, 1278, 78, 281, 498, 286, 445, 4482, 552, 13, 50845], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 730, "seek": 321636, "start": 3228.26, "end": 3229.96, "text": " Okay, we're not going to be able to.", "tokens": [50960, 1033, 11, 321, 434, 406, 516, 281, 312, 1075, 281, 13, 51045], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 731, "seek": 321636, "start": 3230.46, "end": 3232.96, "text": " Okay, we're not going to be able to just stare it down but.", "tokens": [51070, 1033, 11, 321, 434, 406, 516, 281, 312, 1075, 281, 445, 22432, 309, 760, 457, 13, 51195], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 732, "seek": 321636, "start": 3235.1600000000003, "end": 3238.96, "text": " Well, let me try expo basically just at the 0th element and expo to at the 0th element.", "tokens": [51305, 1042, 11, 718, 385, 853, 1278, 78, 1936, 445, 412, 264, 1958, 392, 4478, 293, 1278, 78, 281, 412, 264, 1958, 392, 4478, 13, 51495], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 733, "seek": 321636, "start": 3238.96, "end": 3244.1600000000003, "text": " So just the first batch and we should see that this and that should be identical which they are.", "tokens": [51495, 407, 445, 264, 700, 15245, 293, 321, 820, 536, 300, 341, 293, 300, 820, 312, 14800, 597, 436, 366, 13, 51755], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 734, "seek": 321636, "start": 3245.36, "end": 3245.76, "text": " Right.", "tokens": [51815, 1779, 13, 51835], "temperature": 0.0, "avg_logprob": -0.18691382448897403, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.000318416889058426}, {"id": 735, "seek": 324576, "start": 3245.86, "end": 3246.86, "text": " So what happened here.", "tokens": [50370, 407, 437, 2011, 510, 13, 50420], "temperature": 0.0, "avg_logprob": -0.16724037087481955, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.00013846077490597963}, {"id": 736, "seek": 324576, "start": 3247.26, "end": 3260.5600000000004, "text": " The trick is we were able to use batch matrix multiply to do this aggregation really and it's a weighted aggregation and the weights are specified in this T by T array.", "tokens": [50440, 440, 4282, 307, 321, 645, 1075, 281, 764, 15245, 8141, 12972, 281, 360, 341, 16743, 399, 534, 293, 309, 311, 257, 32807, 16743, 399, 293, 264, 17443, 366, 22206, 294, 341, 314, 538, 314, 10225, 13, 51105], "temperature": 0.0, "avg_logprob": -0.16724037087481955, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.00013846077490597963}, {"id": 737, "seek": 324576, "start": 3261.46, "end": 3271.36, "text": " And we're basically doing weighted sums and these weighted sums are according to the weights inside here that take on sort of this triangular form.", "tokens": [51150, 400, 321, 434, 1936, 884, 32807, 34499, 293, 613, 32807, 34499, 366, 4650, 281, 264, 17443, 1854, 510, 300, 747, 322, 1333, 295, 341, 38190, 1254, 13, 51645], "temperature": 0.0, "avg_logprob": -0.16724037087481955, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.00013846077490597963}, {"id": 738, "seek": 324576, "start": 3272.1600000000003, "end": 3275.6600000000003, "text": " And so that means that a token at the teeth dimension will only get.", "tokens": [51685, 400, 370, 300, 1355, 300, 257, 14862, 412, 264, 7798, 10139, 486, 787, 483, 13, 51860], "temperature": 0.0, "avg_logprob": -0.16724037087481955, "compression_ratio": 1.7510729613733906, "no_speech_prob": 0.00013846077490597963}, {"id": 739, "seek": 327576, "start": 3275.86, "end": 3280.36, "text": " Sort of information from the tokens preceding it.", "tokens": [50370, 26149, 295, 1589, 490, 264, 22667, 16969, 278, 309, 13, 50595], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 740, "seek": 327576, "start": 3280.6600000000003, "end": 3281.86, "text": " So that's exactly what we want.", "tokens": [50610, 407, 300, 311, 2293, 437, 321, 528, 13, 50670], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 741, "seek": 327576, "start": 3282.26, "end": 3284.76, "text": " And finally, I would like to rewrite it in one more way.", "tokens": [50690, 400, 2721, 11, 286, 576, 411, 281, 28132, 309, 294, 472, 544, 636, 13, 50815], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 742, "seek": 327576, "start": 3285.46, "end": 3287.1600000000003, "text": " And we're going to see why that's useful.", "tokens": [50850, 400, 321, 434, 516, 281, 536, 983, 300, 311, 4420, 13, 50935], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 743, "seek": 327576, "start": 3288.0600000000004, "end": 3293.76, "text": " So this is the third version and it's also identical to the first and second, but let me talk through it.", "tokens": [50980, 407, 341, 307, 264, 2636, 3037, 293, 309, 311, 611, 14800, 281, 264, 700, 293, 1150, 11, 457, 718, 385, 751, 807, 309, 13, 51265], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 744, "seek": 327576, "start": 3293.76, "end": 3294.86, "text": " It uses softmax.", "tokens": [51265, 467, 4960, 2787, 41167, 13, 51320], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 745, "seek": 327576, "start": 3295.6600000000003, "end": 3304.86, "text": " So trill here is this Matrix lower triangular once way begins as all zero.", "tokens": [51360, 407, 504, 373, 510, 307, 341, 36274, 3126, 38190, 1564, 636, 7338, 382, 439, 4018, 13, 51820], "temperature": 0.0, "avg_logprob": -0.14101521019796723, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.00011198418360436335}, {"id": 746, "seek": 330576, "start": 3305.86, "end": 3313.5600000000004, "text": " Okay, so if I just print way in the beginning, it's all zero then I used masked fill.", "tokens": [50370, 1033, 11, 370, 498, 286, 445, 4482, 636, 294, 264, 2863, 11, 309, 311, 439, 4018, 550, 286, 1143, 45249, 2836, 13, 50755], "temperature": 0.0, "avg_logprob": -0.23239378327304877, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.00013280290295369923}, {"id": 747, "seek": 330576, "start": 3314.26, "end": 3318.26, "text": " So what this is doing is wait that masked fill it's all zeros.", "tokens": [50790, 407, 437, 341, 307, 884, 307, 1699, 300, 45249, 2836, 309, 311, 439, 35193, 13, 50990], "temperature": 0.0, "avg_logprob": -0.23239378327304877, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.00013280290295369923}, {"id": 748, "seek": 330576, "start": 3318.26, "end": 3324.76, "text": " And I'm saying for all the elements where trill is equals equals zero make them be negative Infinity.", "tokens": [50990, 400, 286, 478, 1566, 337, 439, 264, 4959, 689, 504, 373, 307, 6915, 6915, 4018, 652, 552, 312, 3671, 34762, 13, 51315], "temperature": 0.0, "avg_logprob": -0.23239378327304877, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.00013280290295369923}, {"id": 749, "seek": 330576, "start": 3325.46, "end": 3329.1600000000003, "text": " So all the elements where trill is zero will become negative Infinity now.", "tokens": [51350, 407, 439, 264, 4959, 689, 504, 373, 307, 4018, 486, 1813, 3671, 34762, 586, 13, 51535], "temperature": 0.0, "avg_logprob": -0.23239378327304877, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.00013280290295369923}, {"id": 750, "seek": 330576, "start": 3330.26, "end": 3331.1600000000003, "text": " So this is what we get.", "tokens": [51590, 407, 341, 307, 437, 321, 483, 13, 51635], "temperature": 0.0, "avg_logprob": -0.23239378327304877, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.00013280290295369923}, {"id": 751, "seek": 330576, "start": 3332.26, "end": 3334.96, "text": " And then the final line here is softmax.", "tokens": [51690, 400, 550, 264, 2572, 1622, 510, 307, 2787, 41167, 13, 51825], "temperature": 0.0, "avg_logprob": -0.23239378327304877, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.00013280290295369923}, {"id": 752, "seek": 333576, "start": 3336.76, "end": 3344.86, "text": " So if I take a softmax along every single so dim is negative one so long every single row if I do a softmax, what is that going to do?", "tokens": [50415, 407, 498, 286, 747, 257, 2787, 41167, 2051, 633, 2167, 370, 5013, 307, 3671, 472, 370, 938, 633, 2167, 5386, 498, 286, 360, 257, 2787, 41167, 11, 437, 307, 300, 516, 281, 360, 30, 50820], "temperature": 0.0, "avg_logprob": -0.2270567218462626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.000188284000614658}, {"id": 753, "seek": 333576, "start": 3347.0600000000004, "end": 3353.1600000000003, "text": " Well softmax is is also like a normalization operation, right?", "tokens": [50930, 1042, 2787, 41167, 307, 307, 611, 411, 257, 2710, 2144, 6916, 11, 558, 30, 51235], "temperature": 0.0, "avg_logprob": -0.2270567218462626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.000188284000614658}, {"id": 754, "seek": 333576, "start": 3354.1600000000003, "end": 3356.86, "text": " And so spoiler alert you get the exact same Matrix.", "tokens": [51285, 400, 370, 26927, 9615, 291, 483, 264, 1900, 912, 36274, 13, 51420], "temperature": 0.0, "avg_logprob": -0.2270567218462626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.000188284000614658}, {"id": 755, "seek": 333576, "start": 3358.5600000000004, "end": 3362.1600000000003, "text": " Let me bring back the softmax and recall that in softmax.", "tokens": [51505, 961, 385, 1565, 646, 264, 2787, 41167, 293, 9901, 300, 294, 2787, 41167, 13, 51685], "temperature": 0.0, "avg_logprob": -0.2270567218462626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.000188284000614658}, {"id": 756, "seek": 333576, "start": 3362.1600000000003, "end": 3364.5600000000004, "text": " We're going to exponentiate every single one of these.", "tokens": [51685, 492, 434, 516, 281, 37871, 13024, 633, 2167, 472, 295, 613, 13, 51805], "temperature": 0.0, "avg_logprob": -0.2270567218462626, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.000188284000614658}, {"id": 757, "seek": 336576, "start": 3365.76, "end": 3367.46, "text": " And then we're going to divide by the sum.", "tokens": [50365, 400, 550, 321, 434, 516, 281, 9845, 538, 264, 2408, 13, 50450], "temperature": 0.0, "avg_logprob": -0.14938165271092976, "compression_ratio": 1.9221311475409837, "no_speech_prob": 0.00010463294165674597}, {"id": 758, "seek": 336576, "start": 3368.26, "end": 3376.46, "text": " And so if we exponentiate every single element here, we're going to get a one and here we're going to get basically zero zero zero zero everywhere else.", "tokens": [50490, 400, 370, 498, 321, 37871, 13024, 633, 2167, 4478, 510, 11, 321, 434, 516, 281, 483, 257, 472, 293, 510, 321, 434, 516, 281, 483, 1936, 4018, 4018, 4018, 4018, 5315, 1646, 13, 50900], "temperature": 0.0, "avg_logprob": -0.14938165271092976, "compression_ratio": 1.9221311475409837, "no_speech_prob": 0.00010463294165674597}, {"id": 759, "seek": 336576, "start": 3377.0600000000004, "end": 3380.26, "text": " And then when we normalize we just get one here.", "tokens": [50930, 400, 550, 562, 321, 2710, 1125, 321, 445, 483, 472, 510, 13, 51090], "temperature": 0.0, "avg_logprob": -0.14938165271092976, "compression_ratio": 1.9221311475409837, "no_speech_prob": 0.00010463294165674597}, {"id": 760, "seek": 336576, "start": 3380.26, "end": 3387.46, "text": " We're going to get one one and then zeros and then softmax will again divide and this will give us 0.5 0.5 and so on.", "tokens": [51090, 492, 434, 516, 281, 483, 472, 472, 293, 550, 35193, 293, 550, 2787, 41167, 486, 797, 9845, 293, 341, 486, 976, 505, 1958, 13, 20, 1958, 13, 20, 293, 370, 322, 13, 51450], "temperature": 0.0, "avg_logprob": -0.14938165271092976, "compression_ratio": 1.9221311475409837, "no_speech_prob": 0.00010463294165674597}, {"id": 761, "seek": 336576, "start": 3388.1600000000003, "end": 3392.46, "text": " And so this is also the same way to produce this mask.", "tokens": [51485, 400, 370, 341, 307, 611, 264, 912, 636, 281, 5258, 341, 6094, 13, 51700], "temperature": 0.0, "avg_logprob": -0.14938165271092976, "compression_ratio": 1.9221311475409837, "no_speech_prob": 0.00010463294165674597}, {"id": 762, "seek": 336576, "start": 3393.36, "end": 3395.26, "text": " Now the reason that this is a bit more interesting.", "tokens": [51745, 823, 264, 1778, 300, 341, 307, 257, 857, 544, 1880, 13, 51840], "temperature": 0.0, "avg_logprob": -0.14938165271092976, "compression_ratio": 1.9221311475409837, "no_speech_prob": 0.00010463294165674597}, {"id": 763, "seek": 339526, "start": 3395.36, "end": 3407.6600000000003, "text": " And the reason we're going to end up using it in self-attention is that these weights here begin with zero and you can think of this as like an interaction strength or like an affinity.", "tokens": [50370, 400, 264, 1778, 321, 434, 516, 281, 917, 493, 1228, 309, 294, 2698, 12, 1591, 1251, 307, 300, 613, 17443, 510, 1841, 365, 4018, 293, 291, 393, 519, 295, 341, 382, 411, 364, 9285, 3800, 420, 411, 364, 39703, 13, 50985], "temperature": 0.0, "avg_logprob": -0.1393626630306244, "compression_ratio": 1.671875, "no_speech_prob": 0.00016027363017201424}, {"id": 764, "seek": 339526, "start": 3408.26, "end": 3423.76, "text": " So basically it's telling us how much of each token from the past do we want to aggregate and average up and then this line is saying tokens from the past cannot communicate by setting them to negative Infinity.", "tokens": [51015, 407, 1936, 309, 311, 3585, 505, 577, 709, 295, 1184, 14862, 490, 264, 1791, 360, 321, 528, 281, 26118, 293, 4274, 493, 293, 550, 341, 1622, 307, 1566, 22667, 490, 264, 1791, 2644, 7890, 538, 3287, 552, 281, 3671, 34762, 13, 51790], "temperature": 0.0, "avg_logprob": -0.1393626630306244, "compression_ratio": 1.671875, "no_speech_prob": 0.00016027363017201424}, {"id": 765, "seek": 339526, "start": 3423.86, "end": 3425.0600000000004, "text": " We're saying that we will not.", "tokens": [51795, 492, 434, 1566, 300, 321, 486, 406, 13, 51855], "temperature": 0.0, "avg_logprob": -0.1393626630306244, "compression_ratio": 1.671875, "no_speech_prob": 0.00016027363017201424}, {"id": 766, "seek": 342526, "start": 3425.26, "end": 3427.0600000000004, "text": " Aggregate anything from those tokens.", "tokens": [50365, 41512, 3375, 473, 1340, 490, 729, 22667, 13, 50455], "temperature": 0.0, "avg_logprob": -0.1477443267559183, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0002931323542725295}, {"id": 767, "seek": 342526, "start": 3428.36, "end": 3433.76, "text": " And so basically this then goes through softmax and through the weighted and this is the aggregation through matrix multiplication.", "tokens": [50520, 400, 370, 1936, 341, 550, 1709, 807, 2787, 41167, 293, 807, 264, 32807, 293, 341, 307, 264, 16743, 399, 807, 8141, 27290, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1477443267559183, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0002931323542725295}, {"id": 768, "seek": 342526, "start": 3435.0600000000004, "end": 3438.0600000000004, "text": " And so what this is now is you can think of these as", "tokens": [50855, 400, 370, 437, 341, 307, 586, 307, 291, 393, 519, 295, 613, 382, 51005], "temperature": 0.0, "avg_logprob": -0.1477443267559183, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0002931323542725295}, {"id": 769, "seek": 342526, "start": 3438.86, "end": 3448.96, "text": " the zeros are currently just set by us to be zero but quick preview is that these affinities between the tokens are not going to be just constant at zero.", "tokens": [51045, 264, 35193, 366, 4362, 445, 992, 538, 505, 281, 312, 4018, 457, 1702, 14281, 307, 300, 613, 2096, 259, 1088, 1296, 264, 22667, 366, 406, 516, 281, 312, 445, 5754, 412, 4018, 13, 51550], "temperature": 0.0, "avg_logprob": -0.1477443267559183, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0002931323542725295}, {"id": 770, "seek": 342526, "start": 3449.26, "end": 3450.96, "text": " They're going to be data dependent.", "tokens": [51565, 814, 434, 516, 281, 312, 1412, 12334, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1477443267559183, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0002931323542725295}, {"id": 771, "seek": 342526, "start": 3451.26, "end": 3455.1600000000003, "text": " These tokens are going to start looking at each other and some tokens will find other tokens.", "tokens": [51665, 1981, 22667, 366, 516, 281, 722, 1237, 412, 1184, 661, 293, 512, 22667, 486, 915, 661, 22667, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1477443267559183, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.0002931323542725295}, {"id": 772, "seek": 345526, "start": 3455.36, "end": 3464.1600000000003, "text": " More or less interesting and depending on what their values are, they're going to find each other interesting to different amounts and I'm going to call those affinities.", "tokens": [50370, 5048, 420, 1570, 1880, 293, 5413, 322, 437, 641, 4190, 366, 11, 436, 434, 516, 281, 915, 1184, 661, 1880, 281, 819, 11663, 293, 286, 478, 516, 281, 818, 729, 2096, 259, 1088, 13, 50810], "temperature": 0.0, "avg_logprob": -0.14749590555826822, "compression_ratio": 1.8741007194244603, "no_speech_prob": 0.00010205674334429204}, {"id": 773, "seek": 345526, "start": 3464.1600000000003, "end": 3468.46, "text": " I think and then here we are saying the future cannot communicate with the past.", "tokens": [50810, 286, 519, 293, 550, 510, 321, 366, 1566, 264, 2027, 2644, 7890, 365, 264, 1791, 13, 51025], "temperature": 0.0, "avg_logprob": -0.14749590555826822, "compression_ratio": 1.8741007194244603, "no_speech_prob": 0.00010205674334429204}, {"id": 774, "seek": 345526, "start": 3468.96, "end": 3470.0600000000004, "text": " We're going to clamp them.", "tokens": [51050, 492, 434, 516, 281, 17690, 552, 13, 51105], "temperature": 0.0, "avg_logprob": -0.14749590555826822, "compression_ratio": 1.8741007194244603, "no_speech_prob": 0.00010205674334429204}, {"id": 775, "seek": 345526, "start": 3471.1600000000003, "end": 3477.86, "text": " And then when we normalize and some we're going to aggregate sort of their values depending on how interesting they find each other.", "tokens": [51160, 400, 550, 562, 321, 2710, 1125, 293, 512, 321, 434, 516, 281, 26118, 1333, 295, 641, 4190, 5413, 322, 577, 1880, 436, 915, 1184, 661, 13, 51495], "temperature": 0.0, "avg_logprob": -0.14749590555826822, "compression_ratio": 1.8741007194244603, "no_speech_prob": 0.00010205674334429204}, {"id": 776, "seek": 345526, "start": 3478.46, "end": 3484.76, "text": " And so that's the preview for self-attention and basically long story short from this entire section is that.", "tokens": [51525, 400, 370, 300, 311, 264, 14281, 337, 2698, 12, 1591, 1251, 293, 1936, 938, 1657, 2099, 490, 341, 2302, 3541, 307, 300, 13, 51840], "temperature": 0.0, "avg_logprob": -0.14749590555826822, "compression_ratio": 1.8741007194244603, "no_speech_prob": 0.00010205674334429204}, {"id": 777, "seek": 348526, "start": 3485.36, "end": 3495.0600000000004, "text": " You can do weighted aggregations of your past elements by having by using matrix multiplication of a lower triangular fashion.", "tokens": [50370, 509, 393, 360, 32807, 16743, 763, 295, 428, 1791, 4959, 538, 1419, 538, 1228, 8141, 27290, 295, 257, 3126, 38190, 6700, 13, 50855], "temperature": 0.0, "avg_logprob": -0.13824452939240828, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0008894902421161532}, {"id": 778, "seek": 348526, "start": 3495.76, "end": 3502.96, "text": " And then the elements here in the lower triangular part are telling you how much of each element fuses into this position.", "tokens": [50890, 400, 550, 264, 4959, 510, 294, 264, 3126, 38190, 644, 366, 3585, 291, 577, 709, 295, 1184, 4478, 283, 8355, 666, 341, 2535, 13, 51250], "temperature": 0.0, "avg_logprob": -0.13824452939240828, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0008894902421161532}, {"id": 779, "seek": 348526, "start": 3503.6600000000003, "end": 3506.76, "text": " So we're going to use this trick now to develop the self-attention block.", "tokens": [51285, 407, 321, 434, 516, 281, 764, 341, 4282, 586, 281, 1499, 264, 2698, 12, 1591, 1251, 3461, 13, 51440], "temperature": 0.0, "avg_logprob": -0.13824452939240828, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0008894902421161532}, {"id": 780, "seek": 348526, "start": 3507.26, "end": 3509.46, "text": " So first let's get some quick preliminaries out of the way.", "tokens": [51465, 407, 700, 718, 311, 483, 512, 1702, 26414, 259, 4889, 484, 295, 264, 636, 13, 51575], "temperature": 0.0, "avg_logprob": -0.13824452939240828, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0008894902421161532}, {"id": 781, "seek": 348526, "start": 3510.76, "end": 3514.96, "text": " First the thing I'm kind of bothered by is that you see how we're passing in vocab size into the constructor.", "tokens": [51640, 2386, 264, 551, 286, 478, 733, 295, 22996, 538, 307, 300, 291, 536, 577, 321, 434, 8437, 294, 2329, 455, 2744, 666, 264, 47479, 13, 51850], "temperature": 0.0, "avg_logprob": -0.13824452939240828, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.0008894902421161532}, {"id": 782, "seek": 351526, "start": 3515.36, "end": 3519.5600000000004, "text": " You don't need to do that because vocab size is already defined up top as a global variable.", "tokens": [50370, 509, 500, 380, 643, 281, 360, 300, 570, 2329, 455, 2744, 307, 1217, 7642, 493, 1192, 382, 257, 4338, 7006, 13, 50580], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 783, "seek": 351526, "start": 3519.5600000000004, "end": 3521.36, "text": " So there's no need to pass this stuff around.", "tokens": [50580, 407, 456, 311, 572, 643, 281, 1320, 341, 1507, 926, 13, 50670], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 784, "seek": 351526, "start": 3522.86, "end": 3526.0600000000004, "text": " Next what I want to do is I don't want to actually create.", "tokens": [50745, 3087, 437, 286, 528, 281, 360, 307, 286, 500, 380, 528, 281, 767, 1884, 13, 50905], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 785, "seek": 351526, "start": 3526.36, "end": 3532.26, "text": " I want to create like a level of indirection here where we don't directly go to the embedding for the logits.", "tokens": [50920, 286, 528, 281, 1884, 411, 257, 1496, 295, 1016, 621, 882, 510, 689, 321, 500, 380, 3838, 352, 281, 264, 12240, 3584, 337, 264, 3565, 1208, 13, 51215], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 786, "seek": 351526, "start": 3532.46, "end": 3536.96, "text": " But instead we go through this intermediate phase because we're going to start making that bigger.", "tokens": [51225, 583, 2602, 321, 352, 807, 341, 19376, 5574, 570, 321, 434, 516, 281, 722, 1455, 300, 3801, 13, 51450], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 787, "seek": 351526, "start": 3537.5600000000004, "end": 3543.36, "text": " So let me introduce a new variable and embed it short for number of embedding dimensions.", "tokens": [51480, 407, 718, 385, 5366, 257, 777, 7006, 293, 12240, 309, 2099, 337, 1230, 295, 12240, 3584, 12819, 13, 51770], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 788, "seek": 351526, "start": 3543.96, "end": 3544.76, "text": " So an embed.", "tokens": [51800, 407, 364, 12240, 13, 51840], "temperature": 0.0, "avg_logprob": -0.18647229394247367, "compression_ratio": 1.7612456747404843, "no_speech_prob": 0.00030326342675834894}, {"id": 789, "seek": 354526, "start": 3545.36, "end": 3548.36, "text": " Here will be say 32.", "tokens": [50370, 1692, 486, 312, 584, 8858, 13, 50520], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 790, "seek": 354526, "start": 3549.0600000000004, "end": 3551.26, "text": " That was a suggestion from GitHub copilot by the way.", "tokens": [50555, 663, 390, 257, 16541, 490, 23331, 2971, 31516, 538, 264, 636, 13, 50665], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 791, "seek": 354526, "start": 3551.96, "end": 3554.1600000000003, "text": " It also suggested 32 which is a good number.", "tokens": [50700, 467, 611, 10945, 8858, 597, 307, 257, 665, 1230, 13, 50810], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 792, "seek": 354526, "start": 3555.46, "end": 3558.86, "text": " So this is an embedding table and only 32 dimensional embeddings.", "tokens": [50875, 407, 341, 307, 364, 12240, 3584, 3199, 293, 787, 8858, 18795, 12240, 29432, 13, 51045], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 793, "seek": 354526, "start": 3559.96, "end": 3562.6600000000003, "text": " So then here this is not going to give us logits directly.", "tokens": [51100, 407, 550, 510, 341, 307, 406, 516, 281, 976, 505, 3565, 1208, 3838, 13, 51235], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 794, "seek": 354526, "start": 3563.1600000000003, "end": 3565.36, "text": " Instead this is going to give us token embeddings.", "tokens": [51260, 7156, 341, 307, 516, 281, 976, 505, 14862, 12240, 29432, 13, 51370], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 795, "seek": 354526, "start": 3565.96, "end": 3566.86, "text": " That's what I'm going to call it.", "tokens": [51400, 663, 311, 437, 286, 478, 516, 281, 818, 309, 13, 51445], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 796, "seek": 354526, "start": 3567.26, "end": 3571.0600000000004, "text": " And then to go from the token embeddings to the logits we're going to need a linear layer.", "tokens": [51465, 400, 550, 281, 352, 490, 264, 14862, 12240, 29432, 281, 264, 3565, 1208, 321, 434, 516, 281, 643, 257, 8213, 4583, 13, 51655], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 797, "seek": 354526, "start": 3571.5600000000004, "end": 3575.1600000000003, "text": " So self.lmhead let's call it short for language modeling head.", "tokens": [51680, 407, 2698, 13, 75, 76, 1934, 718, 311, 818, 309, 2099, 337, 2856, 15983, 1378, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15294069318629022, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.000244778988417238}, {"id": 798, "seek": 357526, "start": 3575.96, "end": 3578.76, "text": " Is an in linear from an embed up to vocab size.", "tokens": [50400, 1119, 364, 294, 8213, 490, 364, 12240, 493, 281, 2329, 455, 2744, 13, 50540], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 799, "seek": 357526, "start": 3579.6600000000003, "end": 3580.86, "text": " And then we swing over here.", "tokens": [50585, 400, 550, 321, 11173, 670, 510, 13, 50645], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 800, "seek": 357526, "start": 3580.96, "end": 3584.6600000000003, "text": " We're actually going to get the logits by exactly what the copilot says.", "tokens": [50650, 492, 434, 767, 516, 281, 483, 264, 3565, 1208, 538, 2293, 437, 264, 2971, 31516, 1619, 13, 50835], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 801, "seek": 357526, "start": 3585.76, "end": 3590.26, "text": " Now we have to be careful here because this C and this C are not equal.", "tokens": [50890, 823, 321, 362, 281, 312, 5026, 510, 570, 341, 383, 293, 341, 383, 366, 406, 2681, 13, 51115], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 802, "seek": 357526, "start": 3591.36, "end": 3593.76, "text": " This is an embed C and this is vocab size.", "tokens": [51170, 639, 307, 364, 12240, 383, 293, 341, 307, 2329, 455, 2744, 13, 51290], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 803, "seek": 357526, "start": 3594.76, "end": 3597.26, "text": " So let's just say that an embed is equal to C.", "tokens": [51340, 407, 718, 311, 445, 584, 300, 364, 12240, 307, 2681, 281, 383, 13, 51465], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 804, "seek": 357526, "start": 3598.6600000000003, "end": 3602.86, "text": " And then this just creates one spurious layer of indirection through a linear layer.", "tokens": [51535, 400, 550, 341, 445, 7829, 472, 637, 24274, 4583, 295, 1016, 621, 882, 807, 257, 8213, 4583, 13, 51745], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 805, "seek": 357526, "start": 3603.36, "end": 3605.0600000000004, "text": " But this should basically run.", "tokens": [51770, 583, 341, 820, 1936, 1190, 13, 51855], "temperature": 0.0, "avg_logprob": -0.17911483446756998, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.0001928462734213099}, {"id": 806, "seek": 360526, "start": 3605.26, "end": 3616.36, "text": " So we see that this runs and this currently looks kind of spurious.", "tokens": [50365, 407, 321, 536, 300, 341, 6676, 293, 341, 4362, 1542, 733, 295, 637, 24274, 13, 50920], "temperature": 0.0, "avg_logprob": -0.17926937600840692, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.00040887887007556856}, {"id": 807, "seek": 360526, "start": 3616.36, "end": 3618.1600000000003, "text": " But we're going to build on top of this.", "tokens": [50920, 583, 321, 434, 516, 281, 1322, 322, 1192, 295, 341, 13, 51010], "temperature": 0.0, "avg_logprob": -0.17926937600840692, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.00040887887007556856}, {"id": 808, "seek": 360526, "start": 3618.86, "end": 3620.26, "text": " Now next up so far.", "tokens": [51045, 823, 958, 493, 370, 1400, 13, 51115], "temperature": 0.0, "avg_logprob": -0.17926937600840692, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.00040887887007556856}, {"id": 809, "seek": 360526, "start": 3620.26, "end": 3627.46, "text": " We've taken these indices and we've encoded them based on the identity of the tokens inside IDX.", "tokens": [51115, 492, 600, 2726, 613, 43840, 293, 321, 600, 2058, 12340, 552, 2361, 322, 264, 6575, 295, 264, 22667, 1854, 7348, 55, 13, 51475], "temperature": 0.0, "avg_logprob": -0.17926937600840692, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.00040887887007556856}, {"id": 810, "seek": 360526, "start": 3628.1600000000003, "end": 3634.26, "text": " The next thing that people very often do is that we're not just encoding the identity of these tokens, but also their position.", "tokens": [51510, 440, 958, 551, 300, 561, 588, 2049, 360, 307, 300, 321, 434, 406, 445, 43430, 264, 6575, 295, 613, 22667, 11, 457, 611, 641, 2535, 13, 51815], "temperature": 0.0, "avg_logprob": -0.17926937600840692, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.00040887887007556856}, {"id": 811, "seek": 363426, "start": 3634.76, "end": 3637.86, "text": " So we're going to have a second position embedding table here.", "tokens": [50390, 407, 321, 434, 516, 281, 362, 257, 1150, 2535, 12240, 3584, 3199, 510, 13, 50545], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 812, "seek": 363426, "start": 3638.26, "end": 3643.46, "text": " So solve that position embedding table is an embedding of block size by an embed.", "tokens": [50565, 407, 5039, 300, 2535, 12240, 3584, 3199, 307, 364, 12240, 3584, 295, 3461, 2744, 538, 364, 12240, 13, 50825], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 813, "seek": 363426, "start": 3643.96, "end": 3648.36, "text": " And so each position from zero to block size minus one will also get its own embedding vector.", "tokens": [50850, 400, 370, 1184, 2535, 490, 4018, 281, 3461, 2744, 3175, 472, 486, 611, 483, 1080, 1065, 12240, 3584, 8062, 13, 51070], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 814, "seek": 363426, "start": 3649.46, "end": 3653.76, "text": " And then here first, let me decode B by T from IDX dot shape.", "tokens": [51125, 400, 550, 510, 700, 11, 718, 385, 979, 1429, 363, 538, 314, 490, 7348, 55, 5893, 3909, 13, 51340], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 815, "seek": 363426, "start": 3655.36, "end": 3659.1600000000003, "text": " And then here we're also going to have a plus embedding, which is the positional embedding.", "tokens": [51420, 400, 550, 510, 321, 434, 611, 516, 281, 362, 257, 1804, 12240, 3584, 11, 597, 307, 264, 2535, 304, 12240, 3584, 13, 51610], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 816, "seek": 363426, "start": 3659.26, "end": 3661.0600000000004, "text": " And these are this is torr dash arrange.", "tokens": [51615, 400, 613, 366, 341, 307, 3930, 81, 8240, 9424, 13, 51705], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 817, "seek": 363426, "start": 3661.5600000000004, "end": 3664.0600000000004, "text": " So this will be basically just integers from zero to zero.", "tokens": [51730, 407, 341, 486, 312, 1936, 445, 41674, 490, 4018, 281, 4018, 13, 51855], "temperature": 0.0, "avg_logprob": -0.21909657559653586, "compression_ratio": 1.972, "no_speech_prob": 0.0001465856039430946}, {"id": 818, "seek": 366426, "start": 3664.26, "end": 3665.1600000000003, "text": " To T minus one.", "tokens": [50365, 1407, 314, 3175, 472, 13, 50410], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 819, "seek": 366426, "start": 3666.1600000000003, "end": 3671.0600000000004, "text": " And all of those integers from zero to T minus one get embedded through the table to create a T by C.", "tokens": [50460, 400, 439, 295, 729, 41674, 490, 4018, 281, 314, 3175, 472, 483, 16741, 807, 264, 3199, 281, 1884, 257, 314, 538, 383, 13, 50705], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 820, "seek": 366426, "start": 3672.36, "end": 3680.76, "text": " And then here this gets renamed to just say X and X will be the addition of the token embeddings with the positional embeddings.", "tokens": [50770, 400, 550, 510, 341, 2170, 40949, 281, 445, 584, 1783, 293, 1783, 486, 312, 264, 4500, 295, 264, 14862, 12240, 29432, 365, 264, 2535, 304, 12240, 29432, 13, 51190], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 821, "seek": 366426, "start": 3681.96, "end": 3683.86, "text": " And here the broadcasting note will work out.", "tokens": [51250, 400, 510, 264, 30024, 3637, 486, 589, 484, 13, 51345], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 822, "seek": 366426, "start": 3683.86, "end": 3685.96, "text": " So B by T by C plus T by C.", "tokens": [51345, 407, 363, 538, 314, 538, 383, 1804, 314, 538, 383, 13, 51450], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 823, "seek": 366426, "start": 3686.46, "end": 3691.5600000000004, "text": " This gets right aligned and new dimension of one gets added and it gets broadcasted across batch.", "tokens": [51475, 639, 2170, 558, 17962, 293, 777, 10139, 295, 472, 2170, 3869, 293, 309, 2170, 9975, 292, 2108, 15245, 13, 51730], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 824, "seek": 366426, "start": 3692.76, "end": 3694.1600000000003, "text": " So at this point X.", "tokens": [51790, 407, 412, 341, 935, 1783, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12454950398412244, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.0002518492692615837}, {"id": 825, "seek": 369426, "start": 3694.26, "end": 3698.86, "text": " Holds not just the token identities, but the positions at which these tokens occur.", "tokens": [50365, 6962, 82, 406, 445, 264, 14862, 24239, 11, 457, 264, 8432, 412, 597, 613, 22667, 5160, 13, 50595], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 826, "seek": 369426, "start": 3699.6600000000003, "end": 3703.36, "text": " And this is currently not that useful because of course, we just have a simple migraine model.", "tokens": [50635, 400, 341, 307, 4362, 406, 300, 4420, 570, 295, 1164, 11, 321, 445, 362, 257, 2199, 6186, 11798, 2316, 13, 50820], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 827, "seek": 369426, "start": 3703.46, "end": 3709.1600000000003, "text": " So it doesn't matter if you're in the fifth position, the second position or wherever it's all translation invariant at this stage.", "tokens": [50825, 407, 309, 1177, 380, 1871, 498, 291, 434, 294, 264, 9266, 2535, 11, 264, 1150, 2535, 420, 8660, 309, 311, 439, 12853, 33270, 394, 412, 341, 3233, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 828, "seek": 369426, "start": 3709.6600000000003, "end": 3711.46, "text": " So this information currently wouldn't help.", "tokens": [51135, 407, 341, 1589, 4362, 2759, 380, 854, 13, 51225], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 829, "seek": 369426, "start": 3712.0600000000004, "end": 3715.6600000000003, "text": " But as we work on the self-attention block, we'll see that this starts to matter.", "tokens": [51255, 583, 382, 321, 589, 322, 264, 2698, 12, 1591, 1251, 3461, 11, 321, 603, 536, 300, 341, 3719, 281, 1871, 13, 51435], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 830, "seek": 369426, "start": 3719.86, "end": 3721.96, "text": " Okay, so now we get the crux of self-attention.", "tokens": [51645, 1033, 11, 370, 586, 321, 483, 264, 5140, 87, 295, 2698, 12, 1591, 1251, 13, 51750], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 831, "seek": 369426, "start": 3722.26, "end": 3724.1600000000003, "text": " So this is probably the most important part of this video.", "tokens": [51765, 407, 341, 307, 1391, 264, 881, 1021, 644, 295, 341, 960, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12950407153498517, "compression_ratio": 1.7548387096774194, "no_speech_prob": 0.0001814194256439805}, {"id": 832, "seek": 372416, "start": 3724.3599999999997, "end": 3725.3599999999997, "text": " To understand.", "tokens": [50375, 1407, 1223, 13, 50425], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 833, "seek": 372416, "start": 3726.46, "end": 3730.56, "text": " We're going to implement a small self-attention for a single individual head as they're called.", "tokens": [50480, 492, 434, 516, 281, 4445, 257, 1359, 2698, 12, 1591, 1251, 337, 257, 2167, 2609, 1378, 382, 436, 434, 1219, 13, 50685], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 834, "seek": 372416, "start": 3731.16, "end": 3732.96, "text": " So we start off with where we were.", "tokens": [50715, 407, 321, 722, 766, 365, 689, 321, 645, 13, 50805], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 835, "seek": 372416, "start": 3733.16, "end": 3734.56, "text": " So all of this code is familiar.", "tokens": [50815, 407, 439, 295, 341, 3089, 307, 4963, 13, 50885], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 836, "seek": 372416, "start": 3735.3599999999997, "end": 3740.06, "text": " So right now I'm working with an example where I change the number of channels from 2 to 32.", "tokens": [50925, 407, 558, 586, 286, 478, 1364, 365, 364, 1365, 689, 286, 1319, 264, 1230, 295, 9235, 490, 568, 281, 8858, 13, 51160], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 837, "seek": 372416, "start": 3740.06, "end": 3748.16, "text": " So we have a 4 by 8 arrangement of tokens and each token and the information at each token is currently 32 dimensional.", "tokens": [51160, 407, 321, 362, 257, 1017, 538, 1649, 17620, 295, 22667, 293, 1184, 14862, 293, 264, 1589, 412, 1184, 14862, 307, 4362, 8858, 18795, 13, 51565], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 838, "seek": 372416, "start": 3748.2599999999998, "end": 3750.06, "text": " But we just are working with random numbers.", "tokens": [51570, 583, 321, 445, 366, 1364, 365, 4974, 3547, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 839, "seek": 372416, "start": 3751.3599999999997, "end": 3752.96, "text": " Now we saw here that", "tokens": [51725, 823, 321, 1866, 510, 300, 51805], "temperature": 0.0, "avg_logprob": -0.14234133898201634, "compression_ratio": 1.6474820143884892, "no_speech_prob": 0.00047018835903145373}, {"id": 840, "seek": 375296, "start": 3752.96, "end": 3762.26, "text": " The code as we had it before does a simple weight simple average of all the past tokens and the current token.", "tokens": [50365, 440, 3089, 382, 321, 632, 309, 949, 775, 257, 2199, 3364, 2199, 4274, 295, 439, 264, 1791, 22667, 293, 264, 2190, 14862, 13, 50830], "temperature": 0.0, "avg_logprob": -0.1739911183272258, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0009800726547837257}, {"id": 841, "seek": 375296, "start": 3762.46, "end": 3766.66, "text": " So it's just the previous information and current information is just being mixed together in an average.", "tokens": [50840, 407, 309, 311, 445, 264, 3894, 1589, 293, 2190, 1589, 307, 445, 885, 7467, 1214, 294, 364, 4274, 13, 51050], "temperature": 0.0, "avg_logprob": -0.1739911183272258, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0009800726547837257}, {"id": 842, "seek": 375296, "start": 3767.36, "end": 3769.26, "text": " And that's what this code currently achieves.", "tokens": [51085, 400, 300, 311, 437, 341, 3089, 4362, 3538, 977, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1739911183272258, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0009800726547837257}, {"id": 843, "seek": 375296, "start": 3769.56, "end": 3777.26, "text": " And it does so by creating this lower triangular structure, which allows us to mask out this weight matrix that we create.", "tokens": [51195, 400, 309, 775, 370, 538, 4084, 341, 3126, 38190, 3877, 11, 597, 4045, 505, 281, 6094, 484, 341, 3364, 8141, 300, 321, 1884, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1739911183272258, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0009800726547837257}, {"id": 844, "seek": 375296, "start": 3777.76, "end": 3781.66, "text": " So we mask it out and then we normalize it and currently", "tokens": [51605, 407, 321, 6094, 309, 484, 293, 550, 321, 2710, 1125, 309, 293, 4362, 51800], "temperature": 0.0, "avg_logprob": -0.1739911183272258, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0009800726547837257}, {"id": 845, "seek": 378166, "start": 3781.66, "end": 3789.7599999999998, "text": " When we initialize the affinities between all the different sort of tokens or nodes, I'm going to use those terms interchangeably.", "tokens": [50365, 1133, 321, 5883, 1125, 264, 2096, 259, 1088, 1296, 439, 264, 819, 1333, 295, 22667, 420, 13891, 11, 286, 478, 516, 281, 764, 729, 2115, 30358, 1188, 13, 50770], "temperature": 0.0, "avg_logprob": -0.15579357147216796, "compression_ratio": 1.772, "no_speech_prob": 0.00016209996829275042}, {"id": 846, "seek": 378166, "start": 3790.66, "end": 3799.66, "text": " So when we initialize the affinities between all the different tokens to be zero, then we see that way gives us this structure where every single row has these", "tokens": [50815, 407, 562, 321, 5883, 1125, 264, 2096, 259, 1088, 1296, 439, 264, 819, 22667, 281, 312, 4018, 11, 550, 321, 536, 300, 636, 2709, 505, 341, 3877, 689, 633, 2167, 5386, 575, 613, 51265], "temperature": 0.0, "avg_logprob": -0.15579357147216796, "compression_ratio": 1.772, "no_speech_prob": 0.00016209996829275042}, {"id": 847, "seek": 378166, "start": 3801.16, "end": 3802.06, "text": " Uniform numbers.", "tokens": [51340, 1156, 8629, 3547, 13, 51385], "temperature": 0.0, "avg_logprob": -0.15579357147216796, "compression_ratio": 1.772, "no_speech_prob": 0.00016209996829275042}, {"id": 848, "seek": 378166, "start": 3802.46, "end": 3808.66, "text": " And so that's what that's what then in this matrix multiply makes it so that we're doing a simple average.", "tokens": [51405, 400, 370, 300, 311, 437, 300, 311, 437, 550, 294, 341, 8141, 12972, 1669, 309, 370, 300, 321, 434, 884, 257, 2199, 4274, 13, 51715], "temperature": 0.0, "avg_logprob": -0.15579357147216796, "compression_ratio": 1.772, "no_speech_prob": 0.00016209996829275042}, {"id": 849, "seek": 378166, "start": 3809.66, "end": 3810.16, "text": " Now,", "tokens": [51765, 823, 11, 51790], "temperature": 0.0, "avg_logprob": -0.15579357147216796, "compression_ratio": 1.772, "no_speech_prob": 0.00016209996829275042}, {"id": 850, "seek": 378166, "start": 3810.8599999999997, "end": 3811.56, "text": " We don't actually want.", "tokens": [51825, 492, 500, 380, 767, 528, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15579357147216796, "compression_ratio": 1.772, "no_speech_prob": 0.00016209996829275042}, {"id": 851, "seek": 381166, "start": 3811.8599999999997, "end": 3812.66, "text": " This to be", "tokens": [50375, 639, 281, 312, 50415], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 852, "seek": 381166, "start": 3813.3599999999997, "end": 3814.2599999999998, "text": " All uniform", "tokens": [50450, 1057, 9452, 50495], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 853, "seek": 381166, "start": 3814.66, "end": 3821.2599999999998, "text": " Because different tokens will find different other tokens more or less interesting and we want that to be data dependent.", "tokens": [50515, 1436, 819, 22667, 486, 915, 819, 661, 22667, 544, 420, 1570, 1880, 293, 321, 528, 300, 281, 312, 1412, 12334, 13, 50845], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 854, "seek": 381166, "start": 3821.46, "end": 3830.06, "text": " So for example, if I'm a vowel then maybe I'm looking for consonants in my past and maybe I want to know what those consonants are and I want that information to flow to me.", "tokens": [50855, 407, 337, 1365, 11, 498, 286, 478, 257, 29410, 550, 1310, 286, 478, 1237, 337, 30843, 1719, 294, 452, 1791, 293, 1310, 286, 528, 281, 458, 437, 729, 30843, 1719, 366, 293, 286, 528, 300, 1589, 281, 3095, 281, 385, 13, 51285], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 855, "seek": 381166, "start": 3831.16, "end": 3835.96, "text": " And so I want to now gather information from the past, but I want to do it in a data dependent way.", "tokens": [51340, 400, 370, 286, 528, 281, 586, 5448, 1589, 490, 264, 1791, 11, 457, 286, 528, 281, 360, 309, 294, 257, 1412, 12334, 636, 13, 51580], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 856, "seek": 381166, "start": 3836.2599999999998, "end": 3838.16, "text": " And this is the problem that self-attention solves.", "tokens": [51595, 400, 341, 307, 264, 1154, 300, 2698, 12, 1591, 1251, 39890, 13, 51690], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 857, "seek": 381166, "start": 3838.96, "end": 3840.8599999999997, "text": " Now the way self-attention solves this", "tokens": [51730, 823, 264, 636, 2698, 12, 1591, 1251, 39890, 341, 51825], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 858, "seek": 381166, "start": 3841.06, "end": 3841.56, "text": " Is the following.", "tokens": [51835, 1119, 264, 3480, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15919501799389832, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0004748903156723827}, {"id": 859, "seek": 384166, "start": 3842.2599999999998, "end": 3847.7599999999998, "text": " Every single node or every single token at each position will emit two vectors.", "tokens": [50395, 2048, 2167, 9984, 420, 633, 2167, 14862, 412, 1184, 2535, 486, 32084, 732, 18875, 13, 50670], "temperature": 0.0, "avg_logprob": -0.17810734282148646, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0004935046308673918}, {"id": 860, "seek": 384166, "start": 3848.46, "end": 3851.8599999999997, "text": " It will emit a query and it will emit a key.", "tokens": [50705, 467, 486, 32084, 257, 14581, 293, 309, 486, 32084, 257, 2141, 13, 50875], "temperature": 0.0, "avg_logprob": -0.17810734282148646, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0004935046308673918}, {"id": 861, "seek": 384166, "start": 3853.3599999999997, "end": 3857.3599999999997, "text": " Now the query vector roughly speaking is what am I looking for?", "tokens": [50950, 823, 264, 14581, 8062, 9810, 4124, 307, 437, 669, 286, 1237, 337, 30, 51150], "temperature": 0.0, "avg_logprob": -0.17810734282148646, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0004935046308673918}, {"id": 862, "seek": 384166, "start": 3858.2599999999998, "end": 3861.16, "text": " And the key vector roughly speaking is what do I contain?", "tokens": [51195, 400, 264, 2141, 8062, 9810, 4124, 307, 437, 360, 286, 5304, 30, 51340], "temperature": 0.0, "avg_logprob": -0.17810734282148646, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0004935046308673918}, {"id": 863, "seek": 384166, "start": 3862.56, "end": 3871.46, "text": " And then the way we get affinities between these tokens now in a sequence is we basically just do a dot product between the keys and the query.", "tokens": [51410, 400, 550, 264, 636, 321, 483, 2096, 259, 1088, 1296, 613, 22667, 586, 294, 257, 8310, 307, 321, 1936, 445, 360, 257, 5893, 1674, 1296, 264, 9317, 293, 264, 14581, 13, 51855], "temperature": 0.0, "avg_logprob": -0.17810734282148646, "compression_ratio": 1.8483412322274881, "no_speech_prob": 0.0004935046308673918}, {"id": 864, "seek": 387166, "start": 3872.2599999999998, "end": 3881.2599999999998, "text": " So my query dot products with all the keys of all the other tokens and that dot product now becomes way.", "tokens": [50395, 407, 452, 14581, 5893, 3383, 365, 439, 264, 9317, 295, 439, 264, 661, 22667, 293, 300, 5893, 1674, 586, 3643, 636, 13, 50845], "temperature": 0.0, "avg_logprob": -0.206475438950937, "compression_ratio": 1.67, "no_speech_prob": 0.001408519223332405}, {"id": 865, "seek": 387166, "start": 3882.2599999999998, "end": 3896.2599999999998, "text": " And so if the key and the query are sort of aligned, they will interact to a very high amount and then I will get to learn more about that specific token as opposed to any other token in the sequence.", "tokens": [50895, 400, 370, 498, 264, 2141, 293, 264, 14581, 366, 1333, 295, 17962, 11, 436, 486, 4648, 281, 257, 588, 1090, 2372, 293, 550, 286, 486, 483, 281, 1466, 544, 466, 300, 2685, 14862, 382, 8851, 281, 604, 661, 14862, 294, 264, 8310, 13, 51595], "temperature": 0.0, "avg_logprob": -0.206475438950937, "compression_ratio": 1.67, "no_speech_prob": 0.001408519223332405}, {"id": 866, "seek": 387166, "start": 3896.46, "end": 3897.46, "text": " So let's implement this now.", "tokens": [51605, 407, 718, 311, 4445, 341, 586, 13, 51655], "temperature": 0.0, "avg_logprob": -0.206475438950937, "compression_ratio": 1.67, "no_speech_prob": 0.001408519223332405}, {"id": 867, "seek": 390166, "start": 3901.66, "end": 3903.06, "text": " We're going to implement a single", "tokens": [50365, 492, 434, 516, 281, 4445, 257, 2167, 50435], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 868, "seek": 390166, "start": 3904.66, "end": 3906.7599999999998, "text": " what's called head of self-attention.", "tokens": [50515, 437, 311, 1219, 1378, 295, 2698, 12, 1591, 1251, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 869, "seek": 390166, "start": 3908.06, "end": 3909.2599999999998, "text": " So this is just one head.", "tokens": [50685, 407, 341, 307, 445, 472, 1378, 13, 50745], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 870, "seek": 390166, "start": 3909.66, "end": 3912.7599999999998, "text": " There's a hyper parameter involved with these heads, which is the head size.", "tokens": [50765, 821, 311, 257, 9848, 13075, 3288, 365, 613, 8050, 11, 597, 307, 264, 1378, 2744, 13, 50920], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 871, "seek": 390166, "start": 3913.46, "end": 3918.06, "text": " And then here I'm initializing linear modules and I'm using bias equals false.", "tokens": [50955, 400, 550, 510, 286, 478, 5883, 3319, 8213, 16679, 293, 286, 478, 1228, 12577, 6915, 7908, 13, 51185], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 872, "seek": 390166, "start": 3918.06, "end": 3921.56, "text": " So these are just going to apply a matrix multiply with some fixed weights.", "tokens": [51185, 407, 613, 366, 445, 516, 281, 3079, 257, 8141, 12972, 365, 512, 6806, 17443, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 873, "seek": 390166, "start": 3922.7599999999998, "end": 3929.8599999999997, "text": " And now let me produce a key and Q K and Q by forwarding these modules on X.", "tokens": [51420, 400, 586, 718, 385, 5258, 257, 2141, 293, 1249, 591, 293, 1249, 538, 2128, 278, 613, 16679, 322, 1783, 13, 51775], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 874, "seek": 390166, "start": 3930.96, "end": 3931.56, "text": " So the size of this.", "tokens": [51830, 407, 264, 2744, 295, 341, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1333575621895168, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0002876119106076658}, {"id": 875, "seek": 393166, "start": 3931.7599999999998, "end": 3932.8599999999997, "text": " This will not become", "tokens": [50370, 639, 486, 406, 1813, 50425], "temperature": 0.0, "avg_logprob": -0.1808126698369565, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00011304874351480976}, {"id": 876, "seek": 393166, "start": 3933.7599999999998, "end": 3940.16, "text": " B by T by 16 because that is the head size and the same here B by T by 16.", "tokens": [50470, 363, 538, 314, 538, 3165, 570, 300, 307, 264, 1378, 2744, 293, 264, 912, 510, 363, 538, 314, 538, 3165, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1808126698369565, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00011304874351480976}, {"id": 877, "seek": 393166, "start": 3945.8599999999997, "end": 3947.06, "text": " So this being the head size.", "tokens": [51075, 407, 341, 885, 264, 1378, 2744, 13, 51135], "temperature": 0.0, "avg_logprob": -0.1808126698369565, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00011304874351480976}, {"id": 878, "seek": 393166, "start": 3947.66, "end": 3959.66, "text": " So you see here that when I forward this linear on top of my X all the tokens in all the positions in the B by T arrangement all of them in parallel and independently produce a key and a query.", "tokens": [51165, 407, 291, 536, 510, 300, 562, 286, 2128, 341, 8213, 322, 1192, 295, 452, 1783, 439, 264, 22667, 294, 439, 264, 8432, 294, 264, 363, 538, 314, 17620, 439, 295, 552, 294, 8952, 293, 21761, 5258, 257, 2141, 293, 257, 14581, 13, 51765], "temperature": 0.0, "avg_logprob": -0.1808126698369565, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00011304874351480976}, {"id": 879, "seek": 393166, "start": 3959.66, "end": 3961.2599999999998, "text": " So no communication has happened yet.", "tokens": [51765, 407, 572, 6101, 575, 2011, 1939, 13, 51845], "temperature": 0.0, "avg_logprob": -0.1808126698369565, "compression_ratio": 1.6713615023474178, "no_speech_prob": 0.00011304874351480976}, {"id": 880, "seek": 396166, "start": 3962.66, "end": 3963.96, "text": " But the communication comes now.", "tokens": [50415, 583, 264, 6101, 1487, 586, 13, 50480], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 881, "seek": 396166, "start": 3964.06, "end": 3967.2599999999998, "text": " All the queries will dot product with all the keys.", "tokens": [50485, 1057, 264, 24109, 486, 5893, 1674, 365, 439, 264, 9317, 13, 50645], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 882, "seek": 396166, "start": 3968.56, "end": 3975.8599999999997, "text": " So basically what we want is we want way now or the affinities between these to be query multiplying key,", "tokens": [50710, 407, 1936, 437, 321, 528, 307, 321, 528, 636, 586, 420, 264, 2096, 259, 1088, 1296, 613, 281, 312, 14581, 30955, 2141, 11, 51075], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 883, "seek": 396166, "start": 3976.56, "end": 3979.16, "text": " but we have to be careful with we can't matrix multiply this.", "tokens": [51110, 457, 321, 362, 281, 312, 5026, 365, 321, 393, 380, 8141, 12972, 341, 13, 51240], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 884, "seek": 396166, "start": 3979.16, "end": 3986.46, "text": " We actually need to transpose K but we have to be also careful because these are when you have the batch dimension.", "tokens": [51240, 492, 767, 643, 281, 25167, 591, 457, 321, 362, 281, 312, 611, 5026, 570, 613, 366, 562, 291, 362, 264, 15245, 10139, 13, 51605], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 885, "seek": 396166, "start": 3986.8599999999997, "end": 3991.16, "text": " So in particular we want to transpose the last two dimensions.", "tokens": [51625, 407, 294, 1729, 321, 528, 281, 25167, 264, 1036, 732, 12819, 13, 51840], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 886, "seek": 396166, "start": 3991.2599999999998, "end": 3991.56, "text": " Dimension.", "tokens": [51845, 20975, 3378, 13, 51860], "temperature": 0.0, "avg_logprob": -0.18306458437884296, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.0004155532515142113}, {"id": 887, "seek": 399156, "start": 3991.66, "end": 3993.36, "text": " Negative one and dimension negative two.", "tokens": [50370, 43230, 472, 293, 10139, 3671, 732, 13, 50455], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 888, "seek": 399156, "start": 3994.06, "end": 3996.2599999999998, "text": " So negative two negative one.", "tokens": [50490, 407, 3671, 732, 3671, 472, 13, 50600], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 889, "seek": 399156, "start": 3997.56, "end": 4003.06, "text": " And so this matrix multiply now will basically do the following B by T by 16.", "tokens": [50665, 400, 370, 341, 8141, 12972, 586, 486, 1936, 360, 264, 3480, 363, 538, 314, 538, 3165, 13, 50940], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 890, "seek": 399156, "start": 4005.2599999999998, "end": 4012.36, "text": " Matrix multiplies B by 16 by T to give us B by T by T.", "tokens": [51050, 36274, 12788, 530, 363, 538, 3165, 538, 314, 281, 976, 505, 363, 538, 314, 538, 314, 13, 51405], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 891, "seek": 399156, "start": 4014.46, "end": 4014.86, "text": " Right?", "tokens": [51510, 1779, 30, 51530], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 892, "seek": 399156, "start": 4016.06, "end": 4017.86, "text": " So for every row of B,", "tokens": [51590, 407, 337, 633, 5386, 295, 363, 11, 51680], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 893, "seek": 399156, "start": 4018.16, "end": 4021.46, "text": " we're not going to have a T square matrix giving us the affinity.", "tokens": [51695, 321, 434, 406, 516, 281, 362, 257, 314, 3732, 8141, 2902, 505, 264, 39703, 13, 51860], "temperature": 0.0, "avg_logprob": -0.28176660647337465, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0001859918556874618}, {"id": 894, "seek": 402146, "start": 4021.56, "end": 4027.36, "text": " We're going to have a T square matrix giving us the affinities and these are now the way so they're not zeros.", "tokens": [50370, 492, 434, 516, 281, 362, 257, 314, 3732, 8141, 2902, 505, 264, 2096, 259, 1088, 293, 613, 366, 586, 264, 636, 370, 436, 434, 406, 35193, 13, 50660], "temperature": 0.0, "avg_logprob": -0.3487935361608995, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0005154867540113628}, {"id": 895, "seek": 402146, "start": 4027.56, "end": 4031.36, "text": " They are now coming from this dot product between the keys in the queries.", "tokens": [50670, 814, 366, 586, 1348, 490, 341, 5893, 1674, 1296, 264, 9317, 294, 264, 24109, 13, 50860], "temperature": 0.0, "avg_logprob": -0.3487935361608995, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0005154867540113628}, {"id": 896, "seek": 402146, "start": 4031.56, "end": 4041.36, "text": " So this can now run I can I can run this and the weighted aggregation now is a function in a data abandoned manner between the keys and queries of these notes.", "tokens": [50870, 407, 341, 393, 586, 1190, 286, 393, 286, 393, 1190, 341, 293, 264, 32807, 16743, 399, 586, 307, 257, 2445, 294, 257, 1412, 13732, 9060, 1296, 264, 9317, 293, 24109, 295, 613, 5570, 13, 51360], "temperature": 0.0, "avg_logprob": -0.3487935361608995, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0005154867540113628}, {"id": 897, "seek": 402146, "start": 4041.56, "end": 4047.16, "text": " So just inspecting what happened here the way takes on this form.", "tokens": [51370, 407, 445, 15018, 278, 437, 2011, 510, 264, 636, 2516, 322, 341, 1254, 13, 51650], "temperature": 0.0, "avg_logprob": -0.3487935361608995, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0005154867540113628}, {"id": 898, "seek": 402146, "start": 4047.36, "end": 4051.16, "text": " And you see that before way was just a constant.", "tokens": [51660, 400, 291, 536, 300, 949, 636, 390, 445, 257, 5754, 13, 51850], "temperature": 0.0, "avg_logprob": -0.3487935361608995, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0005154867540113628}, {"id": 899, "seek": 405116, "start": 4051.16, "end": 4053.06, "text": " There's no way to all the batch elements.", "tokens": [50365, 821, 311, 572, 636, 281, 439, 264, 15245, 4959, 13, 50460], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 900, "seek": 405116, "start": 4053.2599999999998, "end": 4061.66, "text": " But now every single batch elements will have different sort of way because every single batch element contains different tokens at different positions.", "tokens": [50470, 583, 586, 633, 2167, 15245, 4959, 486, 362, 819, 1333, 295, 636, 570, 633, 2167, 15245, 4478, 8306, 819, 22667, 412, 819, 8432, 13, 50890], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 901, "seek": 405116, "start": 4061.8599999999997, "end": 4063.3599999999997, "text": " And so this is not data dependent.", "tokens": [50900, 400, 370, 341, 307, 406, 1412, 12334, 13, 50975], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 902, "seek": 405116, "start": 4064.16, "end": 4067.06, "text": " So when we look at just the zero row,", "tokens": [51015, 407, 562, 321, 574, 412, 445, 264, 4018, 5386, 11, 51160], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 903, "seek": 405116, "start": 4067.2599999999998, "end": 4068.46, "text": " for example in the input,", "tokens": [51170, 337, 1365, 294, 264, 4846, 11, 51230], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 904, "seek": 405116, "start": 4069.06, "end": 4070.8599999999997, "text": " these are the weights that came out.", "tokens": [51260, 613, 366, 264, 17443, 300, 1361, 484, 13, 51350], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 905, "seek": 405116, "start": 4071.2599999999998, "end": 4073.46, "text": " And so you can see now that they're not just exactly uniform.", "tokens": [51370, 400, 370, 291, 393, 536, 586, 300, 436, 434, 406, 445, 2293, 9452, 13, 51480], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 906, "seek": 405116, "start": 4075.2599999999998, "end": 4077.46, "text": " And in particular as an example here for the last row,", "tokens": [51570, 400, 294, 1729, 382, 364, 1365, 510, 337, 264, 1036, 5386, 11, 51680], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 907, "seek": 405116, "start": 4077.8599999999997, "end": 4081.06, "text": " this was the eighth token and the eighth token knows what content.", "tokens": [51700, 341, 390, 264, 19495, 14862, 293, 264, 19495, 14862, 3255, 437, 2701, 13, 51860], "temperature": 0.0, "avg_logprob": -0.25047065734863283, "compression_ratio": 1.8690909090909091, "no_speech_prob": 0.0002477969683241099}, {"id": 908, "seek": 408106, "start": 4081.16, "end": 4083.46, "text": " It has and it knows at what position it's in.", "tokens": [50370, 467, 575, 293, 309, 3255, 412, 437, 2535, 309, 311, 294, 13, 50485], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 909, "seek": 408106, "start": 4084.16, "end": 4088.2599999999998, "text": " And now the eight token based on that creates a query.", "tokens": [50520, 400, 586, 264, 3180, 14862, 2361, 322, 300, 7829, 257, 14581, 13, 50725], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 910, "seek": 408106, "start": 4088.56, "end": 4089.96, "text": " Hey, I'm looking for this kind of stuff.", "tokens": [50740, 1911, 11, 286, 478, 1237, 337, 341, 733, 295, 1507, 13, 50810], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 911, "seek": 408106, "start": 4091.06, "end": 4091.66, "text": " I'm a vowel.", "tokens": [50865, 286, 478, 257, 29410, 13, 50895], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 912, "seek": 408106, "start": 4091.86, "end": 4092.66, "text": " I'm on the eighth position.", "tokens": [50905, 286, 478, 322, 264, 19495, 2535, 13, 50945], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 913, "seek": 408106, "start": 4092.86, "end": 4095.36, "text": " I'm looking for any consonants at positions up to four.", "tokens": [50955, 286, 478, 1237, 337, 604, 30843, 1719, 412, 8432, 493, 281, 1451, 13, 51080], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 914, "seek": 408106, "start": 4096.46, "end": 4104.46, "text": " And then all the nodes get to emit keys and maybe one of the channels could be I am a I am a consonant and I am in a position up to four.", "tokens": [51135, 400, 550, 439, 264, 13891, 483, 281, 32084, 9317, 293, 1310, 472, 295, 264, 9235, 727, 312, 286, 669, 257, 286, 669, 257, 43647, 293, 286, 669, 294, 257, 2535, 493, 281, 1451, 13, 51535], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 915, "seek": 408106, "start": 4105.16, "end": 4108.86, "text": " And that key would have a high number in that specific channel.", "tokens": [51570, 400, 300, 2141, 576, 362, 257, 1090, 1230, 294, 300, 2685, 2269, 13, 51755], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 916, "seek": 408106, "start": 4109.36, "end": 4110.96, "text": " And that's how the query and the key when they", "tokens": [51780, 400, 300, 311, 577, 264, 14581, 293, 264, 2141, 562, 436, 51860], "temperature": 0.0, "avg_logprob": -0.16100962265678073, "compression_ratio": 1.90234375, "no_speech_prob": 6.645249959547073e-05}, {"id": 917, "seek": 411096, "start": 4111.06, "end": 4111.66, "text": " dark product,", "tokens": [50370, 2877, 1674, 11, 50400], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 918, "seek": 411096, "start": 4111.66, "end": 4113.76, "text": " they can find each other and create a high affinity.", "tokens": [50400, 436, 393, 915, 1184, 661, 293, 1884, 257, 1090, 39703, 13, 50505], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 919, "seek": 411096, "start": 4114.76, "end": 4115.86, "text": " And when they have a high affinity,", "tokens": [50555, 400, 562, 436, 362, 257, 1090, 39703, 11, 50610], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 920, "seek": 411096, "start": 4115.86, "end": 4121.06, "text": " like say this token was pretty interesting to to this eighth token.", "tokens": [50610, 411, 584, 341, 14862, 390, 1238, 1880, 281, 281, 341, 19495, 14862, 13, 50870], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 921, "seek": 411096, "start": 4122.36, "end": 4123.56, "text": " When they have a high affinity,", "tokens": [50935, 1133, 436, 362, 257, 1090, 39703, 11, 50995], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 922, "seek": 411096, "start": 4123.86, "end": 4125.06, "text": " then through the softmax,", "tokens": [51010, 550, 807, 264, 2787, 41167, 11, 51070], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 923, "seek": 411096, "start": 4125.26, "end": 4128.66, "text": " I will end up aggregating a lot of its information into my position.", "tokens": [51080, 286, 486, 917, 493, 16743, 990, 257, 688, 295, 1080, 1589, 666, 452, 2535, 13, 51250], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 924, "seek": 411096, "start": 4129.26, "end": 4131.16, "text": " And so I'll get to learn a lot about it.", "tokens": [51280, 400, 370, 286, 603, 483, 281, 1466, 257, 688, 466, 309, 13, 51375], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 925, "seek": 411096, "start": 4132.66, "end": 4137.46, "text": " Now just this was looking at way after this has already happened.", "tokens": [51450, 823, 445, 341, 390, 1237, 412, 636, 934, 341, 575, 1217, 2011, 13, 51690], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 926, "seek": 411096, "start": 4139.36, "end": 4140.86, "text": " Let me erase this operation as well.", "tokens": [51785, 961, 385, 23525, 341, 6916, 382, 731, 13, 51860], "temperature": 0.0, "avg_logprob": -0.14159541659884983, "compression_ratio": 1.764, "no_speech_prob": 0.00010036715684691444}, {"id": 927, "seek": 414096, "start": 4140.96, "end": 4145.76, "text": " So let me erase the masking and the softmax just to show you the under the hood internals and how that works.", "tokens": [50365, 407, 718, 385, 23525, 264, 31226, 293, 264, 2787, 41167, 445, 281, 855, 291, 264, 833, 264, 13376, 2154, 1124, 293, 577, 300, 1985, 13, 50605], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 928, "seek": 414096, "start": 4146.56, "end": 4150.56, "text": " So without the masking and the softmax way comes out like this,", "tokens": [50645, 407, 1553, 264, 31226, 293, 264, 2787, 41167, 636, 1487, 484, 411, 341, 11, 50845], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 929, "seek": 414096, "start": 4150.56, "end": 4150.86, "text": " right?", "tokens": [50845, 558, 30, 50860], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 930, "seek": 414096, "start": 4150.86, "end": 4152.56, "text": " This is the outputs of the dark products.", "tokens": [50860, 639, 307, 264, 23930, 295, 264, 2877, 3383, 13, 50945], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 931, "seek": 414096, "start": 4153.76, "end": 4156.36, "text": " And these are the raw outputs and they take on values from negative,", "tokens": [51005, 400, 613, 366, 264, 8936, 23930, 293, 436, 747, 322, 4190, 490, 3671, 11, 51135], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 932, "seek": 414096, "start": 4156.56, "end": 4156.96, "text": " you know,", "tokens": [51145, 291, 458, 11, 51165], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 933, "seek": 414096, "start": 4157.06, "end": 4158.46, "text": " two to positive two Etc.", "tokens": [51170, 732, 281, 3353, 732, 3790, 66, 13, 51240], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 934, "seek": 414096, "start": 4159.76, "end": 4163.66, "text": " So that's the raw interactions and raw Affinities between all the nodes.", "tokens": [51305, 407, 300, 311, 264, 8936, 13280, 293, 8936, 12840, 259, 1088, 1296, 439, 264, 13891, 13, 51500], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 935, "seek": 414096, "start": 4164.36, "end": 4166.56, "text": " But now if I'm a if I'm a fifth node,", "tokens": [51535, 583, 586, 498, 286, 478, 257, 498, 286, 478, 257, 9266, 9984, 11, 51645], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 936, "seek": 414096, "start": 4166.66, "end": 4170.86, "text": " I will not want to aggregate anything from the sixth node seventh node and the eighth node.", "tokens": [51650, 286, 486, 406, 528, 281, 26118, 1340, 490, 264, 15102, 9984, 17875, 9984, 293, 264, 19495, 9984, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15093686547077878, "compression_ratio": 1.8054607508532423, "no_speech_prob": 0.00026824165252037346}, {"id": 937, "seek": 417096, "start": 4171.36, "end": 4174.46, "text": " So actually we use the upper triangular masking.", "tokens": [50385, 407, 767, 321, 764, 264, 6597, 38190, 31226, 13, 50540], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 938, "seek": 417096, "start": 4174.96, "end": 4176.56, "text": " So those are not allowed to communicate.", "tokens": [50565, 407, 729, 366, 406, 4350, 281, 7890, 13, 50645], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 939, "seek": 417096, "start": 4178.36, "end": 4181.86, "text": " And now we actually want to have a nice distribution.", "tokens": [50735, 400, 586, 321, 767, 528, 281, 362, 257, 1481, 7316, 13, 50910], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 940, "seek": 417096, "start": 4182.46, "end": 4185.66, "text": " So we don't want to aggregate negative point one one of this note.", "tokens": [50940, 407, 321, 500, 380, 528, 281, 26118, 3671, 935, 472, 472, 295, 341, 3637, 13, 51100], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 941, "seek": 417096, "start": 4185.66, "end": 4186.26, "text": " That's crazy.", "tokens": [51100, 663, 311, 3219, 13, 51130], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 942, "seek": 417096, "start": 4186.66, "end": 4188.46, "text": " So instead we exponentiate and normalize.", "tokens": [51150, 407, 2602, 321, 37871, 13024, 293, 2710, 1125, 13, 51240], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 943, "seek": 417096, "start": 4189.06, "end": 4190.86, "text": " And now we get a nice distribution that sums to one.", "tokens": [51270, 400, 586, 321, 483, 257, 1481, 7316, 300, 34499, 281, 472, 13, 51360], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 944, "seek": 417096, "start": 4191.66, "end": 4193.66, "text": " And this is telling us now in the data dependent manner,", "tokens": [51400, 400, 341, 307, 3585, 505, 586, 294, 264, 1412, 12334, 9060, 11, 51500], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 945, "seek": 417096, "start": 4193.66, "end": 4197.56, "text": " how much of information to aggregate from any of these tokens in the past.", "tokens": [51500, 577, 709, 295, 1589, 281, 26118, 490, 604, 295, 613, 22667, 294, 264, 1791, 13, 51695], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 946, "seek": 417096, "start": 4199.56, "end": 4200.86, "text": " So that's way.", "tokens": [51795, 407, 300, 311, 636, 13, 51860], "temperature": 0.0, "avg_logprob": -0.16373634338378906, "compression_ratio": 1.7992277992277992, "no_speech_prob": 0.00019395975687075406}, {"id": 947, "seek": 420096, "start": 4201.26, "end": 4202.26, "text": " And it's not zeros anymore,", "tokens": [50380, 400, 309, 311, 406, 35193, 3602, 11, 50430], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 948, "seek": 420096, "start": 4202.26, "end": 4204.46, "text": " but but it's calculated in this way.", "tokens": [50430, 457, 457, 309, 311, 15598, 294, 341, 636, 13, 50540], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 949, "seek": 420096, "start": 4205.06, "end": 4205.16, "text": " Now,", "tokens": [50570, 823, 11, 50575], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 950, "seek": 420096, "start": 4205.16, "end": 4209.76, "text": " there's one more part to a single self-attention head.", "tokens": [50575, 456, 311, 472, 544, 644, 281, 257, 2167, 2698, 12, 1591, 1251, 1378, 13, 50805], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 951, "seek": 420096, "start": 4210.26, "end": 4211.96, "text": " And that is that when we do the aggregation,", "tokens": [50830, 400, 300, 307, 300, 562, 321, 360, 264, 16743, 399, 11, 50915], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 952, "seek": 420096, "start": 4211.96, "end": 4213.86, "text": " we don't actually aggregate the tokens.", "tokens": [50915, 321, 500, 380, 767, 26118, 264, 22667, 13, 51010], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 953, "seek": 420096, "start": 4213.86, "end": 4214.36, "text": " Exactly.", "tokens": [51010, 7587, 13, 51035], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 954, "seek": 420096, "start": 4214.86, "end": 4215.76, "text": " We aggregate,", "tokens": [51060, 492, 26118, 11, 51105], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 955, "seek": 420096, "start": 4215.76, "end": 4219.16, "text": " we produce one more value here and we call that the value.", "tokens": [51105, 321, 5258, 472, 544, 2158, 510, 293, 321, 818, 300, 264, 2158, 13, 51275], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 956, "seek": 420096, "start": 4221.16, "end": 4223.06, "text": " So in the same way that we produced key and query,", "tokens": [51375, 407, 294, 264, 912, 636, 300, 321, 7126, 2141, 293, 14581, 11, 51470], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 957, "seek": 420096, "start": 4223.06, "end": 4224.86, "text": " we're also going to create a value.", "tokens": [51470, 321, 434, 611, 516, 281, 1884, 257, 2158, 13, 51560], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 958, "seek": 420096, "start": 4226.06, "end": 4230.16, "text": " And then here we don't aggregate.", "tokens": [51620, 400, 550, 510, 321, 500, 380, 26118, 13, 51825], "temperature": 0.0, "avg_logprob": -0.15295745105278202, "compression_ratio": 1.7457627118644068, "no_speech_prob": 5.148327545612119e-05}, {"id": 959, "seek": 423096, "start": 4231.26, "end": 4233.86, "text": " X we calculate a V,", "tokens": [50380, 1783, 321, 8873, 257, 691, 11, 50510], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 960, "seek": 423096, "start": 4233.86, "end": 4238.56, "text": " which is just achieved by propagating this linear on top of X again.", "tokens": [50510, 597, 307, 445, 11042, 538, 12425, 990, 341, 8213, 322, 1192, 295, 1783, 797, 13, 50745], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 961, "seek": 423096, "start": 4239.06, "end": 4242.76, "text": " And then we output way multiplied by V.", "tokens": [50770, 400, 550, 321, 5598, 636, 17207, 538, 691, 13, 50955], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 962, "seek": 423096, "start": 4243.16, "end": 4248.66, "text": " So V is the elements that we aggregate or the vector that we aggregate instead of the raw X.", "tokens": [50975, 407, 691, 307, 264, 4959, 300, 321, 26118, 420, 264, 8062, 300, 321, 26118, 2602, 295, 264, 8936, 1783, 13, 51250], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 963, "seek": 423096, "start": 4249.86, "end": 4250.96, "text": " And now of course,", "tokens": [51310, 400, 586, 295, 1164, 11, 51365], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 964, "seek": 423096, "start": 4250.96, "end": 4256.86, "text": " this will make it so that the output here of the single head will be 16 dimensional because that is the head size.", "tokens": [51365, 341, 486, 652, 309, 370, 300, 264, 5598, 510, 295, 264, 2167, 1378, 486, 312, 3165, 18795, 570, 300, 307, 264, 1378, 2744, 13, 51660], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 965, "seek": 423096, "start": 4258.36, "end": 4260.36, "text": " So you can think of X as kind of like private information.", "tokens": [51735, 407, 291, 393, 519, 295, 1783, 382, 733, 295, 411, 4551, 1589, 13, 51835], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 966, "seek": 423096, "start": 4260.36, "end": 4260.86, "text": " So you can think of X as kind of like private information.", "tokens": [51835, 407, 291, 393, 519, 295, 1783, 382, 733, 295, 411, 4551, 1589, 13, 51860], "temperature": 0.0, "avg_logprob": -0.21148656829585874, "compression_ratio": 1.892, "no_speech_prob": 0.00042310409480705857}, {"id": 967, "seek": 426086, "start": 4260.86, "end": 4261.66, "text": " To this token,", "tokens": [50365, 1407, 341, 14862, 11, 50405], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 968, "seek": 426086, "start": 4261.66, "end": 4263.46, "text": " if you if you think about it that way.", "tokens": [50405, 498, 291, 498, 291, 519, 466, 309, 300, 636, 13, 50495], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 969, "seek": 426086, "start": 4263.66, "end": 4265.66, "text": " So X is kind of private to this token.", "tokens": [50505, 407, 1783, 307, 733, 295, 4551, 281, 341, 14862, 13, 50605], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 970, "seek": 426086, "start": 4265.86, "end": 4272.5599999999995, "text": " So I'm a fifth token at some and I have some identity and my information is kept in vector X.", "tokens": [50615, 407, 286, 478, 257, 9266, 14862, 412, 512, 293, 286, 362, 512, 6575, 293, 452, 1589, 307, 4305, 294, 8062, 1783, 13, 50950], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 971, "seek": 426086, "start": 4273.16, "end": 4275.36, "text": " And now for the purposes of the single head,", "tokens": [50980, 400, 586, 337, 264, 9932, 295, 264, 2167, 1378, 11, 51090], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 972, "seek": 426086, "start": 4275.5599999999995, "end": 4277.0599999999995, "text": " here's what I'm interested in.", "tokens": [51100, 510, 311, 437, 286, 478, 3102, 294, 13, 51175], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 973, "seek": 426086, "start": 4277.46, "end": 4278.96, "text": " Here's what I have.", "tokens": [51195, 1692, 311, 437, 286, 362, 13, 51270], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 974, "seek": 426086, "start": 4279.5599999999995, "end": 4281.259999999999, "text": " And if you find me interesting,", "tokens": [51300, 400, 498, 291, 915, 385, 1880, 11, 51385], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 975, "seek": 426086, "start": 4281.259999999999, "end": 4282.759999999999, "text": " here's what I will communicate to you.", "tokens": [51385, 510, 311, 437, 286, 486, 7890, 281, 291, 13, 51460], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 976, "seek": 426086, "start": 4283.16, "end": 4284.46, "text": " And that's stored in V.", "tokens": [51480, 400, 300, 311, 12187, 294, 691, 13, 51545], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 977, "seek": 426086, "start": 4285.0599999999995, "end": 4290.66, "text": " And so V is the thing that gets aggregated for the purposes of this single head between the different nodes.", "tokens": [51575, 400, 370, 691, 307, 264, 551, 300, 2170, 16743, 770, 337, 264, 9932, 295, 341, 2167, 1378, 1296, 264, 819, 13891, 13, 51855], "temperature": 0.0, "avg_logprob": -0.13189274331797723, "compression_ratio": 1.8339622641509434, "no_speech_prob": 0.0002376795600866899}, {"id": 978, "seek": 429086, "start": 4291.66, "end": 4295.259999999999, "text": " And that's basically the self-attention mechanism.", "tokens": [50405, 400, 300, 311, 1936, 264, 2698, 12, 1591, 1251, 7513, 13, 50585], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 979, "seek": 429086, "start": 4295.259999999999, "end": 4297.36, "text": " This is this is what it does.", "tokens": [50585, 639, 307, 341, 307, 437, 309, 775, 13, 50690], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 980, "seek": 429086, "start": 4298.16, "end": 4301.259999999999, "text": " There are a few notes that I would make like to make about attention.", "tokens": [50730, 821, 366, 257, 1326, 5570, 300, 286, 576, 652, 411, 281, 652, 466, 3202, 13, 50885], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 981, "seek": 429086, "start": 4301.759999999999, "end": 4304.86, "text": " Number one attention is a communication mechanism.", "tokens": [50910, 5118, 472, 3202, 307, 257, 6101, 7513, 13, 51065], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 982, "seek": 429086, "start": 4305.259999999999, "end": 4307.66, "text": " You can really think about it as a communication mechanism", "tokens": [51085, 509, 393, 534, 519, 466, 309, 382, 257, 6101, 7513, 51205], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 983, "seek": 429086, "start": 4307.96, "end": 4310.5599999999995, "text": " where you have a number of nodes in a directed graph", "tokens": [51220, 689, 291, 362, 257, 1230, 295, 13891, 294, 257, 12898, 4295, 51350], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 984, "seek": 429086, "start": 4310.96, "end": 4313.5599999999995, "text": " where basically you have edges pointing between nodes like this.", "tokens": [51370, 689, 1936, 291, 362, 8819, 12166, 1296, 13891, 411, 341, 13, 51500], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 985, "seek": 429086, "start": 4314.759999999999, "end": 4318.0599999999995, "text": " And what happens is every node has some vector of information", "tokens": [51560, 400, 437, 2314, 307, 633, 9984, 575, 512, 8062, 295, 1589, 51725], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 986, "seek": 429086, "start": 4318.46, "end": 4320.259999999999, "text": " and it gets to aggregate information", "tokens": [51745, 293, 309, 2170, 281, 26118, 1589, 51835], "temperature": 0.0, "avg_logprob": -0.13906068200463648, "compression_ratio": 1.8560311284046693, "no_speech_prob": 0.000868596660438925}, {"id": 987, "seek": 432026, "start": 4320.26, "end": 4323.76, "text": " via a weighted sum from all of the nodes that point to it.", "tokens": [50365, 5766, 257, 32807, 2408, 490, 439, 295, 264, 13891, 300, 935, 281, 309, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 988, "seek": 432026, "start": 4324.76, "end": 4326.56, "text": " And this is done in a data dependent manner.", "tokens": [50590, 400, 341, 307, 1096, 294, 257, 1412, 12334, 9060, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 989, "seek": 432026, "start": 4326.56, "end": 4330.26, "text": " So depending on whatever data is actually stored at each node at any point in time.", "tokens": [50680, 407, 5413, 322, 2035, 1412, 307, 767, 12187, 412, 1184, 9984, 412, 604, 935, 294, 565, 13, 50865], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 990, "seek": 432026, "start": 4331.16, "end": 4331.76, "text": " Now,", "tokens": [50910, 823, 11, 50940], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 991, "seek": 432026, "start": 4332.66, "end": 4333.96, "text": " our graph doesn't look like this.", "tokens": [50985, 527, 4295, 1177, 380, 574, 411, 341, 13, 51050], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 992, "seek": 432026, "start": 4333.96, "end": 4335.46, "text": " Our graph has a different structure.", "tokens": [51050, 2621, 4295, 575, 257, 819, 3877, 13, 51125], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 993, "seek": 432026, "start": 4335.76, "end": 4340.26, "text": " We have eight nodes because the block size is eight and there's always eight tokens.", "tokens": [51140, 492, 362, 3180, 13891, 570, 264, 3461, 2744, 307, 3180, 293, 456, 311, 1009, 3180, 22667, 13, 51365], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 994, "seek": 432026, "start": 4341.16, "end": 4344.26, "text": " And the first node is only pointed to by itself.", "tokens": [51410, 400, 264, 700, 9984, 307, 787, 10932, 281, 538, 2564, 13, 51565], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 995, "seek": 432026, "start": 4344.66, "end": 4347.56, "text": " The second node is pointed to by the first node and itself", "tokens": [51585, 440, 1150, 9984, 307, 10932, 281, 538, 264, 700, 9984, 293, 2564, 51730], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 996, "seek": 432026, "start": 4347.860000000001, "end": 4349.56, "text": " all the way up to the eighth node,", "tokens": [51745, 439, 264, 636, 493, 281, 264, 19495, 9984, 11, 51830], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 997, "seek": 432026, "start": 4349.66, "end": 4350.06, "text": " which is pointed to by itself.", "tokens": [51835, 597, 307, 10932, 281, 538, 2564, 13, 51855], "temperature": 0.0, "avg_logprob": -0.1427961570629175, "compression_ratio": 1.831578947368421, "no_speech_prob": 0.0002835015766322613}, {"id": 998, "seek": 435006, "start": 4350.06, "end": 4352.860000000001, "text": " Pointed to by all the previous nodes and itself.", "tokens": [50365, 12387, 292, 281, 538, 439, 264, 3894, 13891, 293, 2564, 13, 50505], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 999, "seek": 435006, "start": 4353.860000000001, "end": 4357.860000000001, "text": " And so that's the structure that are directed graph has or happens happens to have", "tokens": [50555, 400, 370, 300, 311, 264, 3877, 300, 366, 12898, 4295, 575, 420, 2314, 2314, 281, 362, 50755], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1000, "seek": 435006, "start": 4357.860000000001, "end": 4360.56, "text": " an autoregressive sort of scenario like language modeling.", "tokens": [50755, 364, 1476, 418, 3091, 488, 1333, 295, 9005, 411, 2856, 15983, 13, 50890], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1001, "seek": 435006, "start": 4361.26, "end": 4364.360000000001, "text": " But in principle attention can be applied to any arbitrary directed graph", "tokens": [50925, 583, 294, 8665, 3202, 393, 312, 6456, 281, 604, 23211, 12898, 4295, 51080], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1002, "seek": 435006, "start": 4364.360000000001, "end": 4366.56, "text": " and it's just a communication mechanism between the nodes.", "tokens": [51080, 293, 309, 311, 445, 257, 6101, 7513, 1296, 264, 13891, 13, 51190], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1003, "seek": 435006, "start": 4367.26, "end": 4370.46, "text": " The second note is that notice that there is no notion of space.", "tokens": [51225, 440, 1150, 3637, 307, 300, 3449, 300, 456, 307, 572, 10710, 295, 1901, 13, 51385], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1004, "seek": 435006, "start": 4370.76, "end": 4375.06, "text": " So attention simply acts over like a set of vectors in this graph.", "tokens": [51400, 407, 3202, 2935, 10672, 670, 411, 257, 992, 295, 18875, 294, 341, 4295, 13, 51615], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1005, "seek": 435006, "start": 4375.46, "end": 4379.160000000001, "text": " And so by default these nodes have no idea where they are positioned in the space.", "tokens": [51635, 400, 370, 538, 7576, 613, 13891, 362, 572, 1558, 689, 436, 366, 24889, 294, 264, 1901, 13, 51820], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1006, "seek": 435006, "start": 4379.360000000001, "end": 4379.96, "text": " And that's why we need to", "tokens": [51830, 400, 300, 311, 983, 321, 643, 281, 51860], "temperature": 0.0, "avg_logprob": -0.15705573289914238, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00034148371196351945}, {"id": 1007, "seek": 438006, "start": 4380.160000000001, "end": 4384.860000000001, "text": " encode them positionally and sort of give them some information that is anchors to a specific", "tokens": [50370, 2058, 1429, 552, 2535, 379, 293, 1333, 295, 976, 552, 512, 1589, 300, 307, 12723, 830, 281, 257, 2685, 50605], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1008, "seek": 438006, "start": 4384.860000000001, "end": 4388.26, "text": " position so that they sort of know where they are.", "tokens": [50605, 2535, 370, 300, 436, 1333, 295, 458, 689, 436, 366, 13, 50775], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1009, "seek": 438006, "start": 4388.660000000001, "end": 4392.160000000001, "text": " And this is different than for example from convolution because if you run for example,", "tokens": [50795, 400, 341, 307, 819, 813, 337, 1365, 490, 45216, 570, 498, 291, 1190, 337, 1365, 11, 50970], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1010, "seek": 438006, "start": 4392.160000000001, "end": 4397.46, "text": " a convolution operation over some input there is a very specific sort of layout of the information", "tokens": [50970, 257, 45216, 6916, 670, 512, 4846, 456, 307, 257, 588, 2685, 1333, 295, 13333, 295, 264, 1589, 51235], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1011, "seek": 438006, "start": 4397.46, "end": 4401.160000000001, "text": " in space and the convolutional filters sort of act in space.", "tokens": [51235, 294, 1901, 293, 264, 45216, 304, 15995, 1333, 295, 605, 294, 1901, 13, 51420], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1012, "seek": 438006, "start": 4401.46, "end": 4407.360000000001, "text": " And so it's it's not like an attention and attention is just a set of vectors out there in space.", "tokens": [51435, 400, 370, 309, 311, 309, 311, 406, 411, 364, 3202, 293, 3202, 307, 445, 257, 992, 295, 18875, 484, 456, 294, 1901, 13, 51730], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1013, "seek": 438006, "start": 4407.660000000001, "end": 4409.860000000001, "text": " They communicate and if you want them to have a", "tokens": [51745, 814, 7890, 293, 498, 291, 528, 552, 281, 362, 257, 51855], "temperature": 0.0, "avg_logprob": -0.15715171813964843, "compression_ratio": 1.9635036496350364, "no_speech_prob": 0.0005750881391577423}, {"id": 1014, "seek": 440986, "start": 4409.86, "end": 4414.66, "text": " notion of space you need to specifically add it which is what we've done when we calculated the", "tokens": [50365, 10710, 295, 1901, 291, 643, 281, 4682, 909, 309, 597, 307, 437, 321, 600, 1096, 562, 321, 15598, 264, 50605], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1015, "seek": 440986, "start": 4415.46, "end": 4420.16, "text": " relative the positional encode encodings and added that information to the vectors.", "tokens": [50645, 4972, 264, 2535, 304, 2058, 1429, 2058, 378, 1109, 293, 3869, 300, 1589, 281, 264, 18875, 13, 50880], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1016, "seek": 440986, "start": 4420.36, "end": 4424.86, "text": " The next thing that I hope is very clear is that the elements across the batch dimension which are", "tokens": [50890, 440, 958, 551, 300, 286, 1454, 307, 588, 1850, 307, 300, 264, 4959, 2108, 264, 15245, 10139, 597, 366, 51115], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1017, "seek": 440986, "start": 4424.86, "end": 4426.86, "text": " independent examples never talk to each other.", "tokens": [51115, 6695, 5110, 1128, 751, 281, 1184, 661, 13, 51215], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1018, "seek": 440986, "start": 4426.86, "end": 4431.46, "text": " They're always processed independently and this is a batch matrix multiply that applies basically a", "tokens": [51215, 814, 434, 1009, 18846, 21761, 293, 341, 307, 257, 15245, 8141, 12972, 300, 13165, 1936, 257, 51445], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1019, "seek": 440986, "start": 4431.46, "end": 4434.86, "text": " matrix multiplication kind of in parallel across the batch dimension.", "tokens": [51445, 8141, 27290, 733, 295, 294, 8952, 2108, 264, 15245, 10139, 13, 51615], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1020, "seek": 440986, "start": 4435.36, "end": 4439.259999999999, "text": " So maybe it would be more accurate to say that in this analogy of a directed graph.", "tokens": [51640, 407, 1310, 309, 576, 312, 544, 8559, 281, 584, 300, 294, 341, 21663, 295, 257, 12898, 4295, 13, 51835], "temperature": 0.0, "avg_logprob": -0.12894211745843653, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.00020663384930230677}, {"id": 1021, "seek": 443986, "start": 4439.86, "end": 4445.0599999999995, "text": " We really have because the batch size is for we really have four separate pools of eight", "tokens": [50365, 492, 534, 362, 570, 264, 15245, 2744, 307, 337, 321, 534, 362, 1451, 4994, 28688, 295, 3180, 50625], "temperature": 0.0, "avg_logprob": -0.15876005435812063, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00038806599332019687}, {"id": 1022, "seek": 443986, "start": 4445.0599999999995, "end": 4449.259999999999, "text": " nodes and those eight nodes only talk to each other but in total there's like 32 nodes that are being", "tokens": [50625, 13891, 293, 729, 3180, 13891, 787, 751, 281, 1184, 661, 457, 294, 3217, 456, 311, 411, 8858, 13891, 300, 366, 885, 50835], "temperature": 0.0, "avg_logprob": -0.15876005435812063, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00038806599332019687}, {"id": 1023, "seek": 443986, "start": 4449.259999999999, "end": 4454.66, "text": " processed but there's sort of four separate pools of eight you can look at it that way.", "tokens": [50835, 18846, 457, 456, 311, 1333, 295, 1451, 4994, 28688, 295, 3180, 291, 393, 574, 412, 309, 300, 636, 13, 51105], "temperature": 0.0, "avg_logprob": -0.15876005435812063, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00038806599332019687}, {"id": 1024, "seek": 443986, "start": 4455.46, "end": 4461.86, "text": " The next note is that here in the case of language modeling we have this specific structure of", "tokens": [51145, 440, 958, 3637, 307, 300, 510, 294, 264, 1389, 295, 2856, 15983, 321, 362, 341, 2685, 3877, 295, 51465], "temperature": 0.0, "avg_logprob": -0.15876005435812063, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00038806599332019687}, {"id": 1025, "seek": 443986, "start": 4461.86, "end": 4467.86, "text": " directed graph where the future tokens will not communicate to the past tokens but this doesn't", "tokens": [51465, 12898, 4295, 689, 264, 2027, 22667, 486, 406, 7890, 281, 264, 1791, 22667, 457, 341, 1177, 380, 51765], "temperature": 0.0, "avg_logprob": -0.15876005435812063, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00038806599332019687}, {"id": 1026, "seek": 443986, "start": 4467.86, "end": 4469.759999999999, "text": " necessarily have to be the constraint in the general case.", "tokens": [51765, 4725, 362, 281, 312, 264, 25534, 294, 264, 2674, 1389, 13, 51860], "temperature": 0.0, "avg_logprob": -0.15876005435812063, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.00038806599332019687}, {"id": 1027, "seek": 446986, "start": 4470.46, "end": 4475.96, "text": " And in fact in many cases you may want to have all of the notes talk to each other fully.", "tokens": [50395, 400, 294, 1186, 294, 867, 3331, 291, 815, 528, 281, 362, 439, 295, 264, 5570, 751, 281, 1184, 661, 4498, 13, 50670], "temperature": 0.0, "avg_logprob": -0.11714357799953884, "compression_ratio": 1.9660377358490566, "no_speech_prob": 0.0002315691381227225}, {"id": 1028, "seek": 446986, "start": 4476.5599999999995, "end": 4480.66, "text": " So as an example if you're doing sentiment analysis or something like that with a transformer you might", "tokens": [50700, 407, 382, 364, 1365, 498, 291, 434, 884, 16149, 5215, 420, 746, 411, 300, 365, 257, 31782, 291, 1062, 50905], "temperature": 0.0, "avg_logprob": -0.11714357799953884, "compression_ratio": 1.9660377358490566, "no_speech_prob": 0.0002315691381227225}, {"id": 1029, "seek": 446986, "start": 4480.66, "end": 4486.0599999999995, "text": " have a number of tokens and you may want to have them all talk to each other fully because later you", "tokens": [50905, 362, 257, 1230, 295, 22667, 293, 291, 815, 528, 281, 362, 552, 439, 751, 281, 1184, 661, 4498, 570, 1780, 291, 51175], "temperature": 0.0, "avg_logprob": -0.11714357799953884, "compression_ratio": 1.9660377358490566, "no_speech_prob": 0.0002315691381227225}, {"id": 1030, "seek": 446986, "start": 4486.0599999999995, "end": 4490.86, "text": " are predicting for example the sentiment of the sentence and so it's okay for these notes to talk to", "tokens": [51175, 366, 32884, 337, 1365, 264, 16149, 295, 264, 8174, 293, 370, 309, 311, 1392, 337, 613, 5570, 281, 751, 281, 51415], "temperature": 0.0, "avg_logprob": -0.11714357799953884, "compression_ratio": 1.9660377358490566, "no_speech_prob": 0.0002315691381227225}, {"id": 1031, "seek": 446986, "start": 4490.86, "end": 4498.0599999999995, "text": " each other and so in those cases you will use an encoder block of self-attention and all it means", "tokens": [51415, 1184, 661, 293, 370, 294, 729, 3331, 291, 486, 764, 364, 2058, 19866, 3461, 295, 2698, 12, 1591, 1251, 293, 439, 309, 1355, 51775], "temperature": 0.0, "avg_logprob": -0.11714357799953884, "compression_ratio": 1.9660377358490566, "no_speech_prob": 0.0002315691381227225}, {"id": 1032, "seek": 446986, "start": 4498.0599999999995, "end": 4499.259999999999, "text": " that it's an encoder block.", "tokens": [51775, 300, 309, 311, 364, 2058, 19866, 3461, 13, 51835], "temperature": 0.0, "avg_logprob": -0.11714357799953884, "compression_ratio": 1.9660377358490566, "no_speech_prob": 0.0002315691381227225}, {"id": 1033, "seek": 449926, "start": 4499.360000000001, "end": 4503.96, "text": " Is that you will delete this line of code allowing all the notes to completely talk to each other.", "tokens": [50370, 1119, 300, 291, 486, 12097, 341, 1622, 295, 3089, 8293, 439, 264, 5570, 281, 2584, 751, 281, 1184, 661, 13, 50600], "temperature": 0.0, "avg_logprob": -0.17392594019571941, "compression_ratio": 1.9185185185185185, "no_speech_prob": 0.00032536551589146256}, {"id": 1034, "seek": 449926, "start": 4504.56, "end": 4510.66, "text": " What we're implementing here is sometimes called a decoder block and it's called a decoder because", "tokens": [50630, 708, 321, 434, 18114, 510, 307, 2171, 1219, 257, 979, 19866, 3461, 293, 309, 311, 1219, 257, 979, 19866, 570, 50935], "temperature": 0.0, "avg_logprob": -0.17392594019571941, "compression_ratio": 1.9185185185185185, "no_speech_prob": 0.00032536551589146256}, {"id": 1035, "seek": 449926, "start": 4510.66, "end": 4517.46, "text": " it is sort of like decoding language and it's got this autoregressive format where you have to mask", "tokens": [50935, 309, 307, 1333, 295, 411, 979, 8616, 2856, 293, 309, 311, 658, 341, 1476, 418, 3091, 488, 7877, 689, 291, 362, 281, 6094, 51275], "temperature": 0.0, "avg_logprob": -0.17392594019571941, "compression_ratio": 1.9185185185185185, "no_speech_prob": 0.00032536551589146256}, {"id": 1036, "seek": 449926, "start": 4517.46, "end": 4523.76, "text": " with the triangular matrix so that notes from the future never talk to the past because they would", "tokens": [51275, 365, 264, 38190, 8141, 370, 300, 5570, 490, 264, 2027, 1128, 751, 281, 264, 1791, 570, 436, 576, 51590], "temperature": 0.0, "avg_logprob": -0.17392594019571941, "compression_ratio": 1.9185185185185185, "no_speech_prob": 0.00032536551589146256}, {"id": 1037, "seek": 449926, "start": 4523.76, "end": 4524.56, "text": " give away the answer.", "tokens": [51590, 976, 1314, 264, 1867, 13, 51630], "temperature": 0.0, "avg_logprob": -0.17392594019571941, "compression_ratio": 1.9185185185185185, "no_speech_prob": 0.00032536551589146256}, {"id": 1038, "seek": 449926, "start": 4525.360000000001, "end": 4529.16, "text": " And so basically in encoder blocks you would delete this allow all the notes to talk to each other.", "tokens": [51670, 400, 370, 1936, 294, 2058, 19866, 8474, 291, 576, 12097, 341, 2089, 439, 264, 5570, 281, 751, 281, 1184, 661, 13, 51860], "temperature": 0.0, "avg_logprob": -0.17392594019571941, "compression_ratio": 1.9185185185185185, "no_speech_prob": 0.00032536551589146256}, {"id": 1039, "seek": 452926, "start": 4529.860000000001, "end": 4535.360000000001, "text": " In decoder blocks this will always be present so that you have this triangular structure but both are", "tokens": [50395, 682, 979, 19866, 8474, 341, 486, 1009, 312, 1974, 370, 300, 291, 362, 341, 38190, 3877, 457, 1293, 366, 50670], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1040, "seek": 452926, "start": 4535.360000000001, "end": 4536.860000000001, "text": " allowed and attention doesn't care.", "tokens": [50670, 4350, 293, 3202, 1177, 380, 1127, 13, 50745], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1041, "seek": 452926, "start": 4536.860000000001, "end": 4539.26, "text": " Attention supports arbitrary connectivity between notes.", "tokens": [50745, 31858, 9346, 23211, 21095, 1296, 5570, 13, 50865], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1042, "seek": 452926, "start": 4539.860000000001, "end": 4544.46, "text": " The next thing I wanted to comment on is you keep me you keep hearing me say attention self-attention", "tokens": [50895, 440, 958, 551, 286, 1415, 281, 2871, 322, 307, 291, 1066, 385, 291, 1066, 4763, 385, 584, 3202, 2698, 12, 1591, 1251, 51125], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1043, "seek": 452926, "start": 4544.46, "end": 4545.06, "text": " etc.", "tokens": [51125, 5183, 13, 51155], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1044, "seek": 452926, "start": 4545.06, "end": 4546.96, "text": " There's actually also something called cross attention.", "tokens": [51155, 821, 311, 767, 611, 746, 1219, 3278, 3202, 13, 51250], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1045, "seek": 452926, "start": 4546.96, "end": 4547.76, "text": " What is the difference?", "tokens": [51250, 708, 307, 264, 2649, 30, 51290], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1046, "seek": 452926, "start": 4548.76, "end": 4558.16, "text": " So basically the reason this attention is self-attention is because the keys queries and the values are all coming", "tokens": [51340, 407, 1936, 264, 1778, 341, 3202, 307, 2698, 12, 1591, 1251, 307, 570, 264, 9317, 24109, 293, 264, 4190, 366, 439, 1348, 51810], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1047, "seek": 452926, "start": 4558.16, "end": 4559.16, "text": " from the same source.", "tokens": [51810, 490, 264, 912, 4009, 13, 51860], "temperature": 0.0, "avg_logprob": -0.14570456284743089, "compression_ratio": 1.75, "no_speech_prob": 0.00048470438923686743}, {"id": 1048, "seek": 455926, "start": 4559.26, "end": 4563.860000000001, "text": " From X so the same source X produces keys queries and values.", "tokens": [50365, 3358, 1783, 370, 264, 912, 4009, 1783, 14725, 9317, 24109, 293, 4190, 13, 50595], "temperature": 0.0, "avg_logprob": -0.14135315936544668, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.0001331845996901393}, {"id": 1049, "seek": 455926, "start": 4563.860000000001, "end": 4569.56, "text": " So these nodes are self-attending but in principle attention is much more general than that.", "tokens": [50595, 407, 613, 13891, 366, 2698, 12, 1591, 2029, 457, 294, 8665, 3202, 307, 709, 544, 2674, 813, 300, 13, 50880], "temperature": 0.0, "avg_logprob": -0.14135315936544668, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.0001331845996901393}, {"id": 1050, "seek": 455926, "start": 4569.56, "end": 4575.66, "text": " So for example in encoder decoder transformers you can have a case where the queries are produced from", "tokens": [50880, 407, 337, 1365, 294, 2058, 19866, 979, 19866, 4088, 433, 291, 393, 362, 257, 1389, 689, 264, 24109, 366, 7126, 490, 51185], "temperature": 0.0, "avg_logprob": -0.14135315936544668, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.0001331845996901393}, {"id": 1051, "seek": 455926, "start": 4575.66, "end": 4582.26, "text": " X but the keys and the values come from a whole separate external source and sometimes from encoder blocks", "tokens": [51185, 1783, 457, 264, 9317, 293, 264, 4190, 808, 490, 257, 1379, 4994, 8320, 4009, 293, 2171, 490, 2058, 19866, 8474, 51515], "temperature": 0.0, "avg_logprob": -0.14135315936544668, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.0001331845996901393}, {"id": 1052, "seek": 455926, "start": 4582.26, "end": 4587.16, "text": " that encode some context that we'd like to condition on and so the keys and the values will actually come", "tokens": [51515, 300, 2058, 1429, 512, 4319, 300, 321, 1116, 411, 281, 4188, 322, 293, 370, 264, 9317, 293, 264, 4190, 486, 767, 808, 51760], "temperature": 0.0, "avg_logprob": -0.14135315936544668, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.0001331845996901393}, {"id": 1053, "seek": 455926, "start": 4587.16, "end": 4588.66, "text": " from a whole separate source.", "tokens": [51760, 490, 257, 1379, 4994, 4009, 13, 51835], "temperature": 0.0, "avg_logprob": -0.14135315936544668, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.0001331845996901393}, {"id": 1054, "seek": 458866, "start": 4588.66, "end": 4590.96, "text": " Those are nodes on the side and here.", "tokens": [50365, 3950, 366, 13891, 322, 264, 1252, 293, 510, 13, 50480], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1055, "seek": 458866, "start": 4590.96, "end": 4594.26, "text": " We're just producing queries and we're reading off information from the side.", "tokens": [50480, 492, 434, 445, 10501, 24109, 293, 321, 434, 3760, 766, 1589, 490, 264, 1252, 13, 50645], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1056, "seek": 458866, "start": 4594.96, "end": 4599.96, "text": " So cross attention is used when there's a separate source of nodes.", "tokens": [50680, 407, 3278, 3202, 307, 1143, 562, 456, 311, 257, 4994, 4009, 295, 13891, 13, 50930], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1057, "seek": 458866, "start": 4600.16, "end": 4604.5599999999995, "text": " We'd like to pull information from into our notes and it's self-attention.", "tokens": [50940, 492, 1116, 411, 281, 2235, 1589, 490, 666, 527, 5570, 293, 309, 311, 2698, 12, 1591, 1251, 13, 51160], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1058, "seek": 458866, "start": 4604.5599999999995, "end": 4607.26, "text": " If we just have nodes that would like to look at each other and talk to each other.", "tokens": [51160, 759, 321, 445, 362, 13891, 300, 576, 411, 281, 574, 412, 1184, 661, 293, 751, 281, 1184, 661, 13, 51295], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1059, "seek": 458866, "start": 4608.0599999999995, "end": 4610.96, "text": " So this attention here happens to be self-attention.", "tokens": [51335, 407, 341, 3202, 510, 2314, 281, 312, 2698, 12, 1591, 1251, 13, 51480], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1060, "seek": 458866, "start": 4612.5599999999995, "end": 4616.16, "text": " But in principle attention is a lot more general.", "tokens": [51560, 583, 294, 8665, 3202, 307, 257, 688, 544, 2674, 13, 51740], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1061, "seek": 458866, "start": 4616.66, "end": 4618.46, "text": " Okay in the last note at this stage is", "tokens": [51765, 1033, 294, 264, 1036, 3637, 412, 341, 3233, 307, 51855], "temperature": 0.0, "avg_logprob": -0.13467757682490156, "compression_ratio": 1.8544061302681993, "no_speech_prob": 8.969655027613044e-05}, {"id": 1062, "seek": 461866, "start": 4618.66, "end": 4620.96, "text": " if we come to the attention is all you need paper here.", "tokens": [50365, 498, 321, 808, 281, 264, 3202, 307, 439, 291, 643, 3035, 510, 13, 50480], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1063, "seek": 461866, "start": 4620.96, "end": 4622.76, "text": " We've already implemented attention.", "tokens": [50480, 492, 600, 1217, 12270, 3202, 13, 50570], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1064, "seek": 461866, "start": 4622.76, "end": 4627.16, "text": " So given query key and value we've multiplied the query on the key.", "tokens": [50570, 407, 2212, 14581, 2141, 293, 2158, 321, 600, 17207, 264, 14581, 322, 264, 2141, 13, 50790], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1065, "seek": 461866, "start": 4627.16, "end": 4630.86, "text": " We've soft maxed it and then we are aggregating the values.", "tokens": [50790, 492, 600, 2787, 11469, 292, 309, 293, 550, 321, 366, 16743, 990, 264, 4190, 13, 50975], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1066, "seek": 461866, "start": 4630.86, "end": 4636.26, "text": " There's one more thing that we're missing here, which is the dividing by 1 over square root of the head size.", "tokens": [50975, 821, 311, 472, 544, 551, 300, 321, 434, 5361, 510, 11, 597, 307, 264, 26764, 538, 502, 670, 3732, 5593, 295, 264, 1378, 2744, 13, 51245], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1067, "seek": 461866, "start": 4636.26, "end": 4637.86, "text": " The DK here is the head size.", "tokens": [51245, 440, 31934, 510, 307, 264, 1378, 2744, 13, 51325], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1068, "seek": 461866, "start": 4637.86, "end": 4639.26, "text": " Why are they doing this?", "tokens": [51325, 1545, 366, 436, 884, 341, 30, 51395], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1069, "seek": 461866, "start": 4639.26, "end": 4640.0599999999995, "text": " Why is this important?", "tokens": [51395, 1545, 307, 341, 1021, 30, 51435], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1070, "seek": 461866, "start": 4640.0599999999995, "end": 4646.5599999999995, "text": " So they call it a scaled attention and it's kind of like an important normalization to basically have.", "tokens": [51435, 407, 436, 818, 309, 257, 36039, 3202, 293, 309, 311, 733, 295, 411, 364, 1021, 2710, 2144, 281, 1936, 362, 13, 51760], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1071, "seek": 461866, "start": 4646.5599999999995, "end": 4648.5599999999995, "text": " The problem is.", "tokens": [51760, 440, 1154, 307, 13, 51860], "temperature": 0.0, "avg_logprob": -0.21572804622512928, "compression_ratio": 1.7392739273927393, "no_speech_prob": 0.0002522571594454348}, {"id": 1072, "seek": 464866, "start": 4648.66, "end": 4653.76, "text": " If you have unit Gaussian inputs, so 0 mean unit variance, K and Q are unit Gaussian.", "tokens": [50365, 759, 291, 362, 4985, 39148, 15743, 11, 370, 1958, 914, 4985, 21977, 11, 591, 293, 1249, 366, 4985, 39148, 13, 50620], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1073, "seek": 464866, "start": 4653.76, "end": 4660.16, "text": " And if you just do way naively, then you see that your way actually will be the variance will be on the order of head size,", "tokens": [50620, 400, 498, 291, 445, 360, 636, 1667, 3413, 11, 550, 291, 536, 300, 428, 636, 767, 486, 312, 264, 21977, 486, 312, 322, 264, 1668, 295, 1378, 2744, 11, 50940], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1074, "seek": 464866, "start": 4660.16, "end": 4661.36, "text": " which in our case is 16.", "tokens": [50940, 597, 294, 527, 1389, 307, 3165, 13, 51000], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1075, "seek": 464866, "start": 4662.46, "end": 4665.46, "text": " But if you multiply by 1 over head size square root,", "tokens": [51055, 583, 498, 291, 12972, 538, 502, 670, 1378, 2744, 3732, 5593, 11, 51205], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1076, "seek": 464866, "start": 4665.46, "end": 4670.66, "text": " so this is square root and this is 1 over then the variance of way will be 1.", "tokens": [51205, 370, 341, 307, 3732, 5593, 293, 341, 307, 502, 670, 550, 264, 21977, 295, 636, 486, 312, 502, 13, 51465], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1077, "seek": 464866, "start": 4670.66, "end": 4671.66, "text": " So it will be preserved.", "tokens": [51465, 407, 309, 486, 312, 22242, 13, 51515], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1078, "seek": 464866, "start": 4672.96, "end": 4674.16, "text": " Now, why is this important?", "tokens": [51580, 823, 11, 983, 307, 341, 1021, 30, 51640], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1079, "seek": 464866, "start": 4674.5599999999995, "end": 4678.66, "text": " You'll notice that way here will feed into softmax.", "tokens": [51660, 509, 603, 3449, 300, 636, 510, 486, 3154, 666, 2787, 41167, 13, 51865], "temperature": 0.0, "avg_logprob": -0.18267607873724412, "compression_ratio": 1.7537313432835822, "no_speech_prob": 0.0003983576316386461}, {"id": 1080, "seek": 467866, "start": 4679.66, "end": 4684.66, "text": " And so it's really important, especially at initialization that way be fairly diffuse.", "tokens": [50415, 400, 370, 309, 311, 534, 1021, 11, 2318, 412, 5883, 2144, 300, 636, 312, 6457, 42165, 13, 50665], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1081, "seek": 467866, "start": 4684.66, "end": 4691.66, "text": " So in our case here, we sort of lucked out here and way had a fairly diffuse numbers here.", "tokens": [50665, 407, 294, 527, 1389, 510, 11, 321, 1333, 295, 3668, 292, 484, 510, 293, 636, 632, 257, 6457, 42165, 3547, 510, 13, 51015], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1082, "seek": 467866, "start": 4691.66, "end": 4694.46, "text": " So like this.", "tokens": [51015, 407, 411, 341, 13, 51155], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1083, "seek": 467866, "start": 4694.46, "end": 4696.46, "text": " Now, the problem is that because of softmax,", "tokens": [51155, 823, 11, 264, 1154, 307, 300, 570, 295, 2787, 41167, 11, 51255], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1084, "seek": 467866, "start": 4696.46, "end": 4700.16, "text": " if weight takes on very positive and very negative numbers inside it,", "tokens": [51255, 498, 3364, 2516, 322, 588, 3353, 293, 588, 3671, 3547, 1854, 309, 11, 51440], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1085, "seek": 467866, "start": 4700.16, "end": 4704.36, "text": " softmax will actually converge towards one hot vectors.", "tokens": [51440, 2787, 41167, 486, 767, 41881, 3030, 472, 2368, 18875, 13, 51650], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1086, "seek": 467866, "start": 4704.36, "end": 4705.76, "text": " And so I can illustrate that here.", "tokens": [51650, 400, 370, 286, 393, 23221, 300, 510, 13, 51720], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1087, "seek": 467866, "start": 4708.0599999999995, "end": 4708.36, "text": " Say,", "tokens": [51835, 6463, 11, 51850], "temperature": 0.0, "avg_logprob": -0.19732506161644345, "compression_ratio": 1.6408163265306122, "no_speech_prob": 4.7124132834142074e-05}, {"id": 1088, "seek": 470836, "start": 4708.36, "end": 4712.0599999999995, "text": " we are applying softmax to a tensor of values that are very close to zero.", "tokens": [50365, 321, 366, 9275, 2787, 41167, 281, 257, 40863, 295, 4190, 300, 366, 588, 1998, 281, 4018, 13, 50550], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1089, "seek": 470836, "start": 4712.36, "end": 4714.66, "text": " Then we're going to get a diffuse thing out of softmax.", "tokens": [50565, 1396, 321, 434, 516, 281, 483, 257, 42165, 551, 484, 295, 2787, 41167, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1090, "seek": 470836, "start": 4715.5599999999995, "end": 4718.36, "text": " But the moment I take the exact same thing and I start sharpening it,", "tokens": [50725, 583, 264, 1623, 286, 747, 264, 1900, 912, 551, 293, 286, 722, 8199, 4559, 309, 11, 50865], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1091, "seek": 470836, "start": 4718.36, "end": 4720.759999999999, "text": " making it bigger by multiplying these numbers by 8,", "tokens": [50865, 1455, 309, 3801, 538, 30955, 613, 3547, 538, 1649, 11, 50985], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1092, "seek": 470836, "start": 4720.759999999999, "end": 4721.46, "text": " for example,", "tokens": [50985, 337, 1365, 11, 51020], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1093, "seek": 470836, "start": 4721.86, "end": 4723.86, "text": " you'll see that the softmax will start to sharpen.", "tokens": [51040, 291, 603, 536, 300, 264, 2787, 41167, 486, 722, 281, 31570, 13, 51140], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1094, "seek": 470836, "start": 4724.259999999999, "end": 4726.66, "text": " And in fact, it will sharpen towards the max.", "tokens": [51160, 400, 294, 1186, 11, 309, 486, 31570, 3030, 264, 11469, 13, 51280], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1095, "seek": 470836, "start": 4726.66, "end": 4729.5599999999995, "text": " So it will sharpen towards whatever number here is the highest.", "tokens": [51280, 407, 309, 486, 31570, 3030, 2035, 1230, 510, 307, 264, 6343, 13, 51425], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1096, "seek": 470836, "start": 4730.16, "end": 4733.36, "text": " And so basically we don't want these values to be too extreme,", "tokens": [51455, 400, 370, 1936, 321, 500, 380, 528, 613, 4190, 281, 312, 886, 8084, 11, 51615], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1097, "seek": 470836, "start": 4733.36, "end": 4734.5599999999995, "text": " especially the initialization.", "tokens": [51615, 2318, 264, 5883, 2144, 13, 51675], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1098, "seek": 470836, "start": 4734.5599999999995, "end": 4737.36, "text": " Otherwise softmax will be way too peaky and", "tokens": [51675, 10328, 2787, 41167, 486, 312, 636, 886, 10651, 88, 293, 51815], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1099, "seek": 470836, "start": 4737.36, "end": 4737.759999999999, "text": " um,", "tokens": [51815, 1105, 11, 51835], "temperature": 0.0, "avg_logprob": -0.14058517154894376, "compression_ratio": 1.8031746031746032, "no_speech_prob": 0.000296400161460042}, {"id": 1100, "seek": 473776, "start": 4737.76, "end": 4741.76, "text": " you're basically aggregating information from like a single node.", "tokens": [50365, 291, 434, 1936, 16743, 990, 1589, 490, 411, 257, 2167, 9984, 13, 50565], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1101, "seek": 473776, "start": 4741.76, "end": 4744.46, "text": " Every node just aggregates information from a single other node.", "tokens": [50565, 2048, 9984, 445, 16743, 1024, 1589, 490, 257, 2167, 661, 9984, 13, 50700], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1102, "seek": 473776, "start": 4744.46, "end": 4745.360000000001, "text": " That's not what we want,", "tokens": [50700, 663, 311, 406, 437, 321, 528, 11, 50745], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1103, "seek": 473776, "start": 4745.360000000001, "end": 4746.76, "text": " especially at initialization.", "tokens": [50745, 2318, 412, 5883, 2144, 13, 50815], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1104, "seek": 473776, "start": 4746.76, "end": 4751.26, "text": " And so the scaling is used just to control the variance at initialization.", "tokens": [50815, 400, 370, 264, 21589, 307, 1143, 445, 281, 1969, 264, 21977, 412, 5883, 2144, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1105, "seek": 473776, "start": 4751.26, "end": 4751.96, "text": " Okay.", "tokens": [51040, 1033, 13, 51075], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1106, "seek": 473776, "start": 4751.96, "end": 4753.06, "text": " So having said all that,", "tokens": [51075, 407, 1419, 848, 439, 300, 11, 51130], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1107, "seek": 473776, "start": 4753.06, "end": 4756.66, "text": " let's now take our self-attention knowledge and let's take it for a spin.", "tokens": [51130, 718, 311, 586, 747, 527, 2698, 12, 1591, 1251, 3601, 293, 718, 311, 747, 309, 337, 257, 6060, 13, 51310], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1108, "seek": 473776, "start": 4756.66, "end": 4758.76, "text": " So here in the code,", "tokens": [51310, 407, 510, 294, 264, 3089, 11, 51415], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1109, "seek": 473776, "start": 4758.76, "end": 4763.06, "text": " I've created this head module and implements a single head of self-attention.", "tokens": [51415, 286, 600, 2942, 341, 1378, 10088, 293, 704, 17988, 257, 2167, 1378, 295, 2698, 12, 1591, 1251, 13, 51630], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1110, "seek": 473776, "start": 4763.06, "end": 4767.16, "text": " So you give it a head size and then here it creates the key query and evaluate.", "tokens": [51630, 407, 291, 976, 309, 257, 1378, 2744, 293, 550, 510, 309, 7829, 264, 2141, 14581, 293, 13059, 13, 51835], "temperature": 0.0, "avg_logprob": -0.17809173795912, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.00011259418533882126}, {"id": 1111, "seek": 476716, "start": 4767.16, "end": 4768.96, "text": " Linear layers.", "tokens": [50365, 14670, 289, 7914, 13, 50455], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1112, "seek": 476716, "start": 4769.36, "end": 4771.26, "text": " Typically people don't use biases in these.", "tokens": [50475, 23129, 561, 500, 380, 764, 32152, 294, 613, 13, 50570], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1113, "seek": 476716, "start": 4772.36, "end": 4775.5599999999995, "text": " So those are the linear projections that we're going to apply to all of our nodes.", "tokens": [50625, 407, 729, 366, 264, 8213, 32371, 300, 321, 434, 516, 281, 3079, 281, 439, 295, 527, 13891, 13, 50785], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1114, "seek": 476716, "start": 4776.36, "end": 4777.0599999999995, "text": " Now here,", "tokens": [50825, 823, 510, 11, 50860], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1115, "seek": 476716, "start": 4777.0599999999995, "end": 4778.96, "text": " I'm creating this trill variable.", "tokens": [50860, 286, 478, 4084, 341, 504, 373, 7006, 13, 50955], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1116, "seek": 476716, "start": 4779.26, "end": 4781.16, "text": " Trill is not a parameter of the module.", "tokens": [50970, 1765, 373, 307, 406, 257, 13075, 295, 264, 10088, 13, 51065], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1117, "seek": 476716, "start": 4781.16, "end": 4783.16, "text": " So in sort of pytorch naming conventions,", "tokens": [51065, 407, 294, 1333, 295, 25878, 284, 339, 25290, 33520, 11, 51165], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1118, "seek": 476716, "start": 4783.5599999999995, "end": 4784.66, "text": " this is called a buffer.", "tokens": [51185, 341, 307, 1219, 257, 21762, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1119, "seek": 476716, "start": 4784.86, "end": 4786.96, "text": " It's not a parameter and you have to call it.", "tokens": [51250, 467, 311, 406, 257, 13075, 293, 291, 362, 281, 818, 309, 13, 51355], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1120, "seek": 476716, "start": 4786.96, "end": 4789.26, "text": " You have to assign it to the module using a register buffer.", "tokens": [51355, 509, 362, 281, 6269, 309, 281, 264, 10088, 1228, 257, 7280, 21762, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1121, "seek": 476716, "start": 4789.66, "end": 4790.76, "text": " So that creates the trill,", "tokens": [51490, 407, 300, 7829, 264, 504, 373, 11, 51545], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1122, "seek": 476716, "start": 4791.66, "end": 4793.66, "text": " the lower triangular matrix.", "tokens": [51590, 264, 3126, 38190, 8141, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1123, "seek": 476716, "start": 4794.46, "end": 4795.76, "text": " And when we're given the input X,", "tokens": [51730, 400, 562, 321, 434, 2212, 264, 4846, 1783, 11, 51795], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1124, "seek": 476716, "start": 4795.76, "end": 4797.16, "text": " this should look very familiar now.", "tokens": [51795, 341, 820, 574, 588, 4963, 586, 13, 51865], "temperature": 0.0, "avg_logprob": -0.1477581278483073, "compression_ratio": 1.7045454545454546, "no_speech_prob": 0.000537517829798162}, {"id": 1125, "seek": 479716, "start": 4797.36, "end": 4798.46, "text": " We calculate the keys,", "tokens": [50375, 492, 8873, 264, 9317, 11, 50430], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1126, "seek": 479716, "start": 4798.46, "end": 4799.26, "text": " the queries,", "tokens": [50430, 264, 24109, 11, 50470], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1127, "seek": 479716, "start": 4799.46, "end": 4802.16, "text": " we calculate the attention scores inside way.", "tokens": [50480, 321, 8873, 264, 3202, 13444, 1854, 636, 13, 50615], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1128, "seek": 479716, "start": 4802.96, "end": 4803.76, "text": " We normalize it.", "tokens": [50655, 492, 2710, 1125, 309, 13, 50695], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1129, "seek": 479716, "start": 4803.76, "end": 4805.5599999999995, "text": " So we're using scaled attention here.", "tokens": [50695, 407, 321, 434, 1228, 36039, 3202, 510, 13, 50785], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1130, "seek": 479716, "start": 4806.26, "end": 4809.36, "text": " Then we make sure that sure doesn't communicate with the past.", "tokens": [50820, 1396, 321, 652, 988, 300, 988, 1177, 380, 7890, 365, 264, 1791, 13, 50975], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1131, "seek": 479716, "start": 4809.5599999999995, "end": 4811.26, "text": " So this makes it a decoder block", "tokens": [50985, 407, 341, 1669, 309, 257, 979, 19866, 3461, 51070], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1132, "seek": 479716, "start": 4812.0599999999995, "end": 4815.0599999999995, "text": " and then softmax and then aggregate the value and output.", "tokens": [51110, 293, 550, 2787, 41167, 293, 550, 26118, 264, 2158, 293, 5598, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1133, "seek": 479716, "start": 4816.5599999999995, "end": 4817.5599999999995, "text": " Then here in the language model,", "tokens": [51335, 1396, 510, 294, 264, 2856, 2316, 11, 51385], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1134, "seek": 479716, "start": 4817.5599999999995, "end": 4822.0599999999995, "text": " I'm creating a head in the constructor and I'm calling it self-attention head", "tokens": [51385, 286, 478, 4084, 257, 1378, 294, 264, 47479, 293, 286, 478, 5141, 309, 2698, 12, 1591, 1251, 1378, 51610], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1135, "seek": 479716, "start": 4822.46, "end": 4823.76, "text": " and the head size.", "tokens": [51630, 293, 264, 1378, 2744, 13, 51695], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1136, "seek": 479716, "start": 4823.76, "end": 4826.96, "text": " I'm going to keep as the same and embed just for now.", "tokens": [51695, 286, 478, 516, 281, 1066, 382, 264, 912, 293, 12240, 445, 337, 586, 13, 51855], "temperature": 0.0, "avg_logprob": -0.1488049339702111, "compression_ratio": 1.7555555555555555, "no_speech_prob": 0.0004237742687109858}, {"id": 1137, "seek": 482716, "start": 4827.16, "end": 4832.96, "text": " And then here once we've encoded the information with the token embeddings", "tokens": [50365, 400, 550, 510, 1564, 321, 600, 2058, 12340, 264, 1589, 365, 264, 14862, 12240, 29432, 50655], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1138, "seek": 482716, "start": 4832.96, "end": 4834.0599999999995, "text": " and the position embeddings,", "tokens": [50655, 293, 264, 2535, 12240, 29432, 11, 50710], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1139, "seek": 482716, "start": 4834.46, "end": 4836.76, "text": " we're simply going to feed it into the self-attention head", "tokens": [50730, 321, 434, 2935, 516, 281, 3154, 309, 666, 264, 2698, 12, 1591, 1251, 1378, 50845], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1140, "seek": 482716, "start": 4837.16, "end": 4842.5599999999995, "text": " and then the output of that is going to go into the decoder language modeling", "tokens": [50865, 293, 550, 264, 5598, 295, 300, 307, 516, 281, 352, 666, 264, 979, 19866, 2856, 15983, 51135], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1141, "seek": 482716, "start": 4842.5599999999995, "end": 4844.16, "text": " head and create the logits.", "tokens": [51135, 1378, 293, 1884, 264, 3565, 1208, 13, 51215], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1142, "seek": 482716, "start": 4844.5599999999995, "end": 4848.26, "text": " So this is sort of the simplest way to plug in a self-attention component", "tokens": [51235, 407, 341, 307, 1333, 295, 264, 22811, 636, 281, 5452, 294, 257, 2698, 12, 1591, 1251, 6542, 51420], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1143, "seek": 482716, "start": 4849.0599999999995, "end": 4850.36, "text": " into our network right now.", "tokens": [51460, 666, 527, 3209, 558, 586, 13, 51525], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1144, "seek": 482716, "start": 4851.16, "end": 4852.46, "text": " I had to make one more change,", "tokens": [51565, 286, 632, 281, 652, 472, 544, 1319, 11, 51630], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1145, "seek": 482716, "start": 4852.86, "end": 4855.96, "text": " which is that here in the generate,", "tokens": [51650, 597, 307, 300, 510, 294, 264, 8460, 11, 51805], "temperature": 0.0, "avg_logprob": -0.151949222271259, "compression_ratio": 1.7764227642276422, "no_speech_prob": 0.00014097924577072263}, {"id": 1146, "seek": 485596, "start": 4855.96, "end": 4860.36, "text": " we have to make sure that our IDX that we feed into the model", "tokens": [50365, 321, 362, 281, 652, 988, 300, 527, 7348, 55, 300, 321, 3154, 666, 264, 2316, 50585], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1147, "seek": 485596, "start": 4860.96, "end": 4862.66, "text": " because now we're using positional embeddings,", "tokens": [50615, 570, 586, 321, 434, 1228, 2535, 304, 12240, 29432, 11, 50700], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1148, "seek": 485596, "start": 4862.96, "end": 4865.76, "text": " we can never have more than block size coming in", "tokens": [50715, 321, 393, 1128, 362, 544, 813, 3461, 2744, 1348, 294, 50855], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1149, "seek": 485596, "start": 4866.16, "end": 4868.66, "text": " because if IDX is more than block size,", "tokens": [50875, 570, 498, 7348, 55, 307, 544, 813, 3461, 2744, 11, 51000], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1150, "seek": 485596, "start": 4868.96, "end": 4871.46, "text": " then our position embedding table is going to run out of scope", "tokens": [51015, 550, 527, 2535, 12240, 3584, 3199, 307, 516, 281, 1190, 484, 295, 11923, 51140], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1151, "seek": 485596, "start": 4871.46, "end": 4873.56, "text": " because it only has embeddings for up to block size.", "tokens": [51140, 570, 309, 787, 575, 12240, 29432, 337, 493, 281, 3461, 2744, 13, 51245], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1152, "seek": 485596, "start": 4874.46, "end": 4877.66, "text": " And so therefore I added some code here to crop the context", "tokens": [51290, 400, 370, 4412, 286, 3869, 512, 3089, 510, 281, 9086, 264, 4319, 51450], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1153, "seek": 485596, "start": 4878.26, "end": 4879.96, "text": " that we're going to feed into self", "tokens": [51480, 300, 321, 434, 516, 281, 3154, 666, 2698, 51565], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1154, "seek": 485596, "start": 4881.66, "end": 4884.66, "text": " so that we never pass in more than block size elements.", "tokens": [51650, 370, 300, 321, 1128, 1320, 294, 544, 813, 3461, 2744, 4959, 13, 51800], "temperature": 0.0, "avg_logprob": -0.13729178319211865, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.00011088662722613662}, {"id": 1155, "seek": 488466, "start": 4884.66, "end": 4887.76, "text": " So those are the changes and let's now train the network.", "tokens": [50365, 407, 729, 366, 264, 2962, 293, 718, 311, 586, 3847, 264, 3209, 13, 50520], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1156, "seek": 488466, "start": 4888.0599999999995, "end": 4891.5599999999995, "text": " Okay, so I also came up to the script here and I decreased the learning rate", "tokens": [50535, 1033, 11, 370, 286, 611, 1361, 493, 281, 264, 5755, 510, 293, 286, 24436, 264, 2539, 3314, 50710], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1157, "seek": 488466, "start": 4891.5599999999995, "end": 4894.96, "text": " because the self-attention can't tolerate very very high learning rates.", "tokens": [50710, 570, 264, 2698, 12, 1591, 1251, 393, 380, 25773, 588, 588, 1090, 2539, 6846, 13, 50880], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1158, "seek": 488466, "start": 4895.5599999999995, "end": 4897.66, "text": " And then I also increased the number of iterations", "tokens": [50910, 400, 550, 286, 611, 6505, 264, 1230, 295, 36540, 51015], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1159, "seek": 488466, "start": 4897.66, "end": 4900.16, "text": " because the learning rate is lower and then I trained it", "tokens": [51015, 570, 264, 2539, 3314, 307, 3126, 293, 550, 286, 8895, 309, 51140], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1160, "seek": 488466, "start": 4900.16, "end": 4902.96, "text": " and previously we were only able to get to up to 2.5", "tokens": [51140, 293, 8046, 321, 645, 787, 1075, 281, 483, 281, 493, 281, 568, 13, 20, 51280], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1161, "seek": 488466, "start": 4903.26, "end": 4904.86, "text": " and now we are down to 2.4.", "tokens": [51295, 293, 586, 321, 366, 760, 281, 568, 13, 19, 13, 51375], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1162, "seek": 488466, "start": 4905.26, "end": 4909.26, "text": " So we definitely see a little bit of improvement from 2.5 to 2.4 roughly,", "tokens": [51395, 407, 321, 2138, 536, 257, 707, 857, 295, 10444, 490, 568, 13, 20, 281, 568, 13, 19, 9810, 11, 51595], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1163, "seek": 488466, "start": 4909.86, "end": 4911.46, "text": " but the text is still not amazing.", "tokens": [51625, 457, 264, 2487, 307, 920, 406, 2243, 13, 51705], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1164, "seek": 488466, "start": 4911.96, "end": 4914.46, "text": " So clearly the self-attention head is doing", "tokens": [51730, 407, 4448, 264, 2698, 12, 1591, 1251, 1378, 307, 884, 51855], "temperature": 0.0, "avg_logprob": -0.1389492839372077, "compression_ratio": 1.8, "no_speech_prob": 0.0002802440430969}, {"id": 1165, "seek": 491466, "start": 4914.66, "end": 4915.96, "text": " some useful communication,", "tokens": [50365, 512, 4420, 6101, 11, 50430], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1166, "seek": 491466, "start": 4916.46, "end": 4919.0599999999995, "text": " but we still have a long way to go.", "tokens": [50455, 457, 321, 920, 362, 257, 938, 636, 281, 352, 13, 50585], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1167, "seek": 491466, "start": 4919.36, "end": 4919.5599999999995, "text": " Okay.", "tokens": [50600, 1033, 13, 50610], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1168, "seek": 491466, "start": 4919.5599999999995, "end": 4921.86, "text": " So now we've implemented the scale dot product attention.", "tokens": [50610, 407, 586, 321, 600, 12270, 264, 4373, 5893, 1674, 3202, 13, 50725], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1169, "seek": 491466, "start": 4922.16, "end": 4924.5599999999995, "text": " Now next up in the attention is all you need paper.", "tokens": [50740, 823, 958, 493, 294, 264, 3202, 307, 439, 291, 643, 3035, 13, 50860], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1170, "seek": 491466, "start": 4925.0599999999995, "end": 4926.66, "text": " There's something called multi-head attention.", "tokens": [50885, 821, 311, 746, 1219, 4825, 12, 1934, 3202, 13, 50965], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1171, "seek": 491466, "start": 4927.0599999999995, "end": 4928.46, "text": " And what is multi-head attention?", "tokens": [50985, 400, 437, 307, 4825, 12, 1934, 3202, 30, 51055], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1172, "seek": 491466, "start": 4928.86, "end": 4931.76, "text": " It's just applying multiple attentions in parallel", "tokens": [51075, 467, 311, 445, 9275, 3866, 30980, 626, 294, 8952, 51220], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1173, "seek": 491466, "start": 4931.96, "end": 4933.46, "text": " and concatenating the results.", "tokens": [51230, 293, 1588, 7186, 990, 264, 3542, 13, 51305], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1174, "seek": 491466, "start": 4934.0599999999995, "end": 4935.96, "text": " So they have a little bit of diagram here.", "tokens": [51335, 407, 436, 362, 257, 707, 857, 295, 10686, 510, 13, 51430], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1175, "seek": 491466, "start": 4936.26, "end": 4937.76, "text": " I don't know if this is super clear.", "tokens": [51445, 286, 500, 380, 458, 498, 341, 307, 1687, 1850, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1176, "seek": 491466, "start": 4938.36, "end": 4941.0599999999995, "text": " It's really just multiple attentions in parallel.", "tokens": [51550, 467, 311, 534, 445, 3866, 30980, 626, 294, 8952, 13, 51685], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1177, "seek": 491466, "start": 4941.76, "end": 4944.26, "text": " So let's implement that fairly straightforward.", "tokens": [51720, 407, 718, 311, 4445, 300, 6457, 15325, 13, 51845], "temperature": 0.0, "avg_logprob": -0.13307880485144846, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.00024990568635985255}, {"id": 1178, "seek": 494466, "start": 4945.36, "end": 4946.96, "text": " If we want a multi-head attention,", "tokens": [50400, 759, 321, 528, 257, 4825, 12, 1934, 3202, 11, 50480], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1179, "seek": 494466, "start": 4947.26, "end": 4949.96, "text": " then we want multiple heads of self-attention running in parallel.", "tokens": [50495, 550, 321, 528, 3866, 8050, 295, 2698, 12, 1591, 1251, 2614, 294, 8952, 13, 50630], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1180, "seek": 494466, "start": 4950.86, "end": 4954.96, "text": " So in PyTorch we can do this by simply creating multiple heads.", "tokens": [50675, 407, 294, 9953, 51, 284, 339, 321, 393, 360, 341, 538, 2935, 4084, 3866, 8050, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1181, "seek": 494466, "start": 4956.16, "end": 4958.96, "text": " So however many heads you want", "tokens": [50940, 407, 4461, 867, 8050, 291, 528, 51080], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1182, "seek": 494466, "start": 4959.16, "end": 4960.76, "text": " and then what is the head size of each", "tokens": [51090, 293, 550, 437, 307, 264, 1378, 2744, 295, 1184, 51170], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1183, "seek": 494466, "start": 4961.5599999999995, "end": 4965.26, "text": " and then we run all of them in parallel into a list", "tokens": [51210, 293, 550, 321, 1190, 439, 295, 552, 294, 8952, 666, 257, 1329, 51395], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1184, "seek": 494466, "start": 4965.5599999999995, "end": 4967.76, "text": " and simply concatenate all of the outputs", "tokens": [51410, 293, 2935, 1588, 7186, 473, 439, 295, 264, 23930, 51520], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1185, "seek": 494466, "start": 4968.36, "end": 4970.5599999999995, "text": " and we're concatenating over the channel dimension.", "tokens": [51550, 293, 321, 434, 1588, 7186, 990, 670, 264, 2269, 10139, 13, 51660], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1186, "seek": 494466, "start": 4971.66, "end": 4974.46, "text": " So the way this looks now is we don't have just a single attention", "tokens": [51715, 407, 264, 636, 341, 1542, 586, 307, 321, 500, 380, 362, 445, 257, 2167, 3202, 51855], "temperature": 0.0, "avg_logprob": -0.11929094791412354, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0005066857556812465}, {"id": 1187, "seek": 497466, "start": 4974.96, "end": 4980.5599999999995, "text": " that has a head size of 32 because remember an embed is 32.", "tokens": [50380, 300, 575, 257, 1378, 2744, 295, 8858, 570, 1604, 364, 12240, 307, 8858, 13, 50660], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1188, "seek": 497466, "start": 4981.66, "end": 4984.16, "text": " Instead of having one communication channel,", "tokens": [50715, 7156, 295, 1419, 472, 6101, 2269, 11, 50840], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1189, "seek": 497466, "start": 4984.46, "end": 4987.86, "text": " we now have four communication channels in parallel", "tokens": [50855, 321, 586, 362, 1451, 6101, 9235, 294, 8952, 51025], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1190, "seek": 497466, "start": 4988.16, "end": 4990.46, "text": " and each one of these communication channels typically", "tokens": [51040, 293, 1184, 472, 295, 613, 6101, 9235, 5850, 51155], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1191, "seek": 497466, "start": 4990.96, "end": 4994.16, "text": " will be smaller correspondingly.", "tokens": [51180, 486, 312, 4356, 11760, 356, 13, 51340], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1192, "seek": 497466, "start": 4994.5599999999995, "end": 4996.46, "text": " So because we have four communication channels,", "tokens": [51360, 407, 570, 321, 362, 1451, 6101, 9235, 11, 51455], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1193, "seek": 497466, "start": 4996.76, "end": 4998.5599999999995, "text": " we want eight-dimensional self-attention.", "tokens": [51470, 321, 528, 3180, 12, 18759, 2698, 12, 1591, 1251, 13, 51560], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1194, "seek": 497466, "start": 4999.16, "end": 5000.86, "text": " And so from each communication channel,", "tokens": [51590, 400, 370, 490, 1184, 6101, 2269, 11, 51675], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1195, "seek": 497466, "start": 5000.86, "end": 5003.0599999999995, "text": " we're getting together eight-dimensional vectors", "tokens": [51675, 321, 434, 1242, 1214, 3180, 12, 18759, 18875, 51785], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1196, "seek": 497466, "start": 5003.36, "end": 5004.5599999999995, "text": " and then we have four of them.", "tokens": [51800, 293, 550, 321, 362, 1451, 295, 552, 13, 51860], "temperature": 0.0, "avg_logprob": -0.23740381513323103, "compression_ratio": 1.9653679653679654, "no_speech_prob": 0.00031900827889330685}, {"id": 1197, "seek": 500466, "start": 5004.66, "end": 5006.76, "text": " And that concatenates to give us 32,", "tokens": [50365, 400, 300, 1588, 7186, 1024, 281, 976, 505, 8858, 11, 50470], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1198, "seek": 500466, "start": 5006.86, "end": 5008.26, "text": " which is the original and embed.", "tokens": [50475, 597, 307, 264, 3380, 293, 12240, 13, 50545], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1199, "seek": 500466, "start": 5009.16, "end": 5012.16, "text": " And so this is kind of similar to if you're familiar with convolutions,", "tokens": [50590, 400, 370, 341, 307, 733, 295, 2531, 281, 498, 291, 434, 4963, 365, 3754, 15892, 11, 50740], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1200, "seek": 500466, "start": 5012.16, "end": 5013.86, "text": " this is kind of like a group convolution", "tokens": [50740, 341, 307, 733, 295, 411, 257, 1594, 45216, 50825], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1201, "seek": 500466, "start": 5014.46, "end": 5017.16, "text": " because basically instead of having one large convolution,", "tokens": [50855, 570, 1936, 2602, 295, 1419, 472, 2416, 45216, 11, 50990], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1202, "seek": 500466, "start": 5017.26, "end": 5021.5599999999995, "text": " we do convolution in groups and that's multi-headed self-attention.", "tokens": [50995, 321, 360, 45216, 294, 3935, 293, 300, 311, 4825, 12, 28409, 2698, 12, 1591, 1251, 13, 51210], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1203, "seek": 500466, "start": 5022.5599999999995, "end": 5025.36, "text": " And so then here we just use essay heads,", "tokens": [51260, 400, 370, 550, 510, 321, 445, 764, 16238, 8050, 11, 51400], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1204, "seek": 500466, "start": 5025.46, "end": 5026.86, "text": " self-attention heads instead.", "tokens": [51405, 2698, 12, 1591, 1251, 8050, 2602, 13, 51475], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1205, "seek": 500466, "start": 5027.76, "end": 5031.0599999999995, "text": " Now, I actually ran it and scrolling down,", "tokens": [51520, 823, 11, 286, 767, 5872, 309, 293, 29053, 760, 11, 51685], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1206, "seek": 500466, "start": 5032.26, "end": 5034.16, "text": " I ran the same thing and then we now get down", "tokens": [51745, 286, 5872, 264, 912, 551, 293, 550, 321, 586, 483, 760, 51840], "temperature": 0.0, "avg_logprob": -0.18240103149414064, "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00027725641848519444}, {"id": 1207, "seek": 503466, "start": 5034.66, "end": 5038.36, "text": " to 2.28 roughly and the output is still,", "tokens": [50365, 281, 568, 13, 11205, 9810, 293, 264, 5598, 307, 920, 11, 50550], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1208, "seek": 503466, "start": 5038.36, "end": 5039.76, "text": " the generation is still not amazing,", "tokens": [50550, 264, 5125, 307, 920, 406, 2243, 11, 50620], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1209, "seek": 503466, "start": 5039.96, "end": 5041.76, "text": " but clearly the validation loss is improving", "tokens": [50630, 457, 4448, 264, 24071, 4470, 307, 11470, 50720], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1210, "seek": 503466, "start": 5041.76, "end": 5043.96, "text": " because we were at 2.4 just now.", "tokens": [50720, 570, 321, 645, 412, 568, 13, 19, 445, 586, 13, 50830], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1211, "seek": 503466, "start": 5044.86, "end": 5047.16, "text": " And so it helps to have multiple communication channels", "tokens": [50875, 400, 370, 309, 3665, 281, 362, 3866, 6101, 9235, 50990], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1212, "seek": 503466, "start": 5047.16, "end": 5050.26, "text": " because obviously these tokens have a lot to talk about.", "tokens": [50990, 570, 2745, 613, 22667, 362, 257, 688, 281, 751, 466, 13, 51145], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1213, "seek": 503466, "start": 5050.76, "end": 5052.66, "text": " They want to find the consonants, the vowels,", "tokens": [51170, 814, 528, 281, 915, 264, 30843, 1719, 11, 264, 44972, 11, 51265], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1214, "seek": 503466, "start": 5052.66, "end": 5054.96, "text": " they want to find the vowels just from certain positions,", "tokens": [51265, 436, 528, 281, 915, 264, 44972, 445, 490, 1629, 8432, 11, 51380], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1215, "seek": 503466, "start": 5055.46, "end": 5058.0599999999995, "text": " they want to find any kinds of different things.", "tokens": [51405, 436, 528, 281, 915, 604, 3685, 295, 819, 721, 13, 51535], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1216, "seek": 503466, "start": 5058.46, "end": 5061.46, "text": " And so it helps to create multiple independent channels of communication,", "tokens": [51555, 400, 370, 309, 3665, 281, 1884, 3866, 6695, 9235, 295, 6101, 11, 51705], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1217, "seek": 503466, "start": 5061.5599999999995, "end": 5063.36, "text": " gather lots of different types of data", "tokens": [51710, 5448, 3195, 295, 819, 3467, 295, 1412, 51800], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1218, "seek": 503466, "start": 5063.76, "end": 5064.36, "text": " and then", "tokens": [51820, 293, 550, 51850], "temperature": 0.0, "avg_logprob": -0.12517774194703066, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.00038709380896762013}, {"id": 1219, "seek": 506466, "start": 5064.66, "end": 5065.76, "text": " decode the output.", "tokens": [50365, 979, 1429, 264, 5598, 13, 50420], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1220, "seek": 506466, "start": 5066.16, "end": 5067.66, "text": " Now going back to the paper for a second,", "tokens": [50440, 823, 516, 646, 281, 264, 3035, 337, 257, 1150, 11, 50515], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1221, "seek": 506466, "start": 5067.86, "end": 5068.26, "text": " of course,", "tokens": [50525, 295, 1164, 11, 50545], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1222, "seek": 506466, "start": 5068.26, "end": 5069.96, "text": " I didn't explain this figure in full detail,", "tokens": [50545, 286, 994, 380, 2903, 341, 2573, 294, 1577, 2607, 11, 50630], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1223, "seek": 506466, "start": 5069.96, "end": 5073.16, "text": " but we are starting to see some components of what we've already implemented.", "tokens": [50630, 457, 321, 366, 2891, 281, 536, 512, 6677, 295, 437, 321, 600, 1217, 12270, 13, 50790], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1224, "seek": 506466, "start": 5073.36, "end": 5074.66, "text": " We have the positional encodings,", "tokens": [50800, 492, 362, 264, 2535, 304, 2058, 378, 1109, 11, 50865], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1225, "seek": 506466, "start": 5074.66, "end": 5076.16, "text": " the token encodings that add,", "tokens": [50865, 264, 14862, 2058, 378, 1109, 300, 909, 11, 50940], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1226, "seek": 506466, "start": 5076.5599999999995, "end": 5079.26, "text": " we have the masked multi-headed attention implemented.", "tokens": [50960, 321, 362, 264, 45249, 4825, 12, 28409, 3202, 12270, 13, 51095], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1227, "seek": 506466, "start": 5079.96, "end": 5082.0599999999995, "text": " Now, here's another multi-headed attention,", "tokens": [51130, 823, 11, 510, 311, 1071, 4825, 12, 28409, 3202, 11, 51235], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1228, "seek": 506466, "start": 5082.0599999999995, "end": 5084.36, "text": " which is a cross attention to an encoder,", "tokens": [51235, 597, 307, 257, 3278, 3202, 281, 364, 2058, 19866, 11, 51350], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1229, "seek": 506466, "start": 5084.36, "end": 5085.26, "text": " which we haven't,", "tokens": [51350, 597, 321, 2378, 380, 11, 51395], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1230, "seek": 506466, "start": 5085.26, "end": 5086.86, "text": " we're not going to implement in this case.", "tokens": [51395, 321, 434, 406, 516, 281, 4445, 294, 341, 1389, 13, 51475], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1231, "seek": 506466, "start": 5087.16, "end": 5088.46, "text": " I'm going to come back to that later.", "tokens": [51490, 286, 478, 516, 281, 808, 646, 281, 300, 1780, 13, 51555], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1232, "seek": 506466, "start": 5089.46, "end": 5091.96, "text": " But I want you to notice that there's a feed forward part here", "tokens": [51605, 583, 286, 528, 291, 281, 3449, 300, 456, 311, 257, 3154, 2128, 644, 510, 51730], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1233, "seek": 506466, "start": 5092.16, "end": 5094.5599999999995, "text": " and then this is grouped into a block that gets repeated.", "tokens": [51740, 293, 550, 341, 307, 41877, 666, 257, 3461, 300, 2170, 10477, 13, 51860], "temperature": 0.0, "avg_logprob": -0.11516097534534543, "compression_ratio": 1.8532934131736527, "no_speech_prob": 0.0001292323286179453}, {"id": 1234, "seek": 509466, "start": 5094.86, "end": 5095.36, "text": " And again,", "tokens": [50375, 400, 797, 11, 50400], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1235, "seek": 509466, "start": 5095.96, "end": 5099.36, "text": " now the feed forward part here is just a simple multi-layer perceptron.", "tokens": [50430, 586, 264, 3154, 2128, 644, 510, 307, 445, 257, 2199, 4825, 12, 8376, 260, 43276, 2044, 13, 50600], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1236, "seek": 509466, "start": 5101.76, "end": 5102.66, "text": " So the multi-headed,", "tokens": [50720, 407, 264, 4825, 12, 28409, 11, 50765], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1237, "seek": 509466, "start": 5103.0599999999995, "end": 5107.46, "text": " so here position wise feed forward networks is just a simple little MLP.", "tokens": [50785, 370, 510, 2535, 10829, 3154, 2128, 9590, 307, 445, 257, 2199, 707, 21601, 47, 13, 51005], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1238, "seek": 509466, "start": 5108.16, "end": 5110.36, "text": " So I want to start basically in a similar fashion.", "tokens": [51040, 407, 286, 528, 281, 722, 1936, 294, 257, 2531, 6700, 13, 51150], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1239, "seek": 509466, "start": 5110.36, "end": 5112.76, "text": " Also adding computation into the network", "tokens": [51150, 2743, 5127, 24903, 666, 264, 3209, 51270], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1240, "seek": 509466, "start": 5113.36, "end": 5115.5599999999995, "text": " and this computation is on the per node level.", "tokens": [51300, 293, 341, 24903, 307, 322, 264, 680, 9984, 1496, 13, 51410], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1241, "seek": 509466, "start": 5116.0599999999995, "end": 5116.5599999999995, "text": " So", "tokens": [51435, 407, 51460], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1242, "seek": 509466, "start": 5117.46, "end": 5120.86, "text": " I've already implemented it and you can see the diff highlighted on the left here", "tokens": [51505, 286, 600, 1217, 12270, 309, 293, 291, 393, 536, 264, 7593, 17173, 322, 264, 1411, 510, 51675], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1243, "seek": 509466, "start": 5120.86, "end": 5122.26, "text": " when I've added or changed things.", "tokens": [51675, 562, 286, 600, 3869, 420, 3105, 721, 13, 51745], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1244, "seek": 509466, "start": 5122.96, "end": 5124.16, "text": " Now before we had the", "tokens": [51780, 823, 949, 321, 632, 264, 51840], "temperature": 0.0, "avg_logprob": -0.173017470304631, "compression_ratio": 1.7245283018867925, "no_speech_prob": 8.484621503157541e-05}, {"id": 1245, "seek": 512466, "start": 5124.66, "end": 5126.96, "text": " multi-headed self-attention that did the communication,", "tokens": [50365, 4825, 12, 28409, 2698, 12, 1591, 1251, 300, 630, 264, 6101, 11, 50480], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1246, "seek": 512466, "start": 5127.36, "end": 5130.26, "text": " but we went way too fast to calculate the logits.", "tokens": [50500, 457, 321, 1437, 636, 886, 2370, 281, 8873, 264, 3565, 1208, 13, 50645], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1247, "seek": 512466, "start": 5130.66, "end": 5132.46, "text": " So the tokens looked at each other,", "tokens": [50665, 407, 264, 22667, 2956, 412, 1184, 661, 11, 50755], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1248, "seek": 512466, "start": 5132.46, "end": 5136.76, "text": " but didn't really have a lot of time to think on what they found from the other tokens.", "tokens": [50755, 457, 994, 380, 534, 362, 257, 688, 295, 565, 281, 519, 322, 437, 436, 1352, 490, 264, 661, 22667, 13, 50970], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1249, "seek": 512466, "start": 5137.5599999999995, "end": 5138.16, "text": " And so", "tokens": [51010, 400, 370, 51040], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1250, "seek": 512466, "start": 5138.76, "end": 5141.86, "text": " what I've implemented here is a little feed forward single layer", "tokens": [51070, 437, 286, 600, 12270, 510, 307, 257, 707, 3154, 2128, 2167, 4583, 51225], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1251, "seek": 512466, "start": 5142.36, "end": 5145.96, "text": " and this little layer is just a linear followed by a relu non-linearity", "tokens": [51250, 293, 341, 707, 4583, 307, 445, 257, 8213, 6263, 538, 257, 1039, 84, 2107, 12, 1889, 17409, 51430], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1252, "seek": 512466, "start": 5146.16, "end": 5146.96, "text": " and that's it.", "tokens": [51440, 293, 300, 311, 309, 13, 51480], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1253, "seek": 512466, "start": 5147.96, "end": 5149.5599999999995, "text": " So it's just a little layer", "tokens": [51530, 407, 309, 311, 445, 257, 707, 4583, 51610], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1254, "seek": 512466, "start": 5150.16, "end": 5151.96, "text": " and then I call it feed forward", "tokens": [51640, 293, 550, 286, 818, 309, 3154, 2128, 51730], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1255, "seek": 512466, "start": 5153.5599999999995, "end": 5154.16, "text": " and embed.", "tokens": [51810, 293, 12240, 13, 51840], "temperature": 0.0, "avg_logprob": -0.14446669816970825, "compression_ratio": 1.7386363636363635, "no_speech_prob": 0.00025879801250994205}, {"id": 1256, "seek": 515466, "start": 5154.66, "end": 5158.5599999999995, "text": " And then this feed forward is just called sequentially right after the self-attention.", "tokens": [50365, 400, 550, 341, 3154, 2128, 307, 445, 1219, 5123, 3137, 558, 934, 264, 2698, 12, 1591, 1251, 13, 50560], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1257, "seek": 515466, "start": 5158.96, "end": 5161.46, "text": " So we self-attend then we feed forward", "tokens": [50580, 407, 321, 2698, 12, 49548, 550, 321, 3154, 2128, 50705], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1258, "seek": 515466, "start": 5161.96, "end": 5164.66, "text": " and you'll notice that the feed forward here when it's applying linear.", "tokens": [50730, 293, 291, 603, 3449, 300, 264, 3154, 2128, 510, 562, 309, 311, 9275, 8213, 13, 50865], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1259, "seek": 515466, "start": 5164.86, "end": 5166.36, "text": " This is on a per token level.", "tokens": [50875, 639, 307, 322, 257, 680, 14862, 1496, 13, 50950], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1260, "seek": 515466, "start": 5166.46, "end": 5168.26, "text": " All the tokens do this independently.", "tokens": [50955, 1057, 264, 22667, 360, 341, 21761, 13, 51045], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1261, "seek": 515466, "start": 5168.66, "end": 5171.0599999999995, "text": " So the self-attention is the communication", "tokens": [51065, 407, 264, 2698, 12, 1591, 1251, 307, 264, 6101, 51185], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1262, "seek": 515466, "start": 5171.46, "end": 5175.26, "text": " and then once they've gathered all the data now they need to think on that data individually.", "tokens": [51205, 293, 550, 1564, 436, 600, 13032, 439, 264, 1412, 586, 436, 643, 281, 519, 322, 300, 1412, 16652, 13, 51395], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1263, "seek": 515466, "start": 5176.16, "end": 5177.66, "text": " And so that's what feed forward is doing", "tokens": [51440, 400, 370, 300, 311, 437, 3154, 2128, 307, 884, 51515], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1264, "seek": 515466, "start": 5178.0599999999995, "end": 5179.5599999999995, "text": " and that's why I've added it here.", "tokens": [51535, 293, 300, 311, 983, 286, 600, 3869, 309, 510, 13, 51610], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1265, "seek": 515466, "start": 5180.26, "end": 5184.36, "text": " Now when I train this the validation laws actually continues to go down now to 2.24.", "tokens": [51645, 823, 562, 286, 3847, 341, 264, 24071, 6064, 767, 6515, 281, 352, 760, 586, 281, 568, 13, 7911, 13, 51850], "temperature": 0.0, "avg_logprob": -0.16458744472927517, "compression_ratio": 1.8642384105960266, "no_speech_prob": 0.00024760575615800917}, {"id": 1266, "seek": 518466, "start": 5185.26, "end": 5186.96, "text": " Which is down from 2.28.", "tokens": [50395, 3013, 307, 760, 490, 568, 13, 11205, 13, 50480], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1267, "seek": 518466, "start": 5187.66, "end": 5189.36, "text": " The output still look kind of terrible,", "tokens": [50515, 440, 5598, 920, 574, 733, 295, 6237, 11, 50600], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1268, "seek": 518466, "start": 5189.66, "end": 5191.36, "text": " but at least we've improved the situation.", "tokens": [50615, 457, 412, 1935, 321, 600, 9689, 264, 2590, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1269, "seek": 518466, "start": 5192.0599999999995, "end": 5193.16, "text": " And so as a preview", "tokens": [50735, 400, 370, 382, 257, 14281, 50790], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1270, "seek": 518466, "start": 5194.16, "end": 5196.0599999999995, "text": " we're going to now start to intersperse", "tokens": [50840, 321, 434, 516, 281, 586, 722, 281, 728, 82, 610, 405, 50935], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1271, "seek": 518466, "start": 5196.5599999999995, "end": 5199.36, "text": " the communication with the computation", "tokens": [50960, 264, 6101, 365, 264, 24903, 51100], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1272, "seek": 518466, "start": 5199.66, "end": 5201.76, "text": " and that's also what the transformer does", "tokens": [51115, 293, 300, 311, 611, 437, 264, 31782, 775, 51220], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1273, "seek": 518466, "start": 5202.16, "end": 5205.0599999999995, "text": " when it has blocks that communicate and then compute", "tokens": [51240, 562, 309, 575, 8474, 300, 7890, 293, 550, 14722, 51385], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1274, "seek": 518466, "start": 5205.36, "end": 5207.5599999999995, "text": " and it groups them and replicates them.", "tokens": [51400, 293, 309, 3935, 552, 293, 3248, 299, 1024, 552, 13, 51510], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1275, "seek": 518466, "start": 5208.76, "end": 5210.76, "text": " Okay, so let me show you what we'd like to do.", "tokens": [51570, 1033, 11, 370, 718, 385, 855, 291, 437, 321, 1116, 411, 281, 360, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1276, "seek": 518466, "start": 5211.36, "end": 5212.46, "text": " We'd like to do something like this.", "tokens": [51700, 492, 1116, 411, 281, 360, 746, 411, 341, 13, 51755], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1277, "seek": 518466, "start": 5212.46, "end": 5213.26, "text": " We have a block", "tokens": [51755, 492, 362, 257, 3461, 51795], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1278, "seek": 518466, "start": 5213.66, "end": 5214.5599999999995, "text": " and this block is basically", "tokens": [51815, 293, 341, 3461, 307, 1936, 51860], "temperature": 0.0, "avg_logprob": -0.13405297019264914, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.00034640327794477344}, {"id": 1279, "seek": 521466, "start": 5214.76, "end": 5215.5599999999995, "text": " this part here", "tokens": [50370, 341, 644, 510, 50410], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1280, "seek": 521466, "start": 5216.16, "end": 5217.46, "text": " except for the cross attention.", "tokens": [50440, 3993, 337, 264, 3278, 3202, 13, 50505], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1281, "seek": 521466, "start": 5218.66, "end": 5222.16, "text": " Now the block basically intersperses communication and then computation.", "tokens": [50565, 823, 264, 3461, 1936, 728, 4952, 433, 279, 6101, 293, 550, 24903, 13, 50740], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1282, "seek": 521466, "start": 5222.66, "end": 5225.96, "text": " The computation is done using multi-headed self-attention", "tokens": [50765, 440, 24903, 307, 1096, 1228, 4825, 12, 28409, 2698, 12, 1591, 1251, 50930], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1283, "seek": 521466, "start": 5226.5599999999995, "end": 5229.26, "text": " and then the computation is done using a feed forward network", "tokens": [50960, 293, 550, 264, 24903, 307, 1096, 1228, 257, 3154, 2128, 3209, 51095], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1284, "seek": 521466, "start": 5229.66, "end": 5230.96, "text": " on all the tokens independently.", "tokens": [51115, 322, 439, 264, 22667, 21761, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1285, "seek": 521466, "start": 5232.5599999999995, "end": 5235.5599999999995, "text": " Now what I've added here also is you'll notice", "tokens": [51260, 823, 437, 286, 600, 3869, 510, 611, 307, 291, 603, 3449, 51410], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1286, "seek": 521466, "start": 5237.26, "end": 5239.5599999999995, "text": " this takes the number of embeddings in the embedding dimension", "tokens": [51495, 341, 2516, 264, 1230, 295, 12240, 29432, 294, 264, 12240, 3584, 10139, 51610], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1287, "seek": 521466, "start": 5239.5599999999995, "end": 5241.0599999999995, "text": " and number of heads that we would like", "tokens": [51610, 293, 1230, 295, 8050, 300, 321, 576, 411, 51685], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1288, "seek": 521466, "start": 5241.0599999999995, "end": 5243.5599999999995, "text": " which is kind of like group size in group convolution.", "tokens": [51685, 597, 307, 733, 295, 411, 1594, 2744, 294, 1594, 45216, 13, 51810], "temperature": 0.0, "avg_logprob": -0.1676715021548064, "compression_ratio": 1.8237547892720307, "no_speech_prob": 6.858124834252521e-05}, {"id": 1289, "seek": 524356, "start": 5244.06, "end": 5246.46, "text": " And I'm saying that number of heads we'd like is four", "tokens": [50390, 400, 286, 478, 1566, 300, 1230, 295, 8050, 321, 1116, 411, 307, 1451, 50510], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1290, "seek": 524356, "start": 5246.860000000001, "end": 5248.56, "text": " and so because this is 32", "tokens": [50530, 293, 370, 570, 341, 307, 8858, 50615], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1291, "seek": 524356, "start": 5248.96, "end": 5250.660000000001, "text": " we calculate that because this is 32", "tokens": [50635, 321, 8873, 300, 570, 341, 307, 8858, 50720], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1292, "seek": 524356, "start": 5250.860000000001, "end": 5252.26, "text": " the number of heads should be four", "tokens": [50730, 264, 1230, 295, 8050, 820, 312, 1451, 50800], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1293, "seek": 524356, "start": 5254.06, "end": 5255.46, "text": " the head size should be eight", "tokens": [50890, 264, 1378, 2744, 820, 312, 3180, 50960], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1294, "seek": 524356, "start": 5255.56, "end": 5257.76, "text": " so that everything sort of works out channel wise.", "tokens": [50965, 370, 300, 1203, 1333, 295, 1985, 484, 2269, 10829, 13, 51075], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1295, "seek": 524356, "start": 5258.96, "end": 5260.860000000001, "text": " So this is how the transformer structures", "tokens": [51135, 407, 341, 307, 577, 264, 31782, 9227, 51230], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1296, "seek": 524356, "start": 5261.160000000001, "end": 5263.56, "text": " sort of the sizes typically.", "tokens": [51245, 1333, 295, 264, 11602, 5850, 13, 51365], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1297, "seek": 524356, "start": 5264.360000000001, "end": 5265.56, "text": " So the head size will become eight", "tokens": [51405, 407, 264, 1378, 2744, 486, 1813, 3180, 51465], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1298, "seek": 524356, "start": 5265.660000000001, "end": 5267.360000000001, "text": " and then this is how we want to intersperse them.", "tokens": [51470, 293, 550, 341, 307, 577, 321, 528, 281, 728, 82, 610, 405, 552, 13, 51555], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1299, "seek": 524356, "start": 5267.860000000001, "end": 5270.06, "text": " And then here I'm trying to create blocks", "tokens": [51580, 400, 550, 510, 286, 478, 1382, 281, 1884, 8474, 51690], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1300, "seek": 524356, "start": 5270.160000000001, "end": 5273.360000000001, "text": " which is just a sequential application of block block block.", "tokens": [51695, 597, 307, 445, 257, 42881, 3861, 295, 3461, 3461, 3461, 13, 51855], "temperature": 0.0, "avg_logprob": -0.1619190275669098, "compression_ratio": 1.8598484848484849, "no_speech_prob": 0.0005548187764361501}, {"id": 1301, "seek": 527356, "start": 5273.660000000001, "end": 5276.860000000001, "text": " So that we're interspersing communication feed forward many many times", "tokens": [50370, 407, 300, 321, 434, 728, 4952, 433, 278, 6101, 3154, 2128, 867, 867, 1413, 50530], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1302, "seek": 527356, "start": 5277.06, "end": 5278.860000000001, "text": " and then finally we decode.", "tokens": [50540, 293, 550, 2721, 321, 979, 1429, 13, 50630], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1303, "seek": 527356, "start": 5279.46, "end": 5281.46, "text": " Now actually try to run this", "tokens": [50660, 823, 767, 853, 281, 1190, 341, 50760], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1304, "seek": 527356, "start": 5281.76, "end": 5284.76, "text": " and the problem is this doesn't actually give a very good answer", "tokens": [50775, 293, 264, 1154, 307, 341, 1177, 380, 767, 976, 257, 588, 665, 1867, 50925], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1305, "seek": 527356, "start": 5285.360000000001, "end": 5286.660000000001, "text": " and very good result.", "tokens": [50955, 293, 588, 665, 1874, 13, 51020], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1306, "seek": 527356, "start": 5286.860000000001, "end": 5290.660000000001, "text": " And the reason for that is we're starting to actually get like a pretty deep neural net", "tokens": [51030, 400, 264, 1778, 337, 300, 307, 321, 434, 2891, 281, 767, 483, 411, 257, 1238, 2452, 18161, 2533, 51220], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1307, "seek": 527356, "start": 5291.06, "end": 5293.76, "text": " and deep neural nets suffer from optimization issues.", "tokens": [51240, 293, 2452, 18161, 36170, 9753, 490, 19618, 2663, 13, 51375], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1308, "seek": 527356, "start": 5293.76, "end": 5296.46, "text": " And I think that's what we're kind of like slightly starting to run into.", "tokens": [51375, 400, 286, 519, 300, 311, 437, 321, 434, 733, 295, 411, 4748, 2891, 281, 1190, 666, 13, 51510], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1309, "seek": 527356, "start": 5296.76, "end": 5299.56, "text": " So we need one more idea that we can borrow from the", "tokens": [51525, 407, 321, 643, 472, 544, 1558, 300, 321, 393, 11172, 490, 264, 51665], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1310, "seek": 527356, "start": 5300.360000000001, "end": 5302.56, "text": " transformer paper to resolve those difficulties.", "tokens": [51705, 31782, 3035, 281, 14151, 729, 14399, 13, 51815], "temperature": 0.0, "avg_logprob": -0.15297503838172324, "compression_ratio": 1.7972972972972974, "no_speech_prob": 9.729008161230013e-05}, {"id": 1311, "seek": 530256, "start": 5302.56, "end": 5305.660000000001, "text": " Now there are two optimizations that dramatically help", "tokens": [50365, 823, 456, 366, 732, 5028, 14455, 300, 17548, 854, 50520], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1312, "seek": 530256, "start": 5305.76, "end": 5307.06, "text": " with the depth of these networks", "tokens": [50525, 365, 264, 7161, 295, 613, 9590, 50590], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1313, "seek": 530256, "start": 5307.360000000001, "end": 5309.96, "text": " and make sure that the networks remain optimizable.", "tokens": [50605, 293, 652, 988, 300, 264, 9590, 6222, 5028, 22395, 13, 50735], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1314, "seek": 530256, "start": 5310.26, "end": 5311.26, "text": " Let's talk about the first one.", "tokens": [50750, 961, 311, 751, 466, 264, 700, 472, 13, 50800], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1315, "seek": 530256, "start": 5311.96, "end": 5314.56, "text": " The first one in this diagram is you see this arrow here", "tokens": [50835, 440, 700, 472, 294, 341, 10686, 307, 291, 536, 341, 11610, 510, 50965], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1316, "seek": 530256, "start": 5315.360000000001, "end": 5317.360000000001, "text": " and then this arrow and this arrow.", "tokens": [51005, 293, 550, 341, 11610, 293, 341, 11610, 13, 51105], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1317, "seek": 530256, "start": 5317.76, "end": 5320.76, "text": " Those are skip connections or sometimes called residual connections.", "tokens": [51125, 3950, 366, 10023, 9271, 420, 2171, 1219, 27980, 9271, 13, 51275], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1318, "seek": 530256, "start": 5321.56, "end": 5322.56, "text": " They come from this paper", "tokens": [51315, 814, 808, 490, 341, 3035, 51365], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1319, "seek": 530256, "start": 5323.46, "end": 5326.860000000001, "text": " the procedural learning for image recognition from about 2015", "tokens": [51410, 264, 43951, 2539, 337, 3256, 11150, 490, 466, 7546, 51580], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1320, "seek": 530256, "start": 5327.76, "end": 5329.160000000001, "text": " that introduced the concept.", "tokens": [51625, 300, 7268, 264, 3410, 13, 51695], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1321, "seek": 530256, "start": 5329.96, "end": 5332.360000000001, "text": " Now these are basically what it means", "tokens": [51735, 823, 613, 366, 1936, 437, 309, 1355, 51855], "temperature": 0.0, "avg_logprob": -0.18535215514046804, "compression_ratio": 1.8007380073800738, "no_speech_prob": 0.0002856200735550374}, {"id": 1322, "seek": 533256, "start": 5332.56, "end": 5334.26, "text": " is you transform the data,", "tokens": [50365, 307, 291, 4088, 264, 1412, 11, 50450], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1323, "seek": 533256, "start": 5334.46, "end": 5336.860000000001, "text": " but then you have a skip connection with addition", "tokens": [50460, 457, 550, 291, 362, 257, 10023, 4984, 365, 4500, 50580], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1324, "seek": 533256, "start": 5337.46, "end": 5338.76, "text": " from the previous features.", "tokens": [50610, 490, 264, 3894, 4122, 13, 50675], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1325, "seek": 533256, "start": 5339.360000000001, "end": 5340.96, "text": " Now the way I like to visualize it", "tokens": [50705, 823, 264, 636, 286, 411, 281, 23273, 309, 50785], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1326, "seek": 533256, "start": 5341.660000000001, "end": 5342.46, "text": " that I prefer", "tokens": [50820, 300, 286, 4382, 50860], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1327, "seek": 533256, "start": 5342.96, "end": 5343.76, "text": " is the following.", "tokens": [50885, 307, 264, 3480, 13, 50925], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1328, "seek": 533256, "start": 5344.26, "end": 5346.96, "text": " Here the computation happens from the top to bottom", "tokens": [50950, 1692, 264, 24903, 2314, 490, 264, 1192, 281, 2767, 51085], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1329, "seek": 533256, "start": 5347.56, "end": 5350.46, "text": " and basically you have this residual pathway", "tokens": [51115, 293, 1936, 291, 362, 341, 27980, 18590, 51260], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1330, "seek": 533256, "start": 5351.06, "end": 5353.46, "text": " and you are free to fork off from the residual pathway,", "tokens": [51290, 293, 291, 366, 1737, 281, 17716, 766, 490, 264, 27980, 18590, 11, 51410], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1331, "seek": 533256, "start": 5353.46, "end": 5354.660000000001, "text": " perform some computation", "tokens": [51410, 2042, 512, 24903, 51470], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1332, "seek": 533256, "start": 5354.96, "end": 5357.76, "text": " and then project back to the residual pathway via addition.", "tokens": [51485, 293, 550, 1716, 646, 281, 264, 27980, 18590, 5766, 4500, 13, 51625], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1333, "seek": 533256, "start": 5358.56, "end": 5359.76, "text": " And so you go from the", "tokens": [51665, 400, 370, 291, 352, 490, 264, 51725], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1334, "seek": 533256, "start": 5360.56, "end": 5362.46, "text": " the inputs to the targets", "tokens": [51765, 264, 15743, 281, 264, 12911, 51860], "temperature": 0.0, "avg_logprob": -0.1601943306300951, "compression_ratio": 1.854251012145749, "no_speech_prob": 0.0005625144112855196}, {"id": 1335, "seek": 536256, "start": 5362.660000000001, "end": 5364.56, "text": " only via plus and plus and plus.", "tokens": [50370, 787, 5766, 1804, 293, 1804, 293, 1804, 13, 50465], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1336, "seek": 536256, "start": 5365.46, "end": 5367.76, "text": " And the reason this is useful is because during dot propagation", "tokens": [50510, 400, 264, 1778, 341, 307, 4420, 307, 570, 1830, 5893, 38377, 50625], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1337, "seek": 536256, "start": 5367.76, "end": 5370.660000000001, "text": " remember from our micrograd video earlier", "tokens": [50625, 1604, 490, 527, 4532, 7165, 960, 3071, 50770], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1338, "seek": 536256, "start": 5371.06, "end": 5374.660000000001, "text": " addition distributes gradients equally to both of its branches", "tokens": [50790, 4500, 4400, 1819, 2771, 2448, 12309, 281, 1293, 295, 1080, 14770, 50970], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1339, "seek": 536256, "start": 5375.26, "end": 5376.56, "text": " that fed as the input.", "tokens": [51000, 300, 4636, 382, 264, 4846, 13, 51065], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1340, "seek": 536256, "start": 5377.06, "end": 5380.96, "text": " And so the supervision or the gradients from the loss", "tokens": [51090, 400, 370, 264, 32675, 420, 264, 2771, 2448, 490, 264, 4470, 51285], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1341, "seek": 536256, "start": 5381.360000000001, "end": 5384.46, "text": " basically hop through every addition node", "tokens": [51305, 1936, 3818, 807, 633, 4500, 9984, 51460], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1342, "seek": 536256, "start": 5384.76, "end": 5386.26, "text": " all the way to the input", "tokens": [51475, 439, 264, 636, 281, 264, 4846, 51550], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1343, "seek": 536256, "start": 5386.76, "end": 5389.96, "text": " and then also fork off into the residual blocks.", "tokens": [51575, 293, 550, 611, 17716, 766, 666, 264, 27980, 8474, 13, 51735], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1344, "seek": 536256, "start": 5391.26, "end": 5392.360000000001, "text": " But basically you have this", "tokens": [51800, 583, 1936, 291, 362, 341, 51855], "temperature": 0.0, "avg_logprob": -0.18695766486010504, "compression_ratio": 1.780590717299578, "no_speech_prob": 0.0006569006945937872}, {"id": 1345, "seek": 539236, "start": 5392.36, "end": 5395.5599999999995, "text": " gradient superhighway that goes directly from the supervision", "tokens": [50365, 16235, 1687, 21454, 676, 300, 1709, 3838, 490, 264, 32675, 50525], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1346, "seek": 539236, "start": 5395.759999999999, "end": 5397.66, "text": " all the way to the input unimpeded.", "tokens": [50535, 439, 264, 636, 281, 264, 4846, 517, 332, 3452, 292, 13, 50630], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1347, "seek": 539236, "start": 5398.36, "end": 5401.0599999999995, "text": " And then these residual blocks are usually initialized in the beginning.", "tokens": [50665, 400, 550, 613, 27980, 8474, 366, 2673, 5883, 1602, 294, 264, 2863, 13, 50800], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1348, "seek": 539236, "start": 5401.36, "end": 5404.36, "text": " So they contribute very very little if anything to the residual pathway.", "tokens": [50815, 407, 436, 10586, 588, 588, 707, 498, 1340, 281, 264, 27980, 18590, 13, 50965], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1349, "seek": 539236, "start": 5404.759999999999, "end": 5406.36, "text": " They are initialized that way.", "tokens": [50985, 814, 366, 5883, 1602, 300, 636, 13, 51065], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1350, "seek": 539236, "start": 5406.759999999999, "end": 5409.759999999999, "text": " So in the beginning they are sort of almost kind of like not there.", "tokens": [51085, 407, 294, 264, 2863, 436, 366, 1333, 295, 1920, 733, 295, 411, 406, 456, 13, 51235], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1351, "seek": 539236, "start": 5410.16, "end": 5413.5599999999995, "text": " But then during the optimization they come online over time", "tokens": [51255, 583, 550, 1830, 264, 19618, 436, 808, 2950, 670, 565, 51425], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1352, "seek": 539236, "start": 5414.16, "end": 5415.759999999999, "text": " and they start to contribute", "tokens": [51455, 293, 436, 722, 281, 10586, 51535], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1353, "seek": 539236, "start": 5416.36, "end": 5418.0599999999995, "text": " but at least at the initialization", "tokens": [51565, 457, 412, 1935, 412, 264, 5883, 2144, 51650], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1354, "seek": 539236, "start": 5418.259999999999, "end": 5420.46, "text": " you can go from directly supervision to the input", "tokens": [51660, 291, 393, 352, 490, 3838, 32675, 281, 264, 4846, 51770], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1355, "seek": 539236, "start": 5420.96, "end": 5422.259999999999, "text": " gradient is unimpeded and just flows.", "tokens": [51795, 16235, 307, 517, 332, 3452, 292, 293, 445, 12867, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1676003448945239, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0012681175721809268}, {"id": 1356, "seek": 542236, "start": 5422.86, "end": 5425.16, "text": " And then the blocks over time kick in.", "tokens": [50390, 400, 550, 264, 8474, 670, 565, 4437, 294, 13, 50505], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1357, "seek": 542236, "start": 5425.759999999999, "end": 5428.36, "text": " And so that dramatically helps with the optimization.", "tokens": [50535, 400, 370, 300, 17548, 3665, 365, 264, 19618, 13, 50665], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1358, "seek": 542236, "start": 5428.66, "end": 5429.5599999999995, "text": " So let's implement this.", "tokens": [50680, 407, 718, 311, 4445, 341, 13, 50725], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1359, "seek": 542236, "start": 5429.86, "end": 5431.16, "text": " So coming back to our block here.", "tokens": [50740, 407, 1348, 646, 281, 527, 3461, 510, 13, 50805], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1360, "seek": 542236, "start": 5431.5599999999995, "end": 5432.96, "text": " Basically what we want to do is", "tokens": [50825, 8537, 437, 321, 528, 281, 360, 307, 50895], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1361, "seek": 542236, "start": 5433.5599999999995, "end": 5435.5599999999995, "text": " we want to do x equals x plus", "tokens": [50925, 321, 528, 281, 360, 2031, 6915, 2031, 1804, 51025], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1362, "seek": 542236, "start": 5436.5599999999995, "end": 5439.5599999999995, "text": " self-attention and x equals x plus self.feedforward.", "tokens": [51075, 2698, 12, 1591, 1251, 293, 2031, 6915, 2031, 1804, 2698, 13, 37036, 13305, 13, 51225], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1363, "seek": 542236, "start": 5440.759999999999, "end": 5445.46, "text": " So this is x and then we fork off and do some communication and come back", "tokens": [51285, 407, 341, 307, 2031, 293, 550, 321, 17716, 766, 293, 360, 512, 6101, 293, 808, 646, 51520], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1364, "seek": 542236, "start": 5445.759999999999, "end": 5448.16, "text": " and we fork off and we do some computation and come back.", "tokens": [51535, 293, 321, 17716, 766, 293, 321, 360, 512, 24903, 293, 808, 646, 13, 51655], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1365, "seek": 542236, "start": 5448.96, "end": 5450.36, "text": " So those are residual connections", "tokens": [51695, 407, 729, 366, 27980, 9271, 51765], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1366, "seek": 542236, "start": 5450.86, "end": 5452.259999999999, "text": " and then swinging back up here.", "tokens": [51790, 293, 550, 29500, 646, 493, 510, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1592712860107422, "compression_ratio": 1.8938775510204082, "no_speech_prob": 0.00048728613182902336}, {"id": 1367, "seek": 545236, "start": 5452.46, "end": 5455.0599999999995, "text": " We also have to introduce this projection.", "tokens": [50370, 492, 611, 362, 281, 5366, 341, 22743, 13, 50500], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1368, "seek": 545236, "start": 5455.96, "end": 5457.0599999999995, "text": " So nn.linear", "tokens": [50545, 407, 297, 77, 13, 28263, 50600], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1369, "seek": 545236, "start": 5458.5599999999995, "end": 5460.86, "text": " and this is going to be from", "tokens": [50675, 293, 341, 307, 516, 281, 312, 490, 50790], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1370, "seek": 545236, "start": 5461.86, "end": 5463.0599999999995, "text": " after we concatenate this.", "tokens": [50840, 934, 321, 1588, 7186, 473, 341, 13, 50900], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1371, "seek": 545236, "start": 5463.0599999999995, "end": 5464.46, "text": " This is the size and embed.", "tokens": [50900, 639, 307, 264, 2744, 293, 12240, 13, 50970], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1372, "seek": 545236, "start": 5464.96, "end": 5467.259999999999, "text": " So this is the output of the self-attention itself.", "tokens": [50995, 407, 341, 307, 264, 5598, 295, 264, 2698, 12, 1591, 1251, 2564, 13, 51110], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1373, "seek": 545236, "start": 5467.96, "end": 5471.259999999999, "text": " But then we actually want the to apply the projection", "tokens": [51145, 583, 550, 321, 767, 528, 264, 281, 3079, 264, 22743, 51310], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1374, "seek": 545236, "start": 5472.259999999999, "end": 5473.16, "text": " and that's the result.", "tokens": [51360, 293, 300, 311, 264, 1874, 13, 51405], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1375, "seek": 545236, "start": 5474.36, "end": 5477.259999999999, "text": " So the projection is just a linear transformation of the outcome of this layer.", "tokens": [51465, 407, 264, 22743, 307, 445, 257, 8213, 9887, 295, 264, 9700, 295, 341, 4583, 13, 51610], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1376, "seek": 545236, "start": 5478.86, "end": 5481.16, "text": " So that's the projection back into the residual pathway.", "tokens": [51690, 407, 300, 311, 264, 22743, 646, 666, 264, 27980, 18590, 13, 51805], "temperature": 0.0, "avg_logprob": -0.17608298911704673, "compression_ratio": 1.8080357142857142, "no_speech_prob": 0.00040772539796307683}, {"id": 1377, "seek": 548116, "start": 5481.86, "end": 5483.16, "text": " And then here in a feedforward,", "tokens": [50400, 400, 550, 510, 294, 257, 3154, 13305, 11, 50465], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1378, "seek": 548116, "start": 5483.26, "end": 5484.46, "text": " it's going to be the same thing.", "tokens": [50470, 309, 311, 516, 281, 312, 264, 912, 551, 13, 50530], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1379, "seek": 548116, "start": 5484.96, "end": 5487.26, "text": " I could have a self.projection here as well.", "tokens": [50555, 286, 727, 362, 257, 2698, 13, 4318, 1020, 313, 510, 382, 731, 13, 50670], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1380, "seek": 548116, "start": 5487.5599999999995, "end": 5488.96, "text": " But let me just simplify it", "tokens": [50685, 583, 718, 385, 445, 20460, 309, 50755], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1381, "seek": 548116, "start": 5489.66, "end": 5490.66, "text": " and let me", "tokens": [50790, 293, 718, 385, 50840], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1382, "seek": 548116, "start": 5492.0599999999995, "end": 5494.0599999999995, "text": " couple it inside the same sequential container.", "tokens": [50910, 1916, 309, 1854, 264, 912, 42881, 10129, 13, 51010], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1383, "seek": 548116, "start": 5494.76, "end": 5497.86, "text": " And so this is the projection layer going back into the residual pathway.", "tokens": [51045, 400, 370, 341, 307, 264, 22743, 4583, 516, 646, 666, 264, 27980, 18590, 13, 51200], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1384, "seek": 548116, "start": 5499.16, "end": 5500.16, "text": " And so", "tokens": [51265, 400, 370, 51315], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1385, "seek": 548116, "start": 5500.96, "end": 5502.36, "text": " that's well, that's it.", "tokens": [51355, 300, 311, 731, 11, 300, 311, 309, 13, 51425], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1386, "seek": 548116, "start": 5502.66, "end": 5503.5599999999995, "text": " So now we can train this.", "tokens": [51440, 407, 586, 321, 393, 3847, 341, 13, 51485], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1387, "seek": 548116, "start": 5503.76, "end": 5505.36, "text": " So I implemented one more small change.", "tokens": [51495, 407, 286, 12270, 472, 544, 1359, 1319, 13, 51575], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1388, "seek": 548116, "start": 5505.96, "end": 5508.16, "text": " When you look into the paper again,", "tokens": [51605, 1133, 291, 574, 666, 264, 3035, 797, 11, 51715], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1389, "seek": 548116, "start": 5508.36, "end": 5510.96, "text": " you see that the dimensionality of input and output", "tokens": [51725, 291, 536, 300, 264, 10139, 1860, 295, 4846, 293, 5598, 51855], "temperature": 0.0, "avg_logprob": -0.17426642206788973, "compression_ratio": 1.7169811320754718, "no_speech_prob": 0.00016710090858396143}, {"id": 1390, "seek": 551116, "start": 5511.16, "end": 5512.46, "text": " is 512 for them.", "tokens": [50365, 307, 1025, 4762, 337, 552, 13, 50430], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1391, "seek": 551116, "start": 5512.76, "end": 5515.16, "text": " And they're saying that the inner layer here in the feedforward", "tokens": [50445, 400, 436, 434, 1566, 300, 264, 7284, 4583, 510, 294, 264, 3154, 13305, 50565], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1392, "seek": 551116, "start": 5515.16, "end": 5516.86, "text": " has dimensionality of 2048.", "tokens": [50565, 575, 10139, 1860, 295, 945, 13318, 13, 50650], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1393, "seek": 551116, "start": 5517.16, "end": 5518.66, "text": " So there's a multiplier of 4.", "tokens": [50665, 407, 456, 311, 257, 44106, 295, 1017, 13, 50740], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1394, "seek": 551116, "start": 5519.36, "end": 5522.0599999999995, "text": " And so the inner layer of the feedforward network", "tokens": [50775, 400, 370, 264, 7284, 4583, 295, 264, 3154, 13305, 3209, 50910], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1395, "seek": 551116, "start": 5522.76, "end": 5524.96, "text": " should be multiplied by 4 in terms of channel sizes.", "tokens": [50945, 820, 312, 17207, 538, 1017, 294, 2115, 295, 2269, 11602, 13, 51055], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1396, "seek": 551116, "start": 5525.16, "end": 5527.5599999999995, "text": " So I came here and I multiplied 4 times embed", "tokens": [51065, 407, 286, 1361, 510, 293, 286, 17207, 1017, 1413, 12240, 51185], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1397, "seek": 551116, "start": 5527.86, "end": 5529.26, "text": " here for the feedforward", "tokens": [51200, 510, 337, 264, 3154, 13305, 51270], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1398, "seek": 551116, "start": 5529.66, "end": 5532.66, "text": " and then from 4 times an embed coming back down to an embed", "tokens": [51290, 293, 550, 490, 1017, 1413, 364, 12240, 1348, 646, 760, 281, 364, 12240, 51440], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1399, "seek": 551116, "start": 5532.86, "end": 5535.0599999999995, "text": " when we go back to the projection.", "tokens": [51450, 562, 321, 352, 646, 281, 264, 22743, 13, 51560], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1400, "seek": 551116, "start": 5535.36, "end": 5538.46, "text": " So adding a bit of computation here and growing that layer", "tokens": [51575, 407, 5127, 257, 857, 295, 24903, 510, 293, 4194, 300, 4583, 51730], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1401, "seek": 551116, "start": 5538.66, "end": 5540.66, "text": " that is in the residual block on the side", "tokens": [51740, 300, 307, 294, 264, 27980, 3461, 322, 264, 1252, 51840], "temperature": 0.0, "avg_logprob": -0.12088218744653854, "compression_ratio": 1.8745387453874538, "no_speech_prob": 0.0001597769296495244}, {"id": 1402, "seek": 554066, "start": 5540.66, "end": 5542.0599999999995, "text": " of the residual pathway.", "tokens": [50365, 295, 264, 27980, 18590, 13, 50435], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1403, "seek": 554066, "start": 5543.16, "end": 5545.96, "text": " And then I train this and we actually get down all the way to", "tokens": [50490, 400, 550, 286, 3847, 341, 293, 321, 767, 483, 760, 439, 264, 636, 281, 50630], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1404, "seek": 554066, "start": 5546.26, "end": 5548.0599999999995, "text": " 2.08 validation loss.", "tokens": [50645, 568, 13, 16133, 24071, 4470, 13, 50735], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1405, "seek": 554066, "start": 5548.26, "end": 5550.46, "text": " And we also see that network is starting to get big enough", "tokens": [50745, 400, 321, 611, 536, 300, 3209, 307, 2891, 281, 483, 955, 1547, 50855], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1406, "seek": 554066, "start": 5550.76, "end": 5553.0599999999995, "text": " that our train loss is getting ahead of validation loss.", "tokens": [50870, 300, 527, 3847, 4470, 307, 1242, 2286, 295, 24071, 4470, 13, 50985], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1407, "seek": 554066, "start": 5553.0599999999995, "end": 5555.16, "text": " So we started to see like a little bit of overfitting", "tokens": [50985, 407, 321, 1409, 281, 536, 411, 257, 707, 857, 295, 670, 69, 2414, 51090], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1408, "seek": 554066, "start": 5556.16, "end": 5556.66, "text": " and", "tokens": [51140, 293, 51165], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1409, "seek": 554066, "start": 5557.0599999999995, "end": 5557.36, "text": " our", "tokens": [51185, 527, 51200], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1410, "seek": 554066, "start": 5557.66, "end": 5558.0599999999995, "text": " our", "tokens": [51215, 527, 51235], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1411, "seek": 554066, "start": 5560.0599999999995, "end": 5561.66, "text": " generations here are still not amazing.", "tokens": [51335, 10593, 510, 366, 920, 406, 2243, 13, 51415], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1412, "seek": 554066, "start": 5561.66, "end": 5565.86, "text": " But at least you see that we can see like is here this now grief sync", "tokens": [51415, 583, 412, 1935, 291, 536, 300, 321, 393, 536, 411, 307, 510, 341, 586, 18998, 20271, 51625], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1413, "seek": 554066, "start": 5566.5599999999995, "end": 5568.66, "text": " like this starts to almost look like English.", "tokens": [51660, 411, 341, 3719, 281, 1920, 574, 411, 3669, 13, 51765], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1414, "seek": 554066, "start": 5568.96, "end": 5569.46, "text": " So", "tokens": [51780, 407, 51805], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1415, "seek": 554066, "start": 5570.0599999999995, "end": 5570.16, "text": " yeah,", "tokens": [51835, 1338, 11, 51840], "temperature": 0.0, "avg_logprob": -0.17753197426019712, "compression_ratio": 1.75, "no_speech_prob": 0.000336428580339998}, {"id": 1416, "seek": 557016, "start": 5570.16, "end": 5571.16, "text": " we're starting to really get there.", "tokens": [50365, 321, 434, 2891, 281, 534, 483, 456, 13, 50415], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1417, "seek": 557016, "start": 5571.66, "end": 5571.86, "text": " Okay.", "tokens": [50440, 1033, 13, 50450], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1418, "seek": 557016, "start": 5571.86, "end": 5574.86, "text": " And the second innovation that is very helpful for optimizing very deep", "tokens": [50450, 400, 264, 1150, 8504, 300, 307, 588, 4961, 337, 40425, 588, 2452, 50600], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1419, "seek": 557016, "start": 5574.86, "end": 5576.36, "text": " neural networks is right here.", "tokens": [50600, 18161, 9590, 307, 558, 510, 13, 50675], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1420, "seek": 557016, "start": 5576.86, "end": 5579.0599999999995, "text": " So we have this addition now that's the residual part.", "tokens": [50700, 407, 321, 362, 341, 4500, 586, 300, 311, 264, 27980, 644, 13, 50810], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1421, "seek": 557016, "start": 5579.26, "end": 5581.76, "text": " But this norm is referring to something called layer norm.", "tokens": [50820, 583, 341, 2026, 307, 13761, 281, 746, 1219, 4583, 2026, 13, 50945], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1422, "seek": 557016, "start": 5582.46, "end": 5584.36, "text": " So layer norm is implemented in pytorch.", "tokens": [50980, 407, 4583, 2026, 307, 12270, 294, 25878, 284, 339, 13, 51075], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1423, "seek": 557016, "start": 5584.36, "end": 5587.36, "text": " It's a paper that came out a while back here.", "tokens": [51075, 467, 311, 257, 3035, 300, 1361, 484, 257, 1339, 646, 510, 13, 51225], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1424, "seek": 557016, "start": 5590.16, "end": 5592.26, "text": " And layer norm is very very similar to bash norm.", "tokens": [51365, 400, 4583, 2026, 307, 588, 588, 2531, 281, 46183, 2026, 13, 51470], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1425, "seek": 557016, "start": 5592.66, "end": 5595.86, "text": " So remember back to our make more series part three.", "tokens": [51490, 407, 1604, 646, 281, 527, 652, 544, 2638, 644, 1045, 13, 51650], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1426, "seek": 557016, "start": 5596.26, "end": 5600.0599999999995, "text": " We implemented bash normalization and bash normalization basically just", "tokens": [51670, 492, 12270, 46183, 2710, 2144, 293, 46183, 2710, 2144, 1936, 445, 51860], "temperature": 0.0, "avg_logprob": -0.17537714151235728, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.00015050942602101713}, {"id": 1427, "seek": 560006, "start": 5600.06, "end": 5603.96, "text": " made sure that across the batch dimension.", "tokens": [50365, 1027, 988, 300, 2108, 264, 15245, 10139, 13, 50560], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1428, "seek": 560006, "start": 5604.160000000001, "end": 5609.96, "text": " Any individual neuron had unit Gaussian distribution.", "tokens": [50570, 2639, 2609, 34090, 632, 4985, 39148, 7316, 13, 50860], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1429, "seek": 560006, "start": 5610.26, "end": 5614.26, "text": " So it was zero mean and unit standard deviation one standard deviation", "tokens": [50875, 407, 309, 390, 4018, 914, 293, 4985, 3832, 25163, 472, 3832, 25163, 51075], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1430, "seek": 560006, "start": 5614.46, "end": 5614.96, "text": " output.", "tokens": [51085, 5598, 13, 51110], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1431, "seek": 560006, "start": 5615.860000000001, "end": 5619.26, "text": " So what I did here is I'm copy pasting the bathroom 1D that we developed", "tokens": [51155, 407, 437, 286, 630, 510, 307, 286, 478, 5055, 1791, 278, 264, 8687, 502, 35, 300, 321, 4743, 51325], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1432, "seek": 560006, "start": 5619.26, "end": 5623.96, "text": " in our make more series and see here we can initialize for example this", "tokens": [51325, 294, 527, 652, 544, 2638, 293, 536, 510, 321, 393, 5883, 1125, 337, 1365, 341, 51560], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1433, "seek": 560006, "start": 5623.96, "end": 5628.660000000001, "text": " module and we can have a batch of 32 100 dimensional vectors feeding through", "tokens": [51560, 10088, 293, 321, 393, 362, 257, 15245, 295, 8858, 2319, 18795, 18875, 12919, 807, 51795], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1434, "seek": 560006, "start": 5628.660000000001, "end": 5629.46, "text": " the bathroom layer.", "tokens": [51795, 264, 8687, 4583, 13, 51835], "temperature": 0.0, "avg_logprob": -0.21115696186922034, "compression_ratio": 1.6613545816733069, "no_speech_prob": 5.656447319779545e-05}, {"id": 1435, "seek": 563006, "start": 5630.160000000001, "end": 5635.76, "text": " So what this does is it guarantees that when we look at just the 0th column,", "tokens": [50370, 407, 437, 341, 775, 307, 309, 32567, 300, 562, 321, 574, 412, 445, 264, 1958, 392, 7738, 11, 50650], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1436, "seek": 563006, "start": 5636.360000000001, "end": 5639.26, "text": " it's a zero mean one standard deviation.", "tokens": [50680, 309, 311, 257, 4018, 914, 472, 3832, 25163, 13, 50825], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1437, "seek": 563006, "start": 5639.76, "end": 5642.96, "text": " So it's normalizing every single column of this input.", "tokens": [50850, 407, 309, 311, 2710, 3319, 633, 2167, 7738, 295, 341, 4846, 13, 51010], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1438, "seek": 563006, "start": 5643.860000000001, "end": 5648.06, "text": " Now the rows are not going to be normalized by default because we're just", "tokens": [51055, 823, 264, 13241, 366, 406, 516, 281, 312, 48704, 538, 7576, 570, 321, 434, 445, 51265], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1439, "seek": 563006, "start": 5648.06, "end": 5649.06, "text": " normalizing columns.", "tokens": [51265, 2710, 3319, 13766, 13, 51315], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1440, "seek": 563006, "start": 5649.660000000001, "end": 5651.06, "text": " So let's not implement layer norm.", "tokens": [51345, 407, 718, 311, 406, 4445, 4583, 2026, 13, 51415], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1441, "seek": 563006, "start": 5651.96, "end": 5653.06, "text": " It's very complicated.", "tokens": [51460, 467, 311, 588, 6179, 13, 51515], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1442, "seek": 563006, "start": 5653.160000000001, "end": 5654.860000000001, "text": " Look we come here.", "tokens": [51520, 2053, 321, 808, 510, 13, 51605], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1443, "seek": 563006, "start": 5655.06, "end": 5659.56, "text": " We change this from 0 to 1 so we don't normalize the columns.", "tokens": [51615, 492, 1319, 341, 490, 1958, 281, 502, 370, 321, 500, 380, 2710, 1125, 264, 13766, 13, 51840], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1444, "seek": 563006, "start": 5659.56, "end": 5659.96, "text": " We normalize.", "tokens": [51840, 492, 2710, 1125, 13, 51860], "temperature": 0.0, "avg_logprob": -0.16794393816564837, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.975327854277566e-05}, {"id": 1445, "seek": 566006, "start": 5660.160000000001, "end": 5663.660000000001, "text": " The rows and now we've implemented layer norm.", "tokens": [50370, 440, 13241, 293, 586, 321, 600, 12270, 4583, 2026, 13, 50545], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1446, "seek": 566006, "start": 5665.06, "end": 5668.26, "text": " So now the columns are not going to be normalized.", "tokens": [50615, 407, 586, 264, 13766, 366, 406, 516, 281, 312, 48704, 13, 50775], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1447, "seek": 566006, "start": 5669.96, "end": 5673.76, "text": " But the rows are going to be normalized for every individual example.", "tokens": [50860, 583, 264, 13241, 366, 516, 281, 312, 48704, 337, 633, 2609, 1365, 13, 51050], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1448, "seek": 566006, "start": 5673.76, "end": 5678.46, "text": " It's 100 dimensional vector is normalized in this way and because our", "tokens": [51050, 467, 311, 2319, 18795, 8062, 307, 48704, 294, 341, 636, 293, 570, 527, 51285], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1449, "seek": 566006, "start": 5678.46, "end": 5683.360000000001, "text": " computation now does not span across examples, we can delete all of this", "tokens": [51285, 24903, 586, 775, 406, 16174, 2108, 5110, 11, 321, 393, 12097, 439, 295, 341, 51530], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1450, "seek": 566006, "start": 5683.360000000001, "end": 5688.96, "text": " buffers stuff because we can always apply this operation and don't need to", "tokens": [51530, 9204, 433, 1507, 570, 321, 393, 1009, 3079, 341, 6916, 293, 500, 380, 643, 281, 51810], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1451, "seek": 566006, "start": 5688.96, "end": 5689.96, "text": " maintain any running buffers.", "tokens": [51810, 6909, 604, 2614, 9204, 433, 13, 51860], "temperature": 0.0, "avg_logprob": -0.16335859003755235, "compression_ratio": 1.6869918699186992, "no_speech_prob": 7.02644611010328e-05}, {"id": 1452, "seek": 569006, "start": 5690.660000000001, "end": 5692.360000000001, "text": " So we don't need the buffers.", "tokens": [50395, 407, 321, 500, 380, 643, 264, 9204, 433, 13, 50480], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1453, "seek": 569006, "start": 5693.360000000001, "end": 5697.46, "text": " We don't there's no distinction between training and test time.", "tokens": [50530, 492, 500, 380, 456, 311, 572, 16844, 1296, 3097, 293, 1500, 565, 13, 50735], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1454, "seek": 569006, "start": 5699.46, "end": 5701.46, "text": " And we don't need these running buffers.", "tokens": [50835, 400, 321, 500, 380, 643, 613, 2614, 9204, 433, 13, 50935], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1455, "seek": 569006, "start": 5701.76, "end": 5703.360000000001, "text": " We do keep gamma and beta.", "tokens": [50950, 492, 360, 1066, 15546, 293, 9861, 13, 51030], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1456, "seek": 569006, "start": 5703.660000000001, "end": 5704.860000000001, "text": " We don't need the momentum.", "tokens": [51045, 492, 500, 380, 643, 264, 11244, 13, 51105], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1457, "seek": 569006, "start": 5704.860000000001, "end": 5706.56, "text": " We don't care if it's training or not.", "tokens": [51105, 492, 500, 380, 1127, 498, 309, 311, 3097, 420, 406, 13, 51190], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1458, "seek": 569006, "start": 5707.360000000001, "end": 5713.160000000001, "text": " And this is now a layer norm and it normalizes the ropes instead of the", "tokens": [51230, 400, 341, 307, 586, 257, 4583, 2026, 293, 309, 2710, 5660, 264, 32964, 2602, 295, 264, 51520], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1459, "seek": 569006, "start": 5713.160000000001, "end": 5718.46, "text": " columns and this here is identical to basically this here.", "tokens": [51520, 13766, 293, 341, 510, 307, 14800, 281, 1936, 341, 510, 13, 51785], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1460, "seek": 569006, "start": 5719.46, "end": 5719.96, "text": " So let's.", "tokens": [51835, 407, 718, 311, 13, 51860], "temperature": 0.0, "avg_logprob": -0.13149608506096733, "compression_ratio": 1.8, "no_speech_prob": 9.863634477369487e-05}, {"id": 1461, "seek": 571996, "start": 5719.96, "end": 5723.36, "text": " Now implement layer norm in our transformer before I incorporate the", "tokens": [50365, 823, 4445, 4583, 2026, 294, 527, 31782, 949, 286, 16091, 264, 50535], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1462, "seek": 571996, "start": 5723.36, "end": 5723.76, "text": " layer norm.", "tokens": [50535, 4583, 2026, 13, 50555], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1463, "seek": 571996, "start": 5723.76, "end": 5727.66, "text": " I just wanted to note that as I said very few details about the transformer", "tokens": [50555, 286, 445, 1415, 281, 3637, 300, 382, 286, 848, 588, 1326, 4365, 466, 264, 31782, 50750], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1464, "seek": 571996, "start": 5727.66, "end": 5730.46, "text": " have changed in the last five years, but this is actually something that's", "tokens": [50750, 362, 3105, 294, 264, 1036, 1732, 924, 11, 457, 341, 307, 767, 746, 300, 311, 50890], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1465, "seek": 571996, "start": 5730.46, "end": 5732.06, "text": " likely departs from the original paper.", "tokens": [50890, 3700, 9110, 82, 490, 264, 3380, 3035, 13, 50970], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1466, "seek": 571996, "start": 5732.56, "end": 5736.36, "text": " You see that the ad and norm is applied after the transformation.", "tokens": [50995, 509, 536, 300, 264, 614, 293, 2026, 307, 6456, 934, 264, 9887, 13, 51185], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1467, "seek": 571996, "start": 5737.36, "end": 5743.56, "text": " But now it is a bit more basically common to apply the layer norm before", "tokens": [51235, 583, 586, 309, 307, 257, 857, 544, 1936, 2689, 281, 3079, 264, 4583, 2026, 949, 51545], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1468, "seek": 571996, "start": 5743.56, "end": 5744.36, "text": " the transformation.", "tokens": [51545, 264, 9887, 13, 51585], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1469, "seek": 571996, "start": 5744.36, "end": 5746.16, "text": " So there's a reshuffling of the layer norms.", "tokens": [51585, 407, 456, 311, 257, 725, 71, 1245, 1688, 295, 264, 4583, 24357, 13, 51675], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1470, "seek": 571996, "start": 5746.96, "end": 5749.56, "text": " So this is called the pre norm formulation and that the one that we're", "tokens": [51715, 407, 341, 307, 1219, 264, 659, 2026, 37642, 293, 300, 264, 472, 300, 321, 434, 51845], "temperature": 0.0, "avg_logprob": -0.1333943704017123, "compression_ratio": 1.8827586206896552, "no_speech_prob": 0.00022674664796795696}, {"id": 1471, "seek": 574956, "start": 5749.56, "end": 5750.660000000001, "text": " going to implement as well.", "tokens": [50365, 516, 281, 4445, 382, 731, 13, 50420], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1472, "seek": 574956, "start": 5750.660000000001, "end": 5752.46, "text": " So slight deviation from the original paper.", "tokens": [50420, 407, 4036, 25163, 490, 264, 3380, 3035, 13, 50510], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1473, "seek": 574956, "start": 5753.26, "end": 5755.360000000001, "text": " Basically, we need to layer norms layer norm.", "tokens": [50550, 8537, 11, 321, 643, 281, 4583, 24357, 4583, 2026, 13, 50655], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1474, "seek": 574956, "start": 5755.360000000001, "end": 5761.360000000001, "text": " One is an end dot layer norm and we tell it how many was the embedding", "tokens": [50655, 1485, 307, 364, 917, 5893, 4583, 2026, 293, 321, 980, 309, 577, 867, 390, 264, 12240, 3584, 50955], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1475, "seek": 574956, "start": 5761.360000000001, "end": 5764.360000000001, "text": " dimension and we need the second layer norm.", "tokens": [50955, 10139, 293, 321, 643, 264, 1150, 4583, 2026, 13, 51105], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1476, "seek": 574956, "start": 5765.26, "end": 5768.660000000001, "text": " And then here the layer norms are applied immediately on X.", "tokens": [51150, 400, 550, 510, 264, 4583, 24357, 366, 6456, 4258, 322, 1783, 13, 51320], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1477, "seek": 574956, "start": 5769.360000000001, "end": 5773.76, "text": " So self-taught layer norm one in applied on X and self-taught layer norm", "tokens": [51355, 407, 2698, 12, 1328, 1599, 4583, 2026, 472, 294, 6456, 322, 1783, 293, 2698, 12, 1328, 1599, 4583, 2026, 51575], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1478, "seek": 574956, "start": 5773.76, "end": 5779.46, "text": " two applied on X before it goes into self-attention and feed forward and", "tokens": [51575, 732, 6456, 322, 1783, 949, 309, 1709, 666, 2698, 12, 1591, 1251, 293, 3154, 2128, 293, 51860], "temperature": 0.0, "avg_logprob": -0.20640709525660464, "compression_ratio": 1.8803418803418803, "no_speech_prob": 7.710079808020964e-05}, {"id": 1479, "seek": 577956, "start": 5779.56, "end": 5782.46, "text": " the size of the layer norm here is an embed so 32.", "tokens": [50365, 264, 2744, 295, 264, 4583, 2026, 510, 307, 364, 12240, 370, 8858, 13, 50510], "temperature": 0.0, "avg_logprob": -0.13913253902160017, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00013011843839194626}, {"id": 1480, "seek": 577956, "start": 5783.06, "end": 5787.96, "text": " So when the layer norm is normalizing our features it is the normalization", "tokens": [50540, 407, 562, 264, 4583, 2026, 307, 2710, 3319, 527, 4122, 309, 307, 264, 2710, 2144, 50785], "temperature": 0.0, "avg_logprob": -0.13913253902160017, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00013011843839194626}, {"id": 1481, "seek": 577956, "start": 5787.96, "end": 5793.860000000001, "text": " here happens the mean and the variance are taken over 32 numbers.", "tokens": [50785, 510, 2314, 264, 914, 293, 264, 21977, 366, 2726, 670, 8858, 3547, 13, 51080], "temperature": 0.0, "avg_logprob": -0.13913253902160017, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00013011843839194626}, {"id": 1482, "seek": 577956, "start": 5794.160000000001, "end": 5797.76, "text": " So the batch and the time act as batch dimensions both of them.", "tokens": [51095, 407, 264, 15245, 293, 264, 565, 605, 382, 15245, 12819, 1293, 295, 552, 13, 51275], "temperature": 0.0, "avg_logprob": -0.13913253902160017, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00013011843839194626}, {"id": 1483, "seek": 577956, "start": 5798.360000000001, "end": 5802.860000000001, "text": " So this is kind of like a per token transformation that just normalizes", "tokens": [51305, 407, 341, 307, 733, 295, 411, 257, 680, 14862, 9887, 300, 445, 2710, 5660, 51530], "temperature": 0.0, "avg_logprob": -0.13913253902160017, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00013011843839194626}, {"id": 1484, "seek": 577956, "start": 5802.860000000001, "end": 5808.56, "text": " the features and makes them a unit mean unit Gaussian at initialization.", "tokens": [51530, 264, 4122, 293, 1669, 552, 257, 4985, 914, 4985, 39148, 412, 5883, 2144, 13, 51815], "temperature": 0.0, "avg_logprob": -0.13913253902160017, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00013011843839194626}, {"id": 1485, "seek": 580856, "start": 5808.56, "end": 5813.360000000001, "text": " But of course because these layer norms inside it have these gamma and beta", "tokens": [50365, 583, 295, 1164, 570, 613, 4583, 24357, 1854, 309, 362, 613, 15546, 293, 9861, 50605], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1486, "seek": 580856, "start": 5813.360000000001, "end": 5819.360000000001, "text": " trainable parameters the layer normal eventually create outputs that might", "tokens": [50605, 3847, 712, 9834, 264, 4583, 2710, 4728, 1884, 23930, 300, 1062, 50905], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1487, "seek": 580856, "start": 5819.360000000001, "end": 5823.860000000001, "text": " not be unit Gaussian but the optimization will determine that so for", "tokens": [50905, 406, 312, 4985, 39148, 457, 264, 19618, 486, 6997, 300, 370, 337, 51130], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1488, "seek": 580856, "start": 5823.860000000001, "end": 5827.660000000001, "text": " now, this is the this is incorporating the layer norms and let's train them", "tokens": [51130, 586, 11, 341, 307, 264, 341, 307, 33613, 264, 4583, 24357, 293, 718, 311, 3847, 552, 51320], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1489, "seek": 580856, "start": 5827.660000000001, "end": 5827.96, "text": " up.", "tokens": [51320, 493, 13, 51335], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1490, "seek": 580856, "start": 5828.56, "end": 5832.660000000001, "text": " Okay, so I let it run and we see that we get down to 2.06 which is better", "tokens": [51365, 1033, 11, 370, 286, 718, 309, 1190, 293, 321, 536, 300, 321, 483, 760, 281, 568, 13, 12791, 597, 307, 1101, 51570], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1491, "seek": 580856, "start": 5832.660000000001, "end": 5834.06, "text": " than the previous 2.08.", "tokens": [51570, 813, 264, 3894, 568, 13, 16133, 13, 51640], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1492, "seek": 580856, "start": 5834.360000000001, "end": 5837.76, "text": " So a slight improvement by adding the layer norms and I'd expect that they", "tokens": [51655, 407, 257, 4036, 10444, 538, 5127, 264, 4583, 24357, 293, 286, 1116, 2066, 300, 436, 51825], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1493, "seek": 580856, "start": 5837.76, "end": 5838.26, "text": " help.", "tokens": [51825, 854, 13, 51850], "temperature": 0.0, "avg_logprob": -0.15853451093037924, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.00012180517660453916}, {"id": 1494, "seek": 583826, "start": 5838.26, "end": 5840.46, "text": " Even more if we have bigger and deeper network.", "tokens": [50365, 2754, 544, 498, 321, 362, 3801, 293, 7731, 3209, 13, 50475], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1495, "seek": 583826, "start": 5841.06, "end": 5841.56, "text": " One more thing.", "tokens": [50505, 1485, 544, 551, 13, 50530], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1496, "seek": 583826, "start": 5841.56, "end": 5844.46, "text": " I forgot to add is that there should be a layer norm here.", "tokens": [50530, 286, 5298, 281, 909, 307, 300, 456, 820, 312, 257, 4583, 2026, 510, 13, 50675], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1497, "seek": 583826, "start": 5844.46, "end": 5849.360000000001, "text": " Also typically as at the end of the transformer and right before the final", "tokens": [50675, 2743, 5850, 382, 412, 264, 917, 295, 264, 31782, 293, 558, 949, 264, 2572, 50920], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1498, "seek": 583826, "start": 5849.66, "end": 5852.26, "text": " linear layer that decodes into vocabulary.", "tokens": [50935, 8213, 4583, 300, 979, 4789, 666, 19864, 13, 51065], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1499, "seek": 583826, "start": 5852.76, "end": 5854.06, "text": " So I added that as well.", "tokens": [51090, 407, 286, 3869, 300, 382, 731, 13, 51155], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1500, "seek": 583826, "start": 5854.76, "end": 5857.96, "text": " So at this stage, we actually have a pretty complete transformer coming to", "tokens": [51190, 407, 412, 341, 3233, 11, 321, 767, 362, 257, 1238, 3566, 31782, 1348, 281, 51350], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1501, "seek": 583826, "start": 5857.96, "end": 5860.860000000001, "text": " the original paper and it's a decoder only transformer.", "tokens": [51350, 264, 3380, 3035, 293, 309, 311, 257, 979, 19866, 787, 31782, 13, 51495], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1502, "seek": 583826, "start": 5860.96, "end": 5865.26, "text": " I'll I'll talk about that in a second but at this stage the major pieces", "tokens": [51500, 286, 603, 286, 603, 751, 466, 300, 294, 257, 1150, 457, 412, 341, 3233, 264, 2563, 3755, 51715], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1503, "seek": 583826, "start": 5865.26, "end": 5868.16, "text": " are in place so we can try to scale this up and see how well we can push", "tokens": [51715, 366, 294, 1081, 370, 321, 393, 853, 281, 4373, 341, 493, 293, 536, 577, 731, 321, 393, 2944, 51860], "temperature": 0.0, "avg_logprob": -0.1293663569859096, "compression_ratio": 1.7712418300653594, "no_speech_prob": 9.793406934477389e-05}, {"id": 1504, "seek": 586816, "start": 5868.16, "end": 5870.76, "text": " this number now in order to scale up the model.", "tokens": [50365, 341, 1230, 586, 294, 1668, 281, 4373, 493, 264, 2316, 13, 50495], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1505, "seek": 586816, "start": 5870.76, "end": 5874.36, "text": " I had to perform some cosmetic changes here to make it nicer.", "tokens": [50495, 286, 632, 281, 2042, 512, 35828, 2962, 510, 281, 652, 309, 22842, 13, 50675], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1506, "seek": 586816, "start": 5874.66, "end": 5877.66, "text": " So I introduced this variable called in layer which just specifies how", "tokens": [50690, 407, 286, 7268, 341, 7006, 1219, 294, 4583, 597, 445, 1608, 11221, 577, 50840], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1507, "seek": 586816, "start": 5877.66, "end": 5879.86, "text": " many layers of the blocks.", "tokens": [50840, 867, 7914, 295, 264, 8474, 13, 50950], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1508, "seek": 586816, "start": 5879.86, "end": 5883.26, "text": " We're going to have I create a bunch of blocks and we have a new variable", "tokens": [50950, 492, 434, 516, 281, 362, 286, 1884, 257, 3840, 295, 8474, 293, 321, 362, 257, 777, 7006, 51120], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1509, "seek": 586816, "start": 5883.26, "end": 5884.46, "text": " number of heads as well.", "tokens": [51120, 1230, 295, 8050, 382, 731, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1510, "seek": 586816, "start": 5885.46, "end": 5886.96, "text": " I pulled out the layer norm here.", "tokens": [51230, 286, 7373, 484, 264, 4583, 2026, 510, 13, 51305], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1511, "seek": 586816, "start": 5887.16, "end": 5888.5599999999995, "text": " And so this is identical.", "tokens": [51315, 400, 370, 341, 307, 14800, 13, 51385], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1512, "seek": 586816, "start": 5889.16, "end": 5892.66, "text": " Now one thing that I did briefly change is I added dropout.", "tokens": [51415, 823, 472, 551, 300, 286, 630, 10515, 1319, 307, 286, 3869, 3270, 346, 13, 51590], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1513, "seek": 586816, "start": 5893.16, "end": 5897.76, "text": " So dropout is something that you can add right before the residual connection.", "tokens": [51615, 407, 3270, 346, 307, 746, 300, 291, 393, 909, 558, 949, 264, 27980, 4984, 13, 51845], "temperature": 0.0, "avg_logprob": -0.1195058895431402, "compression_ratio": 1.6946308724832215, "no_speech_prob": 0.00018500970327295363}, {"id": 1514, "seek": 589776, "start": 5897.76, "end": 5901.06, "text": " Back right before the connection back into the residual pathway.", "tokens": [50365, 5833, 558, 949, 264, 4984, 646, 666, 264, 27980, 18590, 13, 50530], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1515, "seek": 589776, "start": 5901.66, "end": 5904.16, "text": " So we can drop out that as the last layer here.", "tokens": [50560, 407, 321, 393, 3270, 484, 300, 382, 264, 1036, 4583, 510, 13, 50685], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1516, "seek": 589776, "start": 5904.76, "end": 5908.76, "text": " We can drop out here at the end of the multi-headed extension as well.", "tokens": [50715, 492, 393, 3270, 484, 510, 412, 264, 917, 295, 264, 4825, 12, 28409, 10320, 382, 731, 13, 50915], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1517, "seek": 589776, "start": 5909.46, "end": 5915.360000000001, "text": " And we can also drop out here when we calculate the basically affinities", "tokens": [50950, 400, 321, 393, 611, 3270, 484, 510, 562, 321, 8873, 264, 1936, 2096, 259, 1088, 51245], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1518, "seek": 589776, "start": 5915.360000000001, "end": 5919.26, "text": " and after the softmax we can drop out some of those so we can randomly", "tokens": [51245, 293, 934, 264, 2787, 41167, 321, 393, 3270, 484, 512, 295, 729, 370, 321, 393, 16979, 51440], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1519, "seek": 589776, "start": 5919.26, "end": 5921.26, "text": " prevent some of the notes from communicating.", "tokens": [51440, 4871, 512, 295, 264, 5570, 490, 17559, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1520, "seek": 589776, "start": 5922.06, "end": 5926.860000000001, "text": " And so dropout comes from this paper from 2014 or so.", "tokens": [51580, 400, 370, 3270, 346, 1487, 490, 341, 3035, 490, 8227, 420, 370, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1380553157241256, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.00015319582598749548}, {"id": 1521, "seek": 592686, "start": 5926.86, "end": 5933.759999999999, "text": " And basically it takes your neural net and it randomly every forward backward", "tokens": [50365, 400, 1936, 309, 2516, 428, 18161, 2533, 293, 309, 16979, 633, 2128, 23897, 50710], "temperature": 0.0, "avg_logprob": -0.17438468112740466, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00018786641885526478}, {"id": 1522, "seek": 592686, "start": 5933.759999999999, "end": 5940.86, "text": " pass shuts off some subset of neurons so randomly drops them to zero and", "tokens": [50710, 1320, 48590, 766, 512, 25993, 295, 22027, 370, 16979, 11438, 552, 281, 4018, 293, 51065], "temperature": 0.0, "avg_logprob": -0.17438468112740466, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00018786641885526478}, {"id": 1523, "seek": 592686, "start": 5940.86, "end": 5945.46, "text": " trains without them and what this does effectively is because the mask of", "tokens": [51065, 16329, 1553, 552, 293, 437, 341, 775, 8659, 307, 570, 264, 6094, 295, 51295], "temperature": 0.0, "avg_logprob": -0.17438468112740466, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00018786641885526478}, {"id": 1524, "seek": 592686, "start": 5945.46, "end": 5948.759999999999, "text": " what being dropped out has changed every single forward backward pass it", "tokens": [51295, 437, 885, 8119, 484, 575, 3105, 633, 2167, 2128, 23897, 1320, 309, 51460], "temperature": 0.0, "avg_logprob": -0.17438468112740466, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00018786641885526478}, {"id": 1525, "seek": 592686, "start": 5948.759999999999, "end": 5954.0599999999995, "text": " ends up kind of training an ensemble of sub networks and then at test time", "tokens": [51460, 5314, 493, 733, 295, 3097, 364, 19492, 295, 1422, 9590, 293, 550, 412, 1500, 565, 51725], "temperature": 0.0, "avg_logprob": -0.17438468112740466, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00018786641885526478}, {"id": 1526, "seek": 592686, "start": 5954.0599999999995, "end": 5956.759999999999, "text": " everything is fully enabled and kind of all those sub networks.", "tokens": [51725, 1203, 307, 4498, 15172, 293, 733, 295, 439, 729, 1422, 9590, 13, 51860], "temperature": 0.0, "avg_logprob": -0.17438468112740466, "compression_ratio": 1.8016528925619835, "no_speech_prob": 0.00018786641885526478}, {"id": 1527, "seek": 595676, "start": 5956.76, "end": 5958.56, "text": " Are merged into a single ensemble.", "tokens": [50365, 2014, 36427, 666, 257, 2167, 19492, 13, 50455], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1528, "seek": 595676, "start": 5958.56, "end": 5960.26, "text": " If you can if you want to think about it that way.", "tokens": [50455, 759, 291, 393, 498, 291, 528, 281, 519, 466, 309, 300, 636, 13, 50540], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1529, "seek": 595676, "start": 5960.96, "end": 5963.96, "text": " So I would read the paper to get the full detail for now.", "tokens": [50575, 407, 286, 576, 1401, 264, 3035, 281, 483, 264, 1577, 2607, 337, 586, 13, 50725], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1530, "seek": 595676, "start": 5963.96, "end": 5967.360000000001, "text": " We're just going to stay on the level of this is a regularization technique", "tokens": [50725, 492, 434, 445, 516, 281, 1754, 322, 264, 1496, 295, 341, 307, 257, 3890, 2144, 6532, 50895], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1531, "seek": 595676, "start": 5967.66, "end": 5970.860000000001, "text": " and I added it because I'm about to scale up the model quite a bit and I", "tokens": [50910, 293, 286, 3869, 309, 570, 286, 478, 466, 281, 4373, 493, 264, 2316, 1596, 257, 857, 293, 286, 51070], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1532, "seek": 595676, "start": 5970.860000000001, "end": 5972.06, "text": " was concerned about overfitting.", "tokens": [51070, 390, 5922, 466, 670, 69, 2414, 13, 51130], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1533, "seek": 595676, "start": 5973.360000000001, "end": 5977.46, "text": " So now when we scroll up to the top we'll see that I changed a number of", "tokens": [51195, 407, 586, 562, 321, 11369, 493, 281, 264, 1192, 321, 603, 536, 300, 286, 3105, 257, 1230, 295, 51400], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1534, "seek": 595676, "start": 5977.46, "end": 5979.360000000001, "text": " hyper parameters here about our neural net.", "tokens": [51400, 9848, 9834, 510, 466, 527, 18161, 2533, 13, 51495], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1535, "seek": 595676, "start": 5979.76, "end": 5982.46, "text": " So I made the batch size be much larger now 64.", "tokens": [51515, 407, 286, 1027, 264, 15245, 2744, 312, 709, 4833, 586, 12145, 13, 51650], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1536, "seek": 595676, "start": 5983.16, "end": 5985.26, "text": " I changed the block size to be 256.", "tokens": [51685, 286, 3105, 264, 3461, 2744, 281, 312, 38882, 13, 51790], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1537, "seek": 595676, "start": 5985.46, "end": 5986.66, "text": " So previously was just eight.", "tokens": [51800, 407, 8046, 390, 445, 3180, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12114308843549514, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.00012497996794991195}, {"id": 1538, "seek": 598676, "start": 5986.96, "end": 5988.16, "text": " Eight characters of context.", "tokens": [50375, 17708, 4342, 295, 4319, 13, 50435], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1539, "seek": 598676, "start": 5988.26, "end": 5992.76, "text": " Now it is 256 characters of context to predict the 257th.", "tokens": [50440, 823, 309, 307, 38882, 4342, 295, 4319, 281, 6069, 264, 3552, 22, 392, 13, 50665], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1540, "seek": 598676, "start": 5994.360000000001, "end": 5997.16, "text": " I brought down the learning rate a little bit because the neural net is", "tokens": [50745, 286, 3038, 760, 264, 2539, 3314, 257, 707, 857, 570, 264, 18161, 2533, 307, 50885], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1541, "seek": 598676, "start": 5997.16, "end": 5997.96, "text": " now much bigger.", "tokens": [50885, 586, 709, 3801, 13, 50925], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1542, "seek": 598676, "start": 5997.96, "end": 5999.360000000001, "text": " So I brought down the learning rate.", "tokens": [50925, 407, 286, 3038, 760, 264, 2539, 3314, 13, 50995], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1543, "seek": 598676, "start": 6000.26, "end": 6003.46, "text": " The embedding dimension is not 384 and there are six heads.", "tokens": [51040, 440, 12240, 3584, 10139, 307, 406, 12843, 19, 293, 456, 366, 2309, 8050, 13, 51200], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1544, "seek": 598676, "start": 6003.96, "end": 6010.26, "text": " So 384 divide 6 means that every head is 64 dimensional as it as a standard", "tokens": [51225, 407, 12843, 19, 9845, 1386, 1355, 300, 633, 1378, 307, 12145, 18795, 382, 309, 382, 257, 3832, 51540], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1545, "seek": 598676, "start": 6011.06, "end": 6014.860000000001, "text": " and then there was going to be six layers of that and the dropout will be", "tokens": [51580, 293, 550, 456, 390, 516, 281, 312, 2309, 7914, 295, 300, 293, 264, 3270, 346, 486, 312, 51770], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1546, "seek": 598676, "start": 6014.860000000001, "end": 6016.66, "text": " a point to so every forward backward pass.", "tokens": [51770, 257, 935, 281, 370, 633, 2128, 23897, 1320, 13, 51860], "temperature": 0.0, "avg_logprob": -0.16256497911185272, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00010815169662237167}, {"id": 1547, "seek": 601676, "start": 6016.96, "end": 6022.76, "text": " 20% of all these intermediate calculations are disabled and dropped to", "tokens": [50375, 945, 4, 295, 439, 613, 19376, 20448, 366, 15191, 293, 8119, 281, 50665], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1548, "seek": 601676, "start": 6022.76, "end": 6026.16, "text": " zero and then I already trained this and I ran it.", "tokens": [50665, 4018, 293, 550, 286, 1217, 8895, 341, 293, 286, 5872, 309, 13, 50835], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1549, "seek": 601676, "start": 6026.16, "end": 6028.96, "text": " So drumroll how does it perform?", "tokens": [50835, 407, 10206, 3970, 577, 775, 309, 2042, 30, 50975], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1550, "seek": 601676, "start": 6029.76, "end": 6030.860000000001, "text": " So let me just scroll up here.", "tokens": [51015, 407, 718, 385, 445, 11369, 493, 510, 13, 51070], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1551, "seek": 601676, "start": 6032.76, "end": 6037.26, "text": " We get a validation loss of 1.48 which is actually quite a bit of an", "tokens": [51165, 492, 483, 257, 24071, 4470, 295, 502, 13, 13318, 597, 307, 767, 1596, 257, 857, 295, 364, 51390], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1552, "seek": 601676, "start": 6037.26, "end": 6040.16, "text": " improvement on what we had before which I think was 2.07.", "tokens": [51390, 10444, 322, 437, 321, 632, 949, 597, 286, 519, 390, 568, 13, 16231, 13, 51535], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1553, "seek": 601676, "start": 6040.66, "end": 6044.16, "text": " So we went from 2.07 all the way down to 1.48 just by scaling up this", "tokens": [51560, 407, 321, 1437, 490, 568, 13, 16231, 439, 264, 636, 760, 281, 502, 13, 13318, 445, 538, 21589, 493, 341, 51735], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1554, "seek": 601676, "start": 6044.16, "end": 6045.860000000001, "text": " neural net with the code that we have.", "tokens": [51735, 18161, 2533, 365, 264, 3089, 300, 321, 362, 13, 51820], "temperature": 0.0, "avg_logprob": -0.154520972300384, "compression_ratio": 1.5827067669172932, "no_speech_prob": 2.3233971660374664e-05}, {"id": 1555, "seek": 604586, "start": 6046.36, "end": 6048.0599999999995, "text": " And this of course ran for a lot longer.", "tokens": [50390, 400, 341, 295, 1164, 5872, 337, 257, 688, 2854, 13, 50475], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1556, "seek": 604586, "start": 6048.0599999999995, "end": 6052.86, "text": " This may be trained for I want to say about 15 minutes on my A100 GPU.", "tokens": [50475, 639, 815, 312, 8895, 337, 286, 528, 281, 584, 466, 2119, 2077, 322, 452, 316, 6879, 18407, 13, 50715], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1557, "seek": 604586, "start": 6052.86, "end": 6055.96, "text": " So that's a pretty good GPU and if you don't have a GPU you're not going to", "tokens": [50715, 407, 300, 311, 257, 1238, 665, 18407, 293, 498, 291, 500, 380, 362, 257, 18407, 291, 434, 406, 516, 281, 50870], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1558, "seek": 604586, "start": 6055.96, "end": 6058.46, "text": " be able to reproduce this on a CPU.", "tokens": [50870, 312, 1075, 281, 29501, 341, 322, 257, 13199, 13, 50995], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1559, "seek": 604586, "start": 6058.46, "end": 6062.36, "text": " This would be I would not run this on the CPU or MacBook or something like", "tokens": [50995, 639, 576, 312, 286, 576, 406, 1190, 341, 322, 264, 13199, 420, 31737, 420, 746, 411, 51190], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1560, "seek": 604586, "start": 6062.36, "end": 6062.86, "text": " that.", "tokens": [51190, 300, 13, 51215], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1561, "seek": 604586, "start": 6062.86, "end": 6066.66, "text": " You'll have to break down the number of layers and the embedding dimension", "tokens": [51215, 509, 603, 362, 281, 1821, 760, 264, 1230, 295, 7914, 293, 264, 12240, 3584, 10139, 51405], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1562, "seek": 604586, "start": 6066.66, "end": 6067.259999999999, "text": " and so on.", "tokens": [51405, 293, 370, 322, 13, 51435], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1563, "seek": 604586, "start": 6068.46, "end": 6073.36, "text": " But in about 15 minutes we can get this kind of a result and I'm printing", "tokens": [51495, 583, 294, 466, 2119, 2077, 321, 393, 483, 341, 733, 295, 257, 1874, 293, 286, 478, 14699, 51740], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1564, "seek": 604586, "start": 6074.0599999999995, "end": 6075.0599999999995, "text": " some of the Shakespeare here.", "tokens": [51775, 512, 295, 264, 22825, 510, 13, 51825], "temperature": 0.0, "avg_logprob": -0.1217405470155126, "compression_ratio": 1.6303630363036303, "no_speech_prob": 6.136060983408242e-05}, {"id": 1565, "seek": 607506, "start": 6075.06, "end": 6077.860000000001, "text": " But what I did also is I printed 10,000 characters.", "tokens": [50365, 583, 437, 286, 630, 611, 307, 286, 13567, 1266, 11, 1360, 4342, 13, 50505], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1566, "seek": 607506, "start": 6077.860000000001, "end": 6079.76, "text": " So a lot more and I wrote them to a file.", "tokens": [50505, 407, 257, 688, 544, 293, 286, 4114, 552, 281, 257, 3991, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1567, "seek": 607506, "start": 6080.56, "end": 6081.96, "text": " And so here we see some of the outputs.", "tokens": [50640, 400, 370, 510, 321, 536, 512, 295, 264, 23930, 13, 50710], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1568, "seek": 607506, "start": 6084.26, "end": 6087.860000000001, "text": " So it's a lot more recognizable as the input text file.", "tokens": [50825, 407, 309, 311, 257, 688, 544, 40757, 382, 264, 4846, 2487, 3991, 13, 51005], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1569, "seek": 607506, "start": 6088.26, "end": 6090.860000000001, "text": " So the input text file just for reference look like this.", "tokens": [51025, 407, 264, 4846, 2487, 3991, 445, 337, 6408, 574, 411, 341, 13, 51155], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1570, "seek": 607506, "start": 6091.76, "end": 6097.26, "text": " So there's always like someone speaking in this matter and our predictions", "tokens": [51200, 407, 456, 311, 1009, 411, 1580, 4124, 294, 341, 1871, 293, 527, 21264, 51475], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1571, "seek": 607506, "start": 6097.26, "end": 6101.56, "text": " now take on that form except of course they're nonsensical when you", "tokens": [51475, 586, 747, 322, 300, 1254, 3993, 295, 1164, 436, 434, 297, 892, 694, 804, 562, 291, 51690], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1572, "seek": 607506, "start": 6101.56, "end": 6102.26, "text": " actually read them.", "tokens": [51690, 767, 1401, 552, 13, 51725], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1573, "seek": 607506, "start": 6102.860000000001, "end": 6103.360000000001, "text": " So", "tokens": [51755, 407, 51780], "temperature": 0.0, "avg_logprob": -0.1297516237225449, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00016590427549090236}, {"id": 1574, "seek": 610336, "start": 6103.36, "end": 6106.96, "text": " it is every crimp to be a house.", "tokens": [50365, 309, 307, 633, 7857, 79, 281, 312, 257, 1782, 13, 50545], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1575, "seek": 610336, "start": 6106.96, "end": 6110.5599999999995, "text": " Oh those probation we give heed.", "tokens": [50545, 876, 729, 41821, 321, 976, 49781, 13, 50725], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1576, "seek": 610336, "start": 6112.5599999999995, "end": 6113.16, "text": " You know.", "tokens": [50825, 509, 458, 13, 50855], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1577, "seek": 610336, "start": 6115.96, "end": 6118.0599999999995, "text": " Oh ho sent me you mighty Lord.", "tokens": [50995, 876, 1106, 2279, 385, 291, 21556, 3257, 13, 51100], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1578, "seek": 610336, "start": 6120.5599999999995, "end": 6122.16, "text": " Anyway, so you can read through this.", "tokens": [51225, 5684, 11, 370, 291, 393, 1401, 807, 341, 13, 51305], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1579, "seek": 610336, "start": 6122.16, "end": 6126.46, "text": " It's nonsensical of course, but this is just a transformer trained on the", "tokens": [51305, 467, 311, 297, 892, 694, 804, 295, 1164, 11, 457, 341, 307, 445, 257, 31782, 8895, 322, 264, 51520], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1580, "seek": 610336, "start": 6126.46, "end": 6130.0599999999995, "text": " character level for 1 million characters that come from Shakespeare.", "tokens": [51520, 2517, 1496, 337, 502, 2459, 4342, 300, 808, 490, 22825, 13, 51700], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1581, "seek": 610336, "start": 6130.0599999999995, "end": 6133.16, "text": " So there's sort of like blabbers on in Shakespeare like math.", "tokens": [51700, 407, 456, 311, 1333, 295, 411, 888, 455, 1616, 322, 294, 22825, 411, 5221, 13, 51855], "temperature": 0.0, "avg_logprob": -0.2935835585302236, "compression_ratio": 1.517391304347826, "no_speech_prob": 0.00045920329284854233}, {"id": 1582, "seek": 613336, "start": 6133.36, "end": 6136.16, "text": " Banner, but it doesn't of course make sense at this scale.", "tokens": [50365, 363, 9805, 11, 457, 309, 1177, 380, 295, 1164, 652, 2020, 412, 341, 4373, 13, 50505], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1583, "seek": 613336, "start": 6137.0599999999995, "end": 6140.5599999999995, "text": " But I think I think still a pretty good demonstration of what's possible.", "tokens": [50550, 583, 286, 519, 286, 519, 920, 257, 1238, 665, 16520, 295, 437, 311, 1944, 13, 50725], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1584, "seek": 613336, "start": 6141.759999999999, "end": 6143.16, "text": " So now", "tokens": [50785, 407, 586, 50855], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1585, "seek": 613336, "start": 6144.5599999999995, "end": 6148.36, "text": " I think that kind of like concludes the programming section of this video.", "tokens": [50925, 286, 519, 300, 733, 295, 411, 24643, 264, 9410, 3541, 295, 341, 960, 13, 51115], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1586, "seek": 613336, "start": 6148.5599999999995, "end": 6152.86, "text": " We basically kind of did a pretty good job and of implementing this", "tokens": [51125, 492, 1936, 733, 295, 630, 257, 1238, 665, 1691, 293, 295, 18114, 341, 51340], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1587, "seek": 613336, "start": 6152.86, "end": 6157.16, "text": " transformer, but the picture doesn't exactly match up to what we've done.", "tokens": [51340, 31782, 11, 457, 264, 3036, 1177, 380, 2293, 2995, 493, 281, 437, 321, 600, 1096, 13, 51555], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1588, "seek": 613336, "start": 6157.36, "end": 6159.5599999999995, "text": " So what's going on with all these additional parts here?", "tokens": [51565, 407, 437, 311, 516, 322, 365, 439, 613, 4497, 3166, 510, 30, 51675], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1589, "seek": 613336, "start": 6160.0599999999995, "end": 6163.259999999999, "text": " So let me finish explaining this architecture and why it looks so funky.", "tokens": [51700, 407, 718, 385, 2413, 13468, 341, 9482, 293, 983, 309, 1542, 370, 33499, 13, 51860], "temperature": 0.0, "avg_logprob": -0.11438403695316637, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0001877859904197976}, {"id": 1590, "seek": 616336, "start": 6164.0599999999995, "end": 6167.96, "text": " Basically, what's happening here is what we implemented here is a decoder", "tokens": [50400, 8537, 11, 437, 311, 2737, 510, 307, 437, 321, 12270, 510, 307, 257, 979, 19866, 50595], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1591, "seek": 616336, "start": 6167.96, "end": 6168.86, "text": " only transformer.", "tokens": [50595, 787, 31782, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1592, "seek": 616336, "start": 6169.36, "end": 6171.16, "text": " So there's no component here.", "tokens": [50665, 407, 456, 311, 572, 6542, 510, 13, 50755], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1593, "seek": 616336, "start": 6171.16, "end": 6175.259999999999, "text": " This part is called the encoder and there's no cross attention block here.", "tokens": [50755, 639, 644, 307, 1219, 264, 2058, 19866, 293, 456, 311, 572, 3278, 3202, 3461, 510, 13, 50960], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1594, "seek": 616336, "start": 6175.759999999999, "end": 6178.96, "text": " Our block only has a self attention and the feed forward.", "tokens": [50985, 2621, 3461, 787, 575, 257, 2698, 3202, 293, 264, 3154, 2128, 13, 51145], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1595, "seek": 616336, "start": 6178.96, "end": 6182.5599999999995, "text": " So it is missing this third in between piece here.", "tokens": [51145, 407, 309, 307, 5361, 341, 2636, 294, 1296, 2522, 510, 13, 51325], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1596, "seek": 616336, "start": 6182.96, "end": 6184.259999999999, "text": " This piece does cross attention.", "tokens": [51345, 639, 2522, 775, 3278, 3202, 13, 51410], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1597, "seek": 616336, "start": 6184.5599999999995, "end": 6186.66, "text": " So we don't have it and we don't have the encoder.", "tokens": [51425, 407, 321, 500, 380, 362, 309, 293, 321, 500, 380, 362, 264, 2058, 19866, 13, 51530], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1598, "seek": 616336, "start": 6186.759999999999, "end": 6191.86, "text": " We just have the decoder and the reason we have a decoder only is because we are", "tokens": [51535, 492, 445, 362, 264, 979, 19866, 293, 264, 1778, 321, 362, 257, 979, 19866, 787, 307, 570, 321, 366, 51790], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1599, "seek": 616336, "start": 6191.86, "end": 6193.0599999999995, "text": " just generating text.", "tokens": [51790, 445, 17746, 2487, 13, 51850], "temperature": 0.0, "avg_logprob": -0.12673678764930138, "compression_ratio": 1.9601593625498008, "no_speech_prob": 0.0010910541750490665}, {"id": 1600, "seek": 619306, "start": 6193.06, "end": 6196.660000000001, "text": " And it's unconditioned on anything or just we're just blabbering on according", "tokens": [50365, 400, 309, 311, 34959, 849, 292, 322, 1340, 420, 445, 321, 434, 445, 888, 455, 607, 278, 322, 4650, 50545], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1601, "seek": 619306, "start": 6196.660000000001, "end": 6197.56, "text": " to a given data set.", "tokens": [50545, 281, 257, 2212, 1412, 992, 13, 50590], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1602, "seek": 619306, "start": 6198.360000000001, "end": 6202.660000000001, "text": " What makes it a decoder is that we are using the triangular mask in our", "tokens": [50630, 708, 1669, 309, 257, 979, 19866, 307, 300, 321, 366, 1228, 264, 38190, 6094, 294, 527, 50845], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1603, "seek": 619306, "start": 6202.860000000001, "end": 6203.46, "text": " transformer.", "tokens": [50855, 31782, 13, 50885], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1604, "seek": 619306, "start": 6203.76, "end": 6207.660000000001, "text": " So it has this autoregressive property where we can just go and sample from it.", "tokens": [50900, 407, 309, 575, 341, 1476, 418, 3091, 488, 4707, 689, 321, 393, 445, 352, 293, 6889, 490, 309, 13, 51095], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1605, "seek": 619306, "start": 6208.56, "end": 6212.46, "text": " So the fact that it's using the triangulate triangular mask to mask out the", "tokens": [51140, 407, 264, 1186, 300, 309, 311, 1228, 264, 19335, 5256, 38190, 6094, 281, 6094, 484, 264, 51335], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1606, "seek": 619306, "start": 6212.46, "end": 6215.96, "text": " attention makes it a decoder and it can be used for language modeling.", "tokens": [51335, 3202, 1669, 309, 257, 979, 19866, 293, 309, 393, 312, 1143, 337, 2856, 15983, 13, 51510], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1607, "seek": 619306, "start": 6216.660000000001, "end": 6221.06, "text": " Now, the reason that the original paper had an encoder decoder architecture is", "tokens": [51545, 823, 11, 264, 1778, 300, 264, 3380, 3035, 632, 364, 2058, 19866, 979, 19866, 9482, 307, 51765], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1608, "seek": 619306, "start": 6221.06, "end": 6222.76, "text": " because it is a machine translation paper.", "tokens": [51765, 570, 309, 307, 257, 3479, 12853, 3035, 13, 51850], "temperature": 0.0, "avg_logprob": -0.10171308236963608, "compression_ratio": 1.8344827586206895, "no_speech_prob": 8.300751505885273e-05}, {"id": 1609, "seek": 622306, "start": 6223.160000000001, "end": 6226.26, "text": " So it is concerned with a different setting in particular.", "tokens": [50370, 407, 309, 307, 5922, 365, 257, 819, 3287, 294, 1729, 13, 50525], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1610, "seek": 622306, "start": 6226.860000000001, "end": 6233.26, "text": " It expects some tokens that encode say for example French and then it is expected to", "tokens": [50555, 467, 33280, 512, 22667, 300, 2058, 1429, 584, 337, 1365, 5522, 293, 550, 309, 307, 5176, 281, 50875], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1611, "seek": 622306, "start": 6233.26, "end": 6235.26, "text": " decode the translation in English.", "tokens": [50875, 979, 1429, 264, 12853, 294, 3669, 13, 50975], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1612, "seek": 622306, "start": 6235.860000000001, "end": 6238.96, "text": " So so you typically these here are special tokens.", "tokens": [51005, 407, 370, 291, 5850, 613, 510, 366, 2121, 22667, 13, 51160], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1613, "seek": 622306, "start": 6239.360000000001, "end": 6244.360000000001, "text": " So you are expected to read in this and condition on it and then you start off the", "tokens": [51180, 407, 291, 366, 5176, 281, 1401, 294, 341, 293, 4188, 322, 309, 293, 550, 291, 722, 766, 264, 51430], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1614, "seek": 622306, "start": 6244.360000000001, "end": 6246.46, "text": " generation with a special token called start.", "tokens": [51430, 5125, 365, 257, 2121, 14862, 1219, 722, 13, 51535], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1615, "seek": 622306, "start": 6246.76, "end": 6252.860000000001, "text": " So this is a special new token that you introduce and always place in the beginning and then", "tokens": [51550, 407, 341, 307, 257, 2121, 777, 14862, 300, 291, 5366, 293, 1009, 1081, 294, 264, 2863, 293, 550, 51855], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1616, "seek": 622306, "start": 6252.860000000001, "end": 6252.96, "text": " the.", "tokens": [51855, 264, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12217058958830657, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.00011764256487367675}, {"id": 1617, "seek": 625306, "start": 6253.160000000001, "end": 6258.76, "text": " Network is expected to output neural networks are awesome and then a special end token to", "tokens": [50370, 12640, 307, 5176, 281, 5598, 18161, 9590, 366, 3476, 293, 550, 257, 2121, 917, 14862, 281, 50650], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1618, "seek": 625306, "start": 6258.76, "end": 6259.660000000001, "text": " finish the generation.", "tokens": [50650, 2413, 264, 5125, 13, 50695], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1619, "seek": 625306, "start": 6261.06, "end": 6266.26, "text": " So this part here will be decoded exactly as we have we've done it neural networks are", "tokens": [50765, 407, 341, 644, 510, 486, 312, 979, 12340, 2293, 382, 321, 362, 321, 600, 1096, 309, 18161, 9590, 366, 51025], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1620, "seek": 625306, "start": 6266.26, "end": 6272.360000000001, "text": " awesome will be identical to what we did but unlike what we did they want to condition the", "tokens": [51025, 3476, 486, 312, 14800, 281, 437, 321, 630, 457, 8343, 437, 321, 630, 436, 528, 281, 4188, 264, 51330], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1621, "seek": 625306, "start": 6272.360000000001, "end": 6275.56, "text": " generation on some additional information.", "tokens": [51330, 5125, 322, 512, 4497, 1589, 13, 51490], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1622, "seek": 625306, "start": 6275.660000000001, "end": 6279.160000000001, "text": " And in that case this additional information is the French sentence that they should be", "tokens": [51495, 400, 294, 300, 1389, 341, 4497, 1589, 307, 264, 5522, 8174, 300, 436, 820, 312, 51670], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1623, "seek": 625306, "start": 6279.160000000001, "end": 6279.660000000001, "text": " translating.", "tokens": [51670, 35030, 13, 51695], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1624, "seek": 625306, "start": 6280.660000000001, "end": 6282.860000000001, "text": " So what they do now is they.", "tokens": [51745, 407, 437, 436, 360, 586, 307, 436, 13, 51855], "temperature": 0.0, "avg_logprob": -0.17194371853234633, "compression_ratio": 1.874493927125506, "no_speech_prob": 0.00022503416403196752}, {"id": 1625, "seek": 628286, "start": 6282.96, "end": 6287.259999999999, "text": " Bring the encoder now the encoder reads this part here.", "tokens": [50370, 12842, 264, 2058, 19866, 586, 264, 2058, 19866, 15700, 341, 644, 510, 13, 50585], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1626, "seek": 628286, "start": 6287.759999999999, "end": 6293.259999999999, "text": " So we're all going to take the part of French and we're going to create tokens from it exactly as", "tokens": [50610, 407, 321, 434, 439, 516, 281, 747, 264, 644, 295, 5522, 293, 321, 434, 516, 281, 1884, 22667, 490, 309, 2293, 382, 50885], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1627, "seek": 628286, "start": 6293.259999999999, "end": 6298.0599999999995, "text": " we've seen in our video and we're going to put a transformer on it, but there's going to be no", "tokens": [50885, 321, 600, 1612, 294, 527, 960, 293, 321, 434, 516, 281, 829, 257, 31782, 322, 309, 11, 457, 456, 311, 516, 281, 312, 572, 51125], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1628, "seek": 628286, "start": 6298.0599999999995, "end": 6299.46, "text": " triangular mask.", "tokens": [51125, 38190, 6094, 13, 51195], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1629, "seek": 628286, "start": 6299.5599999999995, "end": 6303.36, "text": " And so all the tokens are allowed to talk to each other as much as they want and they're just", "tokens": [51200, 400, 370, 439, 264, 22667, 366, 4350, 281, 751, 281, 1184, 661, 382, 709, 382, 436, 528, 293, 436, 434, 445, 51390], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1630, "seek": 628286, "start": 6303.36, "end": 6311.16, "text": " encoding whatever the content of this French sentence once they've encoded it they've they", "tokens": [51390, 43430, 2035, 264, 2701, 295, 341, 5522, 8174, 1564, 436, 600, 2058, 12340, 309, 436, 600, 436, 51780], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1631, "seek": 628286, "start": 6311.16, "end": 6312.759999999999, "text": " basically come out in the top here.", "tokens": [51780, 1936, 808, 484, 294, 264, 1192, 510, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1251485483433173, "compression_ratio": 1.8202247191011236, "no_speech_prob": 5.8709676522994414e-05}, {"id": 1632, "seek": 631286, "start": 6313.36, "end": 6318.259999999999, "text": " And then what happens here is in our decoder which does the language modeling.", "tokens": [50390, 400, 550, 437, 2314, 510, 307, 294, 527, 979, 19866, 597, 775, 264, 2856, 15983, 13, 50635], "temperature": 0.0, "avg_logprob": -0.1155206940390847, "compression_ratio": 2.052401746724891, "no_speech_prob": 0.00029849729617126286}, {"id": 1633, "seek": 631286, "start": 6318.66, "end": 6324.5599999999995, "text": " There's an additional connection here to the outputs of the encoder and that is brought in", "tokens": [50655, 821, 311, 364, 4497, 4984, 510, 281, 264, 23930, 295, 264, 2058, 19866, 293, 300, 307, 3038, 294, 50950], "temperature": 0.0, "avg_logprob": -0.1155206940390847, "compression_ratio": 2.052401746724891, "no_speech_prob": 0.00029849729617126286}, {"id": 1634, "seek": 631286, "start": 6324.5599999999995, "end": 6326.46, "text": " through a cross attention.", "tokens": [50950, 807, 257, 3278, 3202, 13, 51045], "temperature": 0.0, "avg_logprob": -0.1155206940390847, "compression_ratio": 2.052401746724891, "no_speech_prob": 0.00029849729617126286}, {"id": 1635, "seek": 631286, "start": 6327.0599999999995, "end": 6331.66, "text": " So the queries are still generated from X but now the keys and the values are coming from the", "tokens": [51075, 407, 264, 24109, 366, 920, 10833, 490, 1783, 457, 586, 264, 9317, 293, 264, 4190, 366, 1348, 490, 264, 51305], "temperature": 0.0, "avg_logprob": -0.1155206940390847, "compression_ratio": 2.052401746724891, "no_speech_prob": 0.00029849729617126286}, {"id": 1636, "seek": 631286, "start": 6331.66, "end": 6337.259999999999, "text": " side the keys and the values are coming from the top generated by the nodes that came outside", "tokens": [51305, 1252, 264, 9317, 293, 264, 4190, 366, 1348, 490, 264, 1192, 10833, 538, 264, 13891, 300, 1361, 2380, 51585], "temperature": 0.0, "avg_logprob": -0.1155206940390847, "compression_ratio": 2.052401746724891, "no_speech_prob": 0.00029849729617126286}, {"id": 1637, "seek": 631286, "start": 6337.259999999999, "end": 6342.759999999999, "text": " of the decode the encoder and those tops the keys and the values there the top of it.", "tokens": [51585, 295, 264, 979, 1429, 264, 2058, 19866, 293, 729, 22836, 264, 9317, 293, 264, 4190, 456, 264, 1192, 295, 309, 13, 51860], "temperature": 0.0, "avg_logprob": -0.1155206940390847, "compression_ratio": 2.052401746724891, "no_speech_prob": 0.00029849729617126286}, {"id": 1638, "seek": 634286, "start": 6343.36, "end": 6348.66, "text": " Feed in on the side into every single block of the decoder and so that's why there's an additional", "tokens": [50390, 33720, 294, 322, 264, 1252, 666, 633, 2167, 3461, 295, 264, 979, 19866, 293, 370, 300, 311, 983, 456, 311, 364, 4497, 50655], "temperature": 0.0, "avg_logprob": -0.15554959549863115, "compression_ratio": 1.864, "no_speech_prob": 0.00025129373534582555}, {"id": 1639, "seek": 634286, "start": 6348.66, "end": 6354.86, "text": " cross attention and really what is doing is it's conditioning the decoding not just on the past of", "tokens": [50655, 3278, 3202, 293, 534, 437, 307, 884, 307, 309, 311, 21901, 264, 979, 8616, 406, 445, 322, 264, 1791, 295, 50965], "temperature": 0.0, "avg_logprob": -0.15554959549863115, "compression_ratio": 1.864, "no_speech_prob": 0.00025129373534582555}, {"id": 1640, "seek": 634286, "start": 6354.86, "end": 6365.0599999999995, "text": " this current decoding but also on having seen the full fully encoded French prompt sort of and so", "tokens": [50965, 341, 2190, 979, 8616, 457, 611, 322, 1419, 1612, 264, 1577, 4498, 2058, 12340, 5522, 12391, 1333, 295, 293, 370, 51475], "temperature": 0.0, "avg_logprob": -0.15554959549863115, "compression_ratio": 1.864, "no_speech_prob": 0.00025129373534582555}, {"id": 1641, "seek": 634286, "start": 6365.0599999999995, "end": 6368.96, "text": " it's an encoder decoder model, which is why we have those two transformers and additional block", "tokens": [51475, 309, 311, 364, 2058, 19866, 979, 19866, 2316, 11, 597, 307, 983, 321, 362, 729, 732, 4088, 433, 293, 4497, 3461, 51670], "temperature": 0.0, "avg_logprob": -0.15554959549863115, "compression_ratio": 1.864, "no_speech_prob": 0.00025129373534582555}, {"id": 1642, "seek": 634286, "start": 6369.259999999999, "end": 6369.86, "text": " and so on.", "tokens": [51685, 293, 370, 322, 13, 51715], "temperature": 0.0, "avg_logprob": -0.15554959549863115, "compression_ratio": 1.864, "no_speech_prob": 0.00025129373534582555}, {"id": 1643, "seek": 634286, "start": 6370.16, "end": 6372.66, "text": " So we did not do this because we have no we have nothing to do.", "tokens": [51730, 407, 321, 630, 406, 360, 341, 570, 321, 362, 572, 321, 362, 1825, 281, 360, 13, 51855], "temperature": 0.0, "avg_logprob": -0.15554959549863115, "compression_ratio": 1.864, "no_speech_prob": 0.00025129373534582555}, {"id": 1644, "seek": 637266, "start": 6372.66, "end": 6373.46, "text": " Nothing to encode.", "tokens": [50365, 6693, 281, 2058, 1429, 13, 50405], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1645, "seek": 637266, "start": 6373.46, "end": 6374.46, "text": " There's no conditioning.", "tokens": [50405, 821, 311, 572, 21901, 13, 50455], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1646, "seek": 637266, "start": 6374.46, "end": 6378.96, "text": " We just have a text file and we just want to imitate it and that's why we are using a decoder only", "tokens": [50455, 492, 445, 362, 257, 2487, 3991, 293, 321, 445, 528, 281, 35556, 309, 293, 300, 311, 983, 321, 366, 1228, 257, 979, 19866, 787, 50680], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1647, "seek": 637266, "start": 6378.96, "end": 6381.66, "text": " transformer exactly as done in GPT.", "tokens": [50680, 31782, 2293, 382, 1096, 294, 26039, 51, 13, 50815], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1648, "seek": 637266, "start": 6382.86, "end": 6383.0599999999995, "text": " Okay.", "tokens": [50875, 1033, 13, 50885], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1649, "seek": 637266, "start": 6383.0599999999995, "end": 6388.96, "text": " So now I wanted to do a very brief walkthrough of nano GPT, which you can find in my GitHub and nano", "tokens": [50885, 407, 586, 286, 1415, 281, 360, 257, 588, 5353, 1792, 11529, 295, 30129, 26039, 51, 11, 597, 291, 393, 915, 294, 452, 23331, 293, 30129, 51180], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1650, "seek": 637266, "start": 6388.96, "end": 6391.0599999999995, "text": " GPT is basically two files of interest.", "tokens": [51180, 26039, 51, 307, 1936, 732, 7098, 295, 1179, 13, 51285], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1651, "seek": 637266, "start": 6391.26, "end": 6396.86, "text": " There's train.pi and model.pi train.pi is all the boilerplate code for training the network.", "tokens": [51295, 821, 311, 3847, 13, 22630, 293, 2316, 13, 22630, 3847, 13, 22630, 307, 439, 264, 39228, 37008, 3089, 337, 3097, 264, 3209, 13, 51575], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1652, "seek": 637266, "start": 6397.0599999999995, "end": 6401.0599999999995, "text": " It is basically all the stuff that we had here is the training loop.", "tokens": [51585, 467, 307, 1936, 439, 264, 1507, 300, 321, 632, 510, 307, 264, 3097, 6367, 13, 51785], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1653, "seek": 637266, "start": 6401.96, "end": 6402.46, "text": " It's just that.", "tokens": [51830, 467, 311, 445, 300, 13, 51855], "temperature": 0.0, "avg_logprob": -0.16867286842186135, "compression_ratio": 1.6993243243243243, "no_speech_prob": 8.647513459436595e-05}, {"id": 1654, "seek": 640246, "start": 6402.46, "end": 6406.66, "text": " It's a lot more complicated because we're saving and loading checkpoints and pre-trained weights", "tokens": [50365, 467, 311, 257, 688, 544, 6179, 570, 321, 434, 6816, 293, 15114, 1520, 20552, 293, 659, 12, 17227, 2001, 17443, 50575], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1655, "seek": 640246, "start": 6406.66, "end": 6411.66, "text": " and we are decaying the learning rate and compiling the model and using distributed training across", "tokens": [50575, 293, 321, 366, 21039, 278, 264, 2539, 3314, 293, 715, 4883, 264, 2316, 293, 1228, 12631, 3097, 2108, 50825], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1656, "seek": 640246, "start": 6411.66, "end": 6413.26, "text": " multiple nodes or GPUs.", "tokens": [50825, 3866, 13891, 420, 18407, 82, 13, 50905], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1657, "seek": 640246, "start": 6413.76, "end": 6417.06, "text": " So the training that Pi gets a little bit more hairy, complicated.", "tokens": [50930, 407, 264, 3097, 300, 17741, 2170, 257, 707, 857, 544, 42346, 11, 6179, 13, 51095], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1658, "seek": 640246, "start": 6417.46, "end": 6423.66, "text": " There's more options Etc, but the model that I should look very very similar to what we've done", "tokens": [51115, 821, 311, 544, 3956, 3790, 66, 11, 457, 264, 2316, 300, 286, 820, 574, 588, 588, 2531, 281, 437, 321, 600, 1096, 51425], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1659, "seek": 640246, "start": 6423.66, "end": 6424.06, "text": " here.", "tokens": [51425, 510, 13, 51445], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1660, "seek": 640246, "start": 6424.26, "end": 6426.46, "text": " In fact, the model is almost identical.", "tokens": [51455, 682, 1186, 11, 264, 2316, 307, 1920, 14800, 13, 51565], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1661, "seek": 640246, "start": 6427.26, "end": 6432.36, "text": " So first here we have the causal self-attention block and all of this should look very very", "tokens": [51605, 407, 700, 510, 321, 362, 264, 38755, 2698, 12, 1591, 1251, 3461, 293, 439, 295, 341, 820, 574, 588, 588, 51860], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1662, "seek": 640246, "start": 6432.36, "end": 6432.46, "text": " very similar.", "tokens": [51860, 588, 2531, 13, 51865], "temperature": 0.8, "avg_logprob": -0.22247925378326186, "compression_ratio": 1.7953020134228188, "no_speech_prob": 1.8415646991343237e-05}, {"id": 1663, "seek": 643246, "start": 6432.46, "end": 6437.9800000000005, "text": " recognizable to you we're producing queries keys values we're doing dot products we're masking", "tokens": [50365, 40757, 281, 291, 321, 434, 10501, 24109, 9317, 4190, 321, 434, 884, 5893, 3383, 321, 434, 31226, 50641], "temperature": 0.0, "avg_logprob": -0.06580148508519303, "compression_ratio": 1.7, "no_speech_prob": 0.01386045478284359}, {"id": 1664, "seek": 643246, "start": 6437.9800000000005, "end": 6445.26, "text": " applying softmax optionally dropping out and here we are pooling the values what is different here", "tokens": [50641, 9275, 2787, 41167, 3614, 379, 13601, 484, 293, 510, 321, 366, 7005, 278, 264, 4190, 437, 307, 819, 510, 51005], "temperature": 0.0, "avg_logprob": -0.06580148508519303, "compression_ratio": 1.7, "no_speech_prob": 0.01386045478284359}, {"id": 1665, "seek": 643246, "start": 6445.26, "end": 6452.1, "text": " is that in our code i have separated out the multi-headed attention into just a single", "tokens": [51005, 307, 300, 294, 527, 3089, 741, 362, 12005, 484, 264, 4825, 12, 28409, 3202, 666, 445, 257, 2167, 51347], "temperature": 0.0, "avg_logprob": -0.06580148508519303, "compression_ratio": 1.7, "no_speech_prob": 0.01386045478284359}, {"id": 1666, "seek": 643246, "start": 6452.1, "end": 6458.1, "text": " individual head and then here i have multiple heads and i explicitly concatenate them whereas", "tokens": [51347, 2609, 1378, 293, 550, 510, 741, 362, 3866, 8050, 293, 741, 20803, 1588, 7186, 473, 552, 9735, 51647], "temperature": 0.0, "avg_logprob": -0.06580148508519303, "compression_ratio": 1.7, "no_speech_prob": 0.01386045478284359}, {"id": 1667, "seek": 645810, "start": 6458.1, "end": 6463.18, "text": " here all of it is implemented in a batched manner inside a single causal self-attention", "tokens": [50365, 510, 439, 295, 309, 307, 12270, 294, 257, 15245, 292, 9060, 1854, 257, 2167, 38755, 2698, 12, 1591, 1251, 50619], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1668, "seek": 645810, "start": 6463.18, "end": 6468.120000000001, "text": " and so we don't just have a b and a t and a c dimension we also end up with a fourth dimension", "tokens": [50619, 293, 370, 321, 500, 380, 445, 362, 257, 272, 293, 257, 256, 293, 257, 269, 10139, 321, 611, 917, 493, 365, 257, 6409, 10139, 50866], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1669, "seek": 645810, "start": 6468.120000000001, "end": 6473.320000000001, "text": " which is the heads and so it just gets a lot more sort of hairy because we have four-dimensional", "tokens": [50866, 597, 307, 264, 8050, 293, 370, 309, 445, 2170, 257, 688, 544, 1333, 295, 42346, 570, 321, 362, 1451, 12, 18759, 51126], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1670, "seek": 645810, "start": 6473.320000000001, "end": 6479.360000000001, "text": " array tensors now but it is equivalent mathematically so the exact same thing is", "tokens": [51126, 10225, 10688, 830, 586, 457, 309, 307, 10344, 44003, 370, 264, 1900, 912, 551, 307, 51428], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1671, "seek": 645810, "start": 6479.360000000001, "end": 6483.740000000001, "text": " happening as what we have it's just it's a bit more efficient because all the heads are now", "tokens": [51428, 2737, 382, 437, 321, 362, 309, 311, 445, 309, 311, 257, 857, 544, 7148, 570, 439, 264, 8050, 366, 586, 51647], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1672, "seek": 645810, "start": 6483.740000000001, "end": 6488.0, "text": " treated as a batch dimension as well then we have the multi-layered perceptron", "tokens": [51647, 8668, 382, 257, 15245, 10139, 382, 731, 550, 321, 362, 264, 4825, 12, 8376, 4073, 43276, 2044, 51860], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1673, "seek": 645810, "start": 6488.0, "end": 6488.08, "text": " and we have the multi-layered perceptron and we have the multi-layered perceptron", "tokens": [51860, 293, 321, 362, 264, 4825, 12, 8376, 4073, 43276, 2044, 293, 321, 362, 264, 4825, 12, 8376, 4073, 43276, 2044, 51864], "temperature": 0.0, "avg_logprob": -0.06421286222950512, "compression_ratio": 2.1284722222222223, "no_speech_prob": 5.43109345017001e-05}, {"id": 1674, "seek": 648810, "start": 6488.1, "end": 6493.56, "text": " it's using the gelu non-linearity which is defined here except instead of relu and this", "tokens": [50365, 309, 311, 1228, 264, 4087, 84, 2107, 12, 1889, 17409, 597, 307, 7642, 510, 3993, 2602, 295, 1039, 84, 293, 341, 50638], "temperature": 0.0, "avg_logprob": -0.043003761291503906, "compression_ratio": 1.8466898954703832, "no_speech_prob": 0.00028465778450481594}, {"id": 1675, "seek": 648810, "start": 6493.56, "end": 6496.52, "text": " is done just because openly i used it and i want to be able to load their checkpoints", "tokens": [50638, 307, 1096, 445, 570, 23109, 741, 1143, 309, 293, 741, 528, 281, 312, 1075, 281, 3677, 641, 1520, 20552, 50786], "temperature": 0.0, "avg_logprob": -0.043003761291503906, "compression_ratio": 1.8466898954703832, "no_speech_prob": 0.00028465778450481594}, {"id": 1676, "seek": 648810, "start": 6496.52, "end": 6502.56, "text": " the blocks of the transformer are identical the communicate and the compute phase as we saw", "tokens": [50786, 264, 8474, 295, 264, 31782, 366, 14800, 264, 7890, 293, 264, 14722, 5574, 382, 321, 1866, 51088], "temperature": 0.0, "avg_logprob": -0.043003761291503906, "compression_ratio": 1.8466898954703832, "no_speech_prob": 0.00028465778450481594}, {"id": 1677, "seek": 648810, "start": 6502.56, "end": 6507.18, "text": " and then the gpt will be identical we have the position encodings token encodings", "tokens": [51088, 293, 550, 264, 290, 662, 486, 312, 14800, 321, 362, 264, 2535, 2058, 378, 1109, 14862, 2058, 378, 1109, 51319], "temperature": 0.0, "avg_logprob": -0.043003761291503906, "compression_ratio": 1.8466898954703832, "no_speech_prob": 0.00028465778450481594}, {"id": 1678, "seek": 648810, "start": 6507.18, "end": 6513.6, "text": " the blocks the layer norm at the end the final linear layer and this should look all very", "tokens": [51319, 264, 8474, 264, 4583, 2026, 412, 264, 917, 264, 2572, 8213, 4583, 293, 341, 820, 574, 439, 588, 51640], "temperature": 0.0, "avg_logprob": -0.043003761291503906, "compression_ratio": 1.8466898954703832, "no_speech_prob": 0.00028465778450481594}, {"id": 1679, "seek": 648810, "start": 6513.6, "end": 6518.0, "text": " recognizable and there's a bit more here because i'm loading checkpoints and stuff like that", "tokens": [51640, 40757, 293, 456, 311, 257, 857, 544, 510, 570, 741, 478, 15114, 1520, 20552, 293, 1507, 411, 300, 51860], "temperature": 0.0, "avg_logprob": -0.043003761291503906, "compression_ratio": 1.8466898954703832, "no_speech_prob": 0.00028465778450481594}, {"id": 1680, "seek": 651800, "start": 6518.0, "end": 6522.16, "text": " i'm separating out the parameters into those that should be weight decayed and those that", "tokens": [50365, 741, 478, 29279, 484, 264, 9834, 666, 729, 300, 820, 312, 3364, 21039, 292, 293, 729, 300, 50573], "temperature": 0.0, "avg_logprob": -0.05711323724073522, "compression_ratio": 1.9464882943143813, "no_speech_prob": 0.0043292720802128315}, {"id": 1681, "seek": 651800, "start": 6522.16, "end": 6528.24, "text": " shouldn't but the generate function should also be very very similar so a few details are different", "tokens": [50573, 4659, 380, 457, 264, 8460, 2445, 820, 611, 312, 588, 588, 2531, 370, 257, 1326, 4365, 366, 819, 50877], "temperature": 0.0, "avg_logprob": -0.05711323724073522, "compression_ratio": 1.9464882943143813, "no_speech_prob": 0.0043292720802128315}, {"id": 1682, "seek": 651800, "start": 6528.24, "end": 6533.16, "text": " but you should definitely be able to look at this file and be able to understand a lot of the pieces", "tokens": [50877, 457, 291, 820, 2138, 312, 1075, 281, 574, 412, 341, 3991, 293, 312, 1075, 281, 1223, 257, 688, 295, 264, 3755, 51123], "temperature": 0.0, "avg_logprob": -0.05711323724073522, "compression_ratio": 1.9464882943143813, "no_speech_prob": 0.0043292720802128315}, {"id": 1683, "seek": 651800, "start": 6533.16, "end": 6538.44, "text": " now so let's now bring things back to chat gpt what would it look like if we wanted to train", "tokens": [51123, 586, 370, 718, 311, 586, 1565, 721, 646, 281, 5081, 290, 662, 437, 576, 309, 574, 411, 498, 321, 1415, 281, 3847, 51387], "temperature": 0.0, "avg_logprob": -0.05711323724073522, "compression_ratio": 1.9464882943143813, "no_speech_prob": 0.0043292720802128315}, {"id": 1684, "seek": 651800, "start": 6538.44, "end": 6543.68, "text": " chat gpt ourselves and how does it relate to what we learned today well to train the chat gpt there", "tokens": [51387, 5081, 290, 662, 4175, 293, 577, 775, 309, 10961, 281, 437, 321, 3264, 965, 731, 281, 3847, 264, 5081, 290, 662, 456, 51649], "temperature": 0.0, "avg_logprob": -0.05711323724073522, "compression_ratio": 1.9464882943143813, "no_speech_prob": 0.0043292720802128315}, {"id": 1685, "seek": 651800, "start": 6543.68, "end": 6547.98, "text": " are roughly two stages first is the pre-training stage and then the fine-tuning stage and then the", "tokens": [51649, 366, 9810, 732, 10232, 700, 307, 264, 659, 12, 17227, 1760, 3233, 293, 550, 264, 2489, 12, 83, 37726, 3233, 293, 550, 264, 51864], "temperature": 0.0, "avg_logprob": -0.05711323724073522, "compression_ratio": 1.9464882943143813, "no_speech_prob": 0.0043292720802128315}, {"id": 1686, "seek": 654800, "start": 6548.0, "end": 6553.92, "text": " pre-training stage in the pre-training stage we are training on a large chunk of internet and just", "tokens": [50365, 659, 12, 17227, 1760, 3233, 294, 264, 659, 12, 17227, 1760, 3233, 321, 366, 3097, 322, 257, 2416, 16635, 295, 4705, 293, 445, 50661], "temperature": 0.0, "avg_logprob": -0.0700517663168251, "compression_ratio": 1.796, "no_speech_prob": 0.0006337338127195835}, {"id": 1687, "seek": 654800, "start": 6553.92, "end": 6560.4, "text": " trying to get a first decoder only transformer to babble text so it's very very similar to what", "tokens": [50661, 1382, 281, 483, 257, 700, 979, 19866, 787, 31782, 281, 7564, 638, 2487, 370, 309, 311, 588, 588, 2531, 281, 437, 50985], "temperature": 0.0, "avg_logprob": -0.0700517663168251, "compression_ratio": 1.796, "no_speech_prob": 0.0006337338127195835}, {"id": 1688, "seek": 654800, "start": 6560.4, "end": 6567.92, "text": " we've done ourselves except we've done like a tiny little baby pre-training step and so in our case", "tokens": [50985, 321, 600, 1096, 4175, 3993, 321, 600, 1096, 411, 257, 5870, 707, 3186, 659, 12, 17227, 1760, 1823, 293, 370, 294, 527, 1389, 51361], "temperature": 0.0, "avg_logprob": -0.0700517663168251, "compression_ratio": 1.796, "no_speech_prob": 0.0006337338127195835}, {"id": 1689, "seek": 654800, "start": 6569.36, "end": 6573.44, "text": " this is how you print a number of parameters i printed it and it's about 10 million", "tokens": [51433, 341, 307, 577, 291, 4482, 257, 1230, 295, 9834, 741, 13567, 309, 293, 309, 311, 466, 1266, 2459, 51637], "temperature": 0.0, "avg_logprob": -0.0700517663168251, "compression_ratio": 1.796, "no_speech_prob": 0.0006337338127195835}, {"id": 1690, "seek": 654800, "start": 6573.44, "end": 6577.12, "text": " so this transformer that i created here to create a little shakespeare", "tokens": [51637, 370, 341, 31782, 300, 741, 2942, 510, 281, 1884, 257, 707, 37891, 22446, 51821], "temperature": 0.0, "avg_logprob": -0.0700517663168251, "compression_ratio": 1.796, "no_speech_prob": 0.0006337338127195835}, {"id": 1691, "seek": 657800, "start": 6578.0, "end": 6585.14, "text": " transformer was about 10 million parameters our data set is roughly 1 million characters so", "tokens": [50365, 31782, 390, 466, 1266, 2459, 9834, 527, 1412, 992, 307, 9810, 502, 2459, 4342, 370, 50722], "temperature": 0.0, "avg_logprob": -0.060921323542692224, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0006094354321248829}, {"id": 1692, "seek": 657800, "start": 6585.14, "end": 6589.9, "text": " roughly 1 million tokens but you have to remember that openai uses different vocabulary they're not", "tokens": [50722, 9810, 502, 2459, 22667, 457, 291, 362, 281, 1604, 300, 1269, 1301, 4960, 819, 19864, 436, 434, 406, 50960], "temperature": 0.0, "avg_logprob": -0.060921323542692224, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0006094354321248829}, {"id": 1693, "seek": 657800, "start": 6589.9, "end": 6595.48, "text": " on the character level they use these subword chunks of words and so they have a vocabulary", "tokens": [50960, 322, 264, 2517, 1496, 436, 764, 613, 1422, 7462, 24004, 295, 2283, 293, 370, 436, 362, 257, 19864, 51239], "temperature": 0.0, "avg_logprob": -0.060921323542692224, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0006094354321248829}, {"id": 1694, "seek": 657800, "start": 6595.48, "end": 6602.64, "text": " of 50 000 roughly elements and so their sequences are a bit more condensed so our data set the", "tokens": [51239, 295, 2625, 13711, 9810, 4959, 293, 370, 641, 22978, 366, 257, 857, 544, 36398, 370, 527, 1412, 992, 264, 51597], "temperature": 0.0, "avg_logprob": -0.060921323542692224, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0006094354321248829}, {"id": 1695, "seek": 657800, "start": 6602.64, "end": 6607.84, "text": " shakespeare data set would be probably around 300 000 tokens in the openai vocabulary roughly", "tokens": [51597, 37891, 22446, 1412, 992, 576, 312, 1391, 926, 6641, 13711, 22667, 294, 264, 1269, 1301, 19864, 9810, 51857], "temperature": 0.0, "avg_logprob": -0.060921323542692224, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.0006094354321248829}, {"id": 1696, "seek": 660800, "start": 6608.48, "end": 6615.52, "text": " so we trained about 10 million parameter model on roughly 300 000 tokens now when you go to the gpt3 paper", "tokens": [50389, 370, 321, 8895, 466, 1266, 2459, 13075, 2316, 322, 9810, 6641, 13711, 22667, 586, 562, 291, 352, 281, 264, 290, 662, 18, 3035, 50741], "temperature": 1.0, "avg_logprob": -0.24215402205785116, "compression_ratio": 2.0093457943925235, "no_speech_prob": 0.00023495161440223455}, {"id": 1697, "seek": 660800, "start": 6616.96, "end": 6622.4, "text": " and you look at the transformers that they trained they trained a number of transformers", "tokens": [50813, 293, 291, 574, 412, 264, 4088, 433, 300, 436, 8895, 436, 8895, 257, 1230, 295, 4088, 433, 51085], "temperature": 1.0, "avg_logprob": -0.24215402205785116, "compression_ratio": 2.0093457943925235, "no_speech_prob": 0.00023495161440223455}, {"id": 1698, "seek": 660800, "start": 6622.4, "end": 6628.64, "text": " of different sizes but the biggest transformer here has 175 billion parameters uh so ours is", "tokens": [51085, 295, 819, 11602, 457, 264, 3880, 31782, 510, 575, 41165, 5218, 9834, 2232, 370, 11896, 307, 51397], "temperature": 1.0, "avg_logprob": -0.24215402205785116, "compression_ratio": 2.0093457943925235, "no_speech_prob": 0.00023495161440223455}, {"id": 1699, "seek": 660800, "start": 6628.64, "end": 6635.04, "text": " again 10 million they used this number of layers in the transformer this is the n embed this is", "tokens": [51397, 797, 1266, 2459, 436, 1143, 341, 1230, 295, 7914, 294, 264, 31782, 341, 307, 264, 297, 12240, 341, 307, 51717], "temperature": 1.0, "avg_logprob": -0.24215402205785116, "compression_ratio": 2.0093457943925235, "no_speech_prob": 0.00023495161440223455}, {"id": 1700, "seek": 660800, "start": 6635.04, "end": 6637.62, "text": " the number of heads and this is the head size", "tokens": [51717, 264, 1230, 295, 8050, 293, 341, 307, 264, 1378, 2744, 51846], "temperature": 1.0, "avg_logprob": -0.24215402205785116, "compression_ratio": 2.0093457943925235, "no_speech_prob": 0.00023495161440223455}, {"id": 1701, "seek": 663800, "start": 6638.0, "end": 6645.44, "text": " and then this is the batch size so ours was 65 and the learning rate is similar", "tokens": [50365, 293, 550, 341, 307, 264, 15245, 2744, 370, 11896, 390, 11624, 293, 264, 2539, 3314, 307, 2531, 50737], "temperature": 0.0, "avg_logprob": -0.11124801635742188, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.19754858314990997}, {"id": 1702, "seek": 663800, "start": 6645.44, "end": 6650.6, "text": " now when they train this transformer they trained on 300 billion tokens so", "tokens": [50737, 586, 562, 436, 3847, 341, 31782, 436, 8895, 322, 6641, 5218, 22667, 370, 50995], "temperature": 0.0, "avg_logprob": -0.11124801635742188, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.19754858314990997}, {"id": 1703, "seek": 663800, "start": 6650.6, "end": 6656.32, "text": " again remember ours is about 300,000 so this is about a million fold increase", "tokens": [50995, 797, 1604, 11896, 307, 466, 6641, 11, 1360, 370, 341, 307, 466, 257, 2459, 4860, 3488, 51281], "temperature": 0.0, "avg_logprob": -0.11124801635742188, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.19754858314990997}, {"id": 1704, "seek": 663800, "start": 6656.32, "end": 6659.58, "text": " and this number would not be even that large by today's standards you'd be", "tokens": [51281, 293, 341, 1230, 576, 406, 312, 754, 300, 2416, 538, 965, 311, 7787, 291, 1116, 312, 51444], "temperature": 0.0, "avg_logprob": -0.11124801635742188, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.19754858314990997}, {"id": 1705, "seek": 663800, "start": 6659.58, "end": 6665.66, "text": " going up 1 trillion and above so they are training a significantly larger", "tokens": [51444, 516, 493, 502, 18723, 293, 3673, 370, 436, 366, 3097, 257, 10591, 4833, 51748], "temperature": 0.0, "avg_logprob": -0.11124801635742188, "compression_ratio": 1.6784140969162995, "no_speech_prob": 0.19754858314990997}, {"id": 1706, "seek": 666566, "start": 6665.66, "end": 6672.0199999999995, "text": " model on a good chunk of the internet and that is the pre-training stage but", "tokens": [50365, 2316, 322, 257, 665, 16635, 295, 264, 4705, 293, 300, 307, 264, 659, 12, 17227, 1760, 3233, 457, 50683], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1707, "seek": 666566, "start": 6672.0199999999995, "end": 6675.38, "text": " otherwise these hyperparameters should be fairly recognizable to you and the", "tokens": [50683, 5911, 613, 9848, 2181, 335, 6202, 820, 312, 6457, 40757, 281, 291, 293, 264, 50851], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1708, "seek": 666566, "start": 6675.38, "end": 6678.42, "text": " architecture is actually like nearly identical to what we implemented", "tokens": [50851, 9482, 307, 767, 411, 6217, 14800, 281, 437, 321, 12270, 51003], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1709, "seek": 666566, "start": 6678.42, "end": 6682.46, "text": " ourselves but of course it's a massive infrastructure challenge to train this", "tokens": [51003, 4175, 457, 295, 1164, 309, 311, 257, 5994, 6896, 3430, 281, 3847, 341, 51205], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1710, "seek": 666566, "start": 6682.46, "end": 6687.139999999999, "text": " you're talking about typically thousands of GPUs having to you know talk to each", "tokens": [51205, 291, 434, 1417, 466, 5850, 5383, 295, 18407, 82, 1419, 281, 291, 458, 751, 281, 1184, 51439], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1711, "seek": 666566, "start": 6687.139999999999, "end": 6691.88, "text": " other to train models of this size so that's just a pre-training stage now", "tokens": [51439, 661, 281, 3847, 5245, 295, 341, 2744, 370, 300, 311, 445, 257, 659, 12, 17227, 1760, 3233, 586, 51676], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1712, "seek": 666566, "start": 6691.88, "end": 6695.54, "text": " after you complete the pre-training stage you don't get something that", "tokens": [51676, 934, 291, 3566, 264, 659, 12, 17227, 1760, 3233, 291, 500, 380, 483, 746, 300, 51859], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1713, "seek": 666566, "start": 6695.54, "end": 6695.639999999999, "text": " you don't get something that you don't get something that you don't get", "tokens": [51859, 291, 500, 380, 483, 746, 300, 291, 500, 380, 483, 746, 300, 291, 500, 380, 483, 51864], "temperature": 0.0, "avg_logprob": -0.13343176634415335, "compression_ratio": 1.9933554817275747, "no_speech_prob": 0.0004874218429904431}, {"id": 1714, "seek": 669564, "start": 6695.64, "end": 6700.240000000001, "text": " a response to your questions with answers and it's not helpful and etc you", "tokens": [50365, 257, 4134, 281, 428, 1651, 365, 6338, 293, 309, 311, 406, 4961, 293, 5183, 291, 50595], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1715, "seek": 669564, "start": 6700.240000000001, "end": 6705.64, "text": " get a document completer right so it babbles but it doesn't babble Shakespeare", "tokens": [50595, 483, 257, 4166, 1557, 260, 558, 370, 309, 7564, 8806, 457, 309, 1177, 380, 7564, 638, 22825, 50865], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1716, "seek": 669564, "start": 6705.64, "end": 6709.56, "text": " it babbles internet it will create arbitrary news articles and documents", "tokens": [50865, 309, 7564, 8806, 4705, 309, 486, 1884, 23211, 2583, 11290, 293, 8512, 51061], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1717, "seek": 669564, "start": 6709.56, "end": 6712.08, "text": " and it will try to complete documents because that's what it's trained for", "tokens": [51061, 293, 309, 486, 853, 281, 3566, 8512, 570, 300, 311, 437, 309, 311, 8895, 337, 51187], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1718, "seek": 669564, "start": 6712.08, "end": 6715.740000000001, "text": " it's trying to complete the sequence so when you give it a question it would", "tokens": [51187, 309, 311, 1382, 281, 3566, 264, 8310, 370, 562, 291, 976, 309, 257, 1168, 309, 576, 51370], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1719, "seek": 669564, "start": 6715.740000000001, "end": 6719.52, "text": " just potentially just give you more questions it would follow with more", "tokens": [51370, 445, 7263, 445, 976, 291, 544, 1651, 309, 576, 1524, 365, 544, 51559], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1720, "seek": 669564, "start": 6719.52, "end": 6724.240000000001, "text": " questions it will do whatever it looks like the some closed document would do", "tokens": [51559, 1651, 309, 486, 360, 2035, 309, 1542, 411, 264, 512, 5395, 4166, 576, 360, 51795], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1721, "seek": 669564, "start": 6724.240000000001, "end": 6725.52, "text": " in the training data", "tokens": [51795, 294, 264, 3097, 1412, 51859], "temperature": 0.0, "avg_logprob": -0.0845877484577458, "compression_ratio": 1.9819494584837545, "no_speech_prob": 0.0005027055740356445}, {"id": 1722, "seek": 672552, "start": 6725.52, "end": 6728.84, "text": " on the internet and so who knows you're getting kind of like undefined behavior", "tokens": [50365, 322, 264, 4705, 293, 370, 567, 3255, 291, 434, 1242, 733, 295, 411, 674, 5666, 2001, 5223, 50531], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1723, "seek": 672552, "start": 6728.84, "end": 6733.320000000001, "text": " it might basically answer with two questions with other questions it might", "tokens": [50531, 309, 1062, 1936, 1867, 365, 732, 1651, 365, 661, 1651, 309, 1062, 50755], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1724, "seek": 672552, "start": 6733.320000000001, "end": 6736.9800000000005, "text": " ignore your question it might just try to complete some news article it's", "tokens": [50755, 11200, 428, 1168, 309, 1062, 445, 853, 281, 3566, 512, 2583, 7222, 309, 311, 50938], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1725, "seek": 672552, "start": 6736.9800000000005, "end": 6742.02, "text": " totally unaligned as we say so the second fine-tuning stage is to actually", "tokens": [50938, 3879, 517, 304, 16690, 382, 321, 584, 370, 264, 1150, 2489, 12, 83, 37726, 3233, 307, 281, 767, 51190], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1726, "seek": 672552, "start": 6742.02, "end": 6748.280000000001, "text": " align it to be an assistant and this is the second stage and so this chat GPT", "tokens": [51190, 7975, 309, 281, 312, 364, 10994, 293, 341, 307, 264, 1150, 3233, 293, 370, 341, 5081, 26039, 51, 51503], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1727, "seek": 672552, "start": 6748.280000000001, "end": 6752.4800000000005, "text": " blog post from opening I talks a little bit about how this stage is achieved we", "tokens": [51503, 6968, 2183, 490, 5193, 286, 6686, 257, 707, 857, 466, 577, 341, 3233, 307, 11042, 321, 51713], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1728, "seek": 672552, "start": 6752.4800000000005, "end": 6754.92, "text": " basically", "tokens": [51713, 1936, 51835], "temperature": 0.0, "avg_logprob": -0.1255296243203653, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.0005004644044674933}, {"id": 1729, "seek": 675552, "start": 6755.52, "end": 6759.540000000001, "text": " roughly three steps to it to this stage so what they do here is they start to", "tokens": [50365, 9810, 1045, 4439, 281, 309, 281, 341, 3233, 370, 437, 436, 360, 510, 307, 436, 722, 281, 50566], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1730, "seek": 675552, "start": 6759.540000000001, "end": 6763.26, "text": " collect training data that looks specifically like what an assistant", "tokens": [50566, 2500, 3097, 1412, 300, 1542, 4682, 411, 437, 364, 10994, 50752], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1731, "seek": 675552, "start": 6763.26, "end": 6766.56, "text": " would do so there are documents that have the format where the question is on", "tokens": [50752, 576, 360, 370, 456, 366, 8512, 300, 362, 264, 7877, 689, 264, 1168, 307, 322, 50917], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1732, "seek": 675552, "start": 6766.56, "end": 6770.700000000001, "text": " top and then an answer is below and they have a large number of these but", "tokens": [50917, 1192, 293, 550, 364, 1867, 307, 2507, 293, 436, 362, 257, 2416, 1230, 295, 613, 457, 51124], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1733, "seek": 675552, "start": 6770.700000000001, "end": 6773.76, "text": " probably not on the order of the internet this is probably on the order", "tokens": [51124, 1391, 406, 322, 264, 1668, 295, 264, 4705, 341, 307, 1391, 322, 264, 1668, 51277], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1734, "seek": 675552, "start": 6773.76, "end": 6780.6, "text": " of maybe thousands of examples and so they they then fine-tune the model to", "tokens": [51277, 295, 1310, 5383, 295, 5110, 293, 370, 436, 436, 550, 2489, 12, 83, 2613, 264, 2316, 281, 51619], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1735, "seek": 675552, "start": 6780.6, "end": 6785.22, "text": " basically only focus on documents that look like that and so you're starting to", "tokens": [51619, 1936, 787, 1879, 322, 8512, 300, 574, 411, 300, 293, 370, 291, 434, 2891, 281, 51850], "temperature": 0.0, "avg_logprob": -0.12309940656026204, "compression_ratio": 1.9127272727272728, "no_speech_prob": 0.0025488303508609533}, {"id": 1736, "seek": 678522, "start": 6785.22, "end": 6788.72, "text": " slowly align it so it's going to expect a question at the top and it's going to", "tokens": [50365, 5692, 7975, 309, 370, 309, 311, 516, 281, 2066, 257, 1168, 412, 264, 1192, 293, 309, 311, 516, 281, 50540], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1737, "seek": 678522, "start": 6788.72, "end": 6793.9800000000005, "text": " expect to complete the answer and these very very large models are very sample", "tokens": [50540, 2066, 281, 3566, 264, 1867, 293, 613, 588, 588, 2416, 5245, 366, 588, 6889, 50803], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1738, "seek": 678522, "start": 6793.9800000000005, "end": 6798.12, "text": " efficient during their fine-tuning so this actually somehow works but that's", "tokens": [50803, 7148, 1830, 641, 2489, 12, 83, 37726, 370, 341, 767, 6063, 1985, 457, 300, 311, 51010], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1739, "seek": 678522, "start": 6798.12, "end": 6801.780000000001, "text": " just step one that's just fine-tuning so then they actually have more steps where", "tokens": [51010, 445, 1823, 472, 300, 311, 445, 2489, 12, 83, 37726, 370, 550, 436, 767, 362, 544, 4439, 689, 51193], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1740, "seek": 678522, "start": 6801.780000000001, "end": 6806.34, "text": " okay the second step is you let the model respond and then different raters", "tokens": [51193, 1392, 264, 1150, 1823, 307, 291, 718, 264, 2316, 4196, 293, 550, 819, 5937, 433, 51421], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1741, "seek": 678522, "start": 6806.34, "end": 6810.54, "text": " look at the different responses and rank them for their preferences to which one", "tokens": [51421, 574, 412, 264, 819, 13019, 293, 6181, 552, 337, 641, 21910, 281, 597, 472, 51631], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1742, "seek": 678522, "start": 6810.54, "end": 6815.0, "text": " is better than the other they use that to train a reward model so they can predict a", "tokens": [51631, 307, 1101, 813, 264, 661, 436, 764, 300, 281, 3847, 257, 7782, 2316, 370, 436, 393, 6069, 257, 51854], "temperature": 0.8, "avg_logprob": -0.10806807436684306, "compression_ratio": 1.934256055363322, "no_speech_prob": 0.0002824129187501967}, {"id": 1743, "seek": 681500, "start": 6815.0, "end": 6822.04, "text": " basically using a different network, how much of any candidate response would be desirable.", "tokens": [50365, 1936, 1228, 257, 819, 3209, 11, 577, 709, 295, 604, 11532, 4134, 576, 312, 30533, 13, 50717], "temperature": 0.0, "avg_logprob": -0.13180002712068103, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.021595347672700882}, {"id": 1744, "seek": 681500, "start": 6822.72, "end": 6827.9, "text": " And then once they have a reward model, they run PPO, which is a form of policy gradient", "tokens": [50751, 400, 550, 1564, 436, 362, 257, 7782, 2316, 11, 436, 1190, 430, 34885, 11, 597, 307, 257, 1254, 295, 3897, 16235, 51010], "temperature": 0.0, "avg_logprob": -0.13180002712068103, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.021595347672700882}, {"id": 1745, "seek": 681500, "start": 6827.9, "end": 6835.64, "text": " reinforcement learning optimizer, to fine-tune this sampling policy so that the answers that", "tokens": [51010, 29280, 2539, 5028, 6545, 11, 281, 2489, 12, 83, 2613, 341, 21179, 3897, 370, 300, 264, 6338, 300, 51397], "temperature": 0.0, "avg_logprob": -0.13180002712068103, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.021595347672700882}, {"id": 1746, "seek": 681500, "start": 6835.64, "end": 6842.7, "text": " the chat GPT now generates are expected to score a high reward according to the reward model.", "tokens": [51397, 264, 5081, 26039, 51, 586, 23815, 366, 5176, 281, 6175, 257, 1090, 7782, 4650, 281, 264, 7782, 2316, 13, 51750], "temperature": 0.0, "avg_logprob": -0.13180002712068103, "compression_ratio": 1.581896551724138, "no_speech_prob": 0.021595347672700882}, {"id": 1747, "seek": 684270, "start": 6842.7, "end": 6848.16, "text": " And so basically there's a whole aligning stage here, or fine-tuning stage. It's got multiple", "tokens": [50365, 400, 370, 1936, 456, 311, 257, 1379, 419, 9676, 3233, 510, 11, 420, 2489, 12, 83, 37726, 3233, 13, 467, 311, 658, 3866, 50638], "temperature": 0.0, "avg_logprob": -0.07265035689823211, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.00015848221664782614}, {"id": 1748, "seek": 684270, "start": 6848.16, "end": 6853.9, "text": " steps in between there as well, and it takes the model from being a document completer to a", "tokens": [50638, 4439, 294, 1296, 456, 382, 731, 11, 293, 309, 2516, 264, 2316, 490, 885, 257, 4166, 1557, 260, 281, 257, 50925], "temperature": 0.0, "avg_logprob": -0.07265035689823211, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.00015848221664782614}, {"id": 1749, "seek": 684270, "start": 6853.9, "end": 6859.5199999999995, "text": " question answerer, and that's like a whole separate stage. A lot of this data is not", "tokens": [50925, 1168, 1867, 260, 11, 293, 300, 311, 411, 257, 1379, 4994, 3233, 13, 316, 688, 295, 341, 1412, 307, 406, 51206], "temperature": 0.0, "avg_logprob": -0.07265035689823211, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.00015848221664782614}, {"id": 1750, "seek": 684270, "start": 6859.5199999999995, "end": 6865.12, "text": " available publicly. It is internal to OpenAI, and it's much harder to replicate this stage.", "tokens": [51206, 2435, 14843, 13, 467, 307, 6920, 281, 7238, 48698, 11, 293, 309, 311, 709, 6081, 281, 25356, 341, 3233, 13, 51486], "temperature": 0.0, "avg_logprob": -0.07265035689823211, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.00015848221664782614}, {"id": 1751, "seek": 684270, "start": 6866.32, "end": 6872.179999999999, "text": " And so that's roughly what would give you a chat GPT. And NanoGPT focuses on the pre-training stage.", "tokens": [51546, 400, 370, 300, 311, 9810, 437, 576, 976, 291, 257, 5081, 26039, 51, 13, 400, 43511, 38, 47, 51, 16109, 322, 264, 659, 12, 17227, 1760, 3233, 13, 51839], "temperature": 0.0, "avg_logprob": -0.07265035689823211, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.00015848221664782614}, {"id": 1752, "seek": 684270, "start": 6872.179999999999, "end": 6872.58, "text": " Okay.", "tokens": [51839, 1033, 13, 51859], "temperature": 0.0, "avg_logprob": -0.07265035689823211, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.00015848221664782614}, {"id": 1753, "seek": 687270, "start": 6872.7, "end": 6878.92, "text": " And that's everything that I wanted to cover today. So we trained, to summarize, a decoder-only", "tokens": [50365, 400, 300, 311, 1203, 300, 286, 1415, 281, 2060, 965, 13, 407, 321, 8895, 11, 281, 20858, 11, 257, 979, 19866, 12, 25202, 50676], "temperature": 0.0, "avg_logprob": -0.07188591023081357, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.0003289587330073118}, {"id": 1754, "seek": 687270, "start": 6878.92, "end": 6885.46, "text": " transformer following this famous paper, Attention is All You Need, from 2017. And so that's", "tokens": [50676, 31782, 3480, 341, 4618, 3035, 11, 31858, 307, 1057, 509, 16984, 11, 490, 6591, 13, 400, 370, 300, 311, 51003], "temperature": 0.0, "avg_logprob": -0.07188591023081357, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.0003289587330073118}, {"id": 1755, "seek": 687270, "start": 6885.46, "end": 6893.42, "text": " basically a GPT. We trained it on tiny Shakespeare and got sensible results. All of the training", "tokens": [51003, 1936, 257, 26039, 51, 13, 492, 8895, 309, 322, 5870, 22825, 293, 658, 25380, 3542, 13, 1057, 295, 264, 3097, 51401], "temperature": 0.0, "avg_logprob": -0.07188591023081357, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.0003289587330073118}, {"id": 1756, "seek": 687270, "start": 6893.42, "end": 6902.139999999999, "text": " code is roughly 200 lines of code. I will be releasing this code base. So also it comes with", "tokens": [51401, 3089, 307, 9810, 2331, 3876, 295, 3089, 13, 286, 486, 312, 16327, 341, 3089, 3096, 13, 407, 611, 309, 1487, 365, 51837], "temperature": 0.0, "avg_logprob": -0.07188591023081357, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.0003289587330073118}, {"id": 1757, "seek": 687270, "start": 6902.139999999999, "end": 6902.42, "text": " all the...", "tokens": [51837, 439, 264, 485, 51851], "temperature": 0.0, "avg_logprob": -0.07188591023081357, "compression_ratio": 1.5254901960784313, "no_speech_prob": 0.0003289587330073118}, {"id": 1758, "seek": 690270, "start": 6902.7, "end": 6908.78, "text": " Git log commits along the way, as we built it up. In addition to this code, I'm going to release", "tokens": [50365, 16939, 3565, 48311, 2051, 264, 636, 11, 382, 321, 3094, 309, 493, 13, 682, 4500, 281, 341, 3089, 11, 286, 478, 516, 281, 4374, 50669], "temperature": 0.0, "avg_logprob": -0.12761056332187798, "compression_ratio": 1.5746753246753247, "no_speech_prob": 0.00040861652814783156}, {"id": 1759, "seek": 690270, "start": 6908.78, "end": 6914.54, "text": " the notebook, of course, the Google Colab. And I hope that gave you a sense for how you can train", "tokens": [50669, 264, 21060, 11, 295, 1164, 11, 264, 3329, 4004, 455, 13, 400, 286, 1454, 300, 2729, 291, 257, 2020, 337, 577, 291, 393, 3847, 50957], "temperature": 0.0, "avg_logprob": -0.12761056332187798, "compression_ratio": 1.5746753246753247, "no_speech_prob": 0.00040861652814783156}, {"id": 1760, "seek": 690270, "start": 6916.139999999999, "end": 6921.099999999999, "text": " these models, like, say, GPT-3, that will be architecturally basically identical to what we", "tokens": [51037, 613, 5245, 11, 411, 11, 584, 11, 26039, 51, 12, 18, 11, 300, 486, 312, 6331, 6512, 1936, 14800, 281, 437, 321, 51285], "temperature": 0.0, "avg_logprob": -0.12761056332187798, "compression_ratio": 1.5746753246753247, "no_speech_prob": 0.00040861652814783156}, {"id": 1761, "seek": 690270, "start": 6921.099999999999, "end": 6925.58, "text": " have, but they are somewhere between 10,000 and 1 million times bigger, depending on how you count.", "tokens": [51285, 362, 11, 457, 436, 366, 4079, 1296, 1266, 11, 1360, 293, 502, 2459, 1413, 3801, 11, 5413, 322, 577, 291, 1207, 13, 51509], "temperature": 0.0, "avg_logprob": -0.12761056332187798, "compression_ratio": 1.5746753246753247, "no_speech_prob": 0.00040861652814783156}, {"id": 1762, "seek": 690270, "start": 6926.62, "end": 6932.54, "text": " And so that's all I have for now. We did not talk about any of the fine-tuning stages. That would,", "tokens": [51561, 400, 370, 300, 311, 439, 286, 362, 337, 586, 13, 492, 630, 406, 751, 466, 604, 295, 264, 2489, 12, 83, 37726, 10232, 13, 663, 576, 11, 51857], "temperature": 0.0, "avg_logprob": -0.12761056332187798, "compression_ratio": 1.5746753246753247, "no_speech_prob": 0.00040861652814783156}, {"id": 1763, "seek": 693254, "start": 6932.54, "end": 6936.46, "text": " typically, go on top of this. So if you're interested in something that's not just language", "tokens": [50365, 5850, 11, 352, 322, 1192, 295, 341, 13, 407, 498, 291, 434, 3102, 294, 746, 300, 311, 406, 445, 2856, 50561], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1764, "seek": 693254, "start": 6936.46, "end": 6941.58, "text": " modeling, but you actually want to, you know, say, perform tasks, or you want them to be aligned in", "tokens": [50561, 15983, 11, 457, 291, 767, 528, 281, 11, 291, 458, 11, 584, 11, 2042, 9608, 11, 420, 291, 528, 552, 281, 312, 17962, 294, 50817], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1765, "seek": 693254, "start": 6941.58, "end": 6947.18, "text": " a specific way, or you want to detect sentiment or anything like that, basically, any time you", "tokens": [50817, 257, 2685, 636, 11, 420, 291, 528, 281, 5531, 16149, 420, 1340, 411, 300, 11, 1936, 11, 604, 565, 291, 51097], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1766, "seek": 693254, "start": 6947.18, "end": 6951.34, "text": " don't want something that's just a document completer, you have to complete further stages", "tokens": [51097, 500, 380, 528, 746, 300, 311, 445, 257, 4166, 1557, 260, 11, 291, 362, 281, 3566, 3052, 10232, 51305], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1767, "seek": 693254, "start": 6951.34, "end": 6956.38, "text": " of fine-tuning, which we did not cover. And that could be simple, supervised fine-tuning,", "tokens": [51305, 295, 2489, 12, 83, 37726, 11, 597, 321, 630, 406, 2060, 13, 400, 300, 727, 312, 2199, 11, 46533, 2489, 12, 83, 37726, 11, 51557], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1768, "seek": 693254, "start": 6956.38, "end": 6960.78, "text": " or it can be something more fancy, like we see in ChatGPT, where we actually train a reward model,", "tokens": [51557, 420, 309, 393, 312, 746, 544, 10247, 11, 411, 321, 536, 294, 27503, 38, 47, 51, 11, 689, 321, 767, 3847, 257, 7782, 2316, 11, 51777], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1769, "seek": 693254, "start": 6960.78, "end": 6962.54, "text": " and then do rounds of PPO to...", "tokens": [51777, 293, 550, 360, 13757, 295, 430, 34885, 281, 485, 51865], "temperature": 0.0, "avg_logprob": -0.10746927568989416, "compression_ratio": 1.7850746268656716, "no_speech_prob": 0.0011561787687242031}, {"id": 1770, "seek": 696254, "start": 6962.54, "end": 6967.34, "text": " align it with respect to the reward model. So there's a lot more that can be done on top of it.", "tokens": [50365, 7975, 309, 365, 3104, 281, 264, 7782, 2316, 13, 407, 456, 311, 257, 688, 544, 300, 393, 312, 1096, 322, 1192, 295, 309, 13, 50605], "temperature": 0.0, "avg_logprob": -0.12751900066028943, "compression_ratio": 1.4787234042553192, "no_speech_prob": 0.00031620077788829803}, {"id": 1771, "seek": 696254, "start": 6967.34, "end": 6972.62, "text": " I think for now, we're starting to get to about two hours, Mark. So I'm going to kind of finish", "tokens": [50605, 286, 519, 337, 586, 11, 321, 434, 2891, 281, 483, 281, 466, 732, 2496, 11, 3934, 13, 407, 286, 478, 516, 281, 733, 295, 2413, 50869], "temperature": 0.0, "avg_logprob": -0.12751900066028943, "compression_ratio": 1.4787234042553192, "no_speech_prob": 0.00031620077788829803}, {"id": 1772, "seek": 696254, "start": 6972.62, "end": 6979.66, "text": " here. I hope you enjoyed the lecture. And yeah, go forth and transform. See you later.", "tokens": [50869, 510, 13, 286, 1454, 291, 4626, 264, 7991, 13, 400, 1338, 11, 352, 5220, 293, 4088, 13, 3008, 291, 1780, 13, 51221], "temperature": 0.0, "avg_logprob": -0.12751900066028943, "compression_ratio": 1.4787234042553192, "no_speech_prob": 0.00031620077788829803}], "language": "en"}