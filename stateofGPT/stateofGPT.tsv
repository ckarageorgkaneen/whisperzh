start	end	text
0	15000	Please welcome AI researcher and founding member of OpenAI, Andrej Karpathy.
15000	23560	Andrej Karpathy Hi, everyone.
23560	28820	I'm happy to be here to tell you about the state of GPT and more generally about the
28820	32140	rapidly growing ecosystem of large language models.
32140	35400	So I would like to partition the talk into two parts.
35400	39580	In the first part, I would like to tell you about how we train GPT assistants.
39580	43740	And then in the second part, we are going to take a look at how we can use these assistants
43740	46760	effectively for your applications.
46760	50500	So first, let's take a look at the emerging recipe for how to train these assistants.
50500	53200	And keep in mind that this is all very new and still rapidly evolving.
53200	55960	But so far, the recipe looks something like this.
55960	58800	Now this is kind of a complicated slide, so I'm going to go through it piece by piece.
58800	65400	But roughly speaking, we have four major stages, pre-training, supervised fine-tuning,
65400	70040	reward modeling, reinforcement learning, and they follow each other serially.
70040	74900	Now in each stage, we have a data set that powers that stage.
74900	81420	We have an algorithm that for our purposes will be an objective for training the neural
81420	82420	network.
82420	84060	And then we have a resulting model.
84060	85980	And then there's some notes on the bottom.
85980	88160	So the first stage we're going to start with is the pre-training stage.
88160	88640	So the first stage we're going to start with is the pre-training stage.
88800	93640	Now this stage is kind of special in this diagram, and this diagram is not to scale.
93640	96460	Because this stage is where all of the computational work basically happens.
96460	102020	This is 99% of the training compute time, and also flops.
102020	108560	And so this is where we are dealing with internet-scale data sets with thousands of GPUs in the supercomputer,
108560	111420	and also months of training, potentially.
111420	116120	The other three stages are fine-tuning stages that are much more along the lines of a small
116120	118160	few number of GPUs and hours or days.
118800	124180	So let's take a look at the pre-training stage to achieve a base model.
124180	127860	First we're going to gather a large amount of data.
127860	133100	Here's an example of what we call a data mixture that comes from this paper that was released
133100	136400	by Meta, where they released this LAMA base model.
136400	140560	Now you can see roughly the kinds of data sets that enter into these collections.
140560	145540	So we have Common Crawl, which is just a web scrape, C4, which is also Common Crawl, and
145540	147360	then some high-quality data sets as well.
147360	147960	So for example, GitHub, Wikipedia, and so on.
147960	148460	So we have common crawl, which is just a web scrape, C4, which is also common crawl, and then some high-quality data sets as well. So for example, GitHub, Wikipedia, and so on.
148800	149300	So we have Common Crawl, which is just a web scrape, C4, which is also common crawl, and then some high-quality data sets as well.
149300	149800	So we have Common Crawl, which is just a web scrape, C4, which is also common crawl, and then some high-quality data sets as well.
149800	156280	These are all mixed up together, and then they are sampled according to some given proportions,
156280	160040	and that forms the training set for the neural net, for the GPT.
160040	165120	Now before we can actually train on this data, we need to go through one more pre-processing
165120	166920	step, and that is tokenization.
166920	170820	And this is basically a translation of the raw text that we scraped from the internet
170820	173300	into sequences of integers.
173300	177420	Because that's the native representation over which GPTs function.
177420	177960	Now, this is a lossless method of group mapping.
177960	178460	Now, this is a lossless method of group mapping.
178460	184180	lossless kind of translation between pieces of text and tokens and integers. And there are a
184180	187960	number of algorithms for this stage. Typically, for example, you could use something like byte
187960	194180	pairing coding, which iteratively merges little text chunks and groups them into tokens. And so
194180	198780	here I'm showing some example chunks of these tokens. And then this is the raw integer sequence
198780	206060	that will actually feed into a transformer. Now, here I'm showing two sort of like examples for
206060	211300	hyperparameters that govern this stage. So GPT-4, we did not release too much information about how
211300	215340	it was trained and so on. So I'm using GPT-3's numbers. But GPT-3 is, of course, a little bit
215340	221180	old by now, about three years ago. But LAMA is a fairly recent model from Meta. So these are
221180	224280	roughly the orders of magnitude that we're dealing with when we're doing pre-training.
225080	229780	The vocabulary size is usually a couple 10,000 tokens. The context length is usually something
229780	235740	like 2,000, 4,000, or nowadays even 100,000. And this governs the maximum number of integers
235740	236040	that we're dealing with. And so this is the order of magnitude that we're dealing with.
236060	240040	That the GPT will look at when it's trying to predict the next integer in a sequence.
241800	245840	You can see that roughly the number of parameters is, say, 65 billion for LAMA.
246280	251040	Now, even though LAMA has only 65B parameters compared to GPT-3's 175 billion parameters,
251440	256360	LAMA is a significantly more powerful model. And intuitively, that's because the model is
256360	260980	trained for significantly longer. In this case, 1.4 trillion tokens instead of just 300 billion
260980	265160	tokens. So you shouldn't judge the power of a model just by the number of parameters that it
265160	265500	contains.
266060	272420	Below, I'm showing some tables of rough hyperparameters that typically go into specifying
272420	276520	the transformer neural network. So the number of heads, the dimension size, number of layers,
276600	282140	and so on. And on the bottom, I'm showing some training hyperparameters. So for example,
282280	290920	to train the 65B model, Meta used 2,000 GPUs, roughly 21 days of training, and roughly several
290920	296040	million dollars. And so that's the rough orders of magnitude that you should have in mind for the
296060	296940	pre-training stage.
299040	303680	Now, when we're actually pre-training, what happens? Roughly speaking, we are going to take our tokens,
303700	308520	and we're going to lay them out into data batches. So we have these arrays that will feed into the
308520	313720	transformer, and these arrays are B, the batch size, and these are all independent examples stacked
313720	319400	up in rows, and B by T, T being the maximum context length. So in my picture, I only have 10,
320180	326040	the context length. So this could be 2,000, 4,000, et cetera. So these are extremely long rows. And what we do is we take these
326060	332440	documents, and we pack them into rows, and we delimit them with these special end-of-text tokens, basically telling the
332440	339020	transformer where a new document begins. And so here I have a few examples of documents, and then I've stretched them out
339020	348640	into this input. Now, we're going to feed all of these numbers into transformer. And let me just focus on a single
348640	355440	particular cell, but the same thing will happen at every cell in this diagram. So let's look at the green cell. The green cell is
356060	363740	going to take a look at all of the tokens before it, so all of the tokens in yellow, and we're going to feed that entire context into the
363740	370820	transformer neural network, and the transformer is going to try to predict the next token in a sequence, in this case, in red. Now, the
370820	376860	transformer, I don't have too much time to, unfortunately, go into the full details of this neural network architecture. It's just a large blob of
376860	383120	neural net stuff for our purposes, and it's got several 10 billion parameters, typically, or something like that. And, of course, as you
383120	386040	tune these parameters, you're getting slightly different predicted distributions. And so what we're going to do is we're going to take a look at the
386060	395300	distributions for every single one of these cells. And so, for example, if our vocabulary size is 50,257 tokens, then we're going to have that
395300	402120	many numbers, because we need to specify a probability distribution for what comes next. So basically, we have a probability for whatever may
402120	410300	follow. Now, in this specific example, for this specific cell, 513 will come next, and so we can use this as a source of supervision to update our
410300	416020	transformer's weights. And so we're applying this, basically, on every single cell in the parallel, and we keep swapping back and forth between the
416060	423000	batches, and we're trying to get the transformer to make the correct predictions over what token comes next in a sequence. So let me show you more
423000	430040	concretely what this looks like when you train one of these models. This is actually coming from the New York Times, and they trained a small GPT on
430040	438140	Shakespeare. And so here's a small snippet of Shakespeare, and they trained a GPT on it. Now, in the beginning, at initialization, the GPT starts with
438140	445740	completely random weights, so you're just getting completely random outputs as well. But over time, as you train the GPT longer and longer,
446060	454920	you are getting more and more coherent and consistent sort of samples from the model. And the way you sample from it, of course, is you predict what comes
454920	463080	next, you sample from that distribution, and you keep feeding that back into the process, and you can basically sample large sequences. And so by the end, you see
463080	469680	that the transformer has learned about words and where to put spaces and where to put commas and so on. And so we're making more and more consistent
469680	476000	predictions over time. These are the kinds of plots that you're looking at when you're doing model pre-training. Effectively, we're looking at
476060	485060	a loss function over time as you train, and low loss means that our transformer is predicting the correct, is giving a higher probability to get the correct next
485060	494900	integer in a sequence. Now, what are we going to do with this model once we've trained it after a month? Well, the first thing that we noticed, we, the field, is that
494900	503540	these models basically, in the process of language modeling, learn very powerful general representations, and it's possible to very efficiently fine-tune them
503540	506040	for any arbitrary downstream task you might be interested in.
506060	517120	So as an example, if you're interested in sentiment classification, the approach used to be that you collect a bunch of positives and negatives, and then you train some kind of an NLP model for that.
517120	530740	But the new approach is, ignore sentiment classification, go off and do large language model pre-training, train the large transformer, and then you can only, you may only have a few examples, and you can very efficiently fine-tune your model for that task.
530740	535940	And so this works very well in practice, and the reason for this is that basically, the transformer is
535940	542260	forced to multitask a huge amount of tasks in the language modeling task, because just in terms of
542260	546680	predicting the next token, it's forced to understand a lot about the structure of the text
546680	552760	and all the different concepts therein. So that was GPT-1. Now, around the time of GPT-2, people
552760	557120	noticed that actually even better than fine-tuning, you can actually prompt these models very
557120	561080	effectively. So these are language models, and they want to complete documents. So you can actually
561080	566340	trick them into performing tasks just by arranging these fake documents. So in this example,
566500	571800	for example, we have some passage, and then we sort of like do QA, QA, QA. This is called a few-shot
571800	576040	prompt. And then we do Q. And then as the transformer is trying to complete the document,
576200	580560	it's actually answering our question. And so this is an example of prompt engineering a base model,
580900	584960	making it believe that it's sort of imitating a document and getting it to perform a task.
585680	590600	And so this picked off, I think, the era of, I would say, prompting over fine-tuning and seeing
590600	591060	that this actually works. And so this is an example of prompt engineering a base model,
591060	594540	actually can work extremely well on a lot of problems, even without training any neural
594540	600380	networks, fine-tuning, or so on. Now, since then, we've seen an entire evolutionary tree of base
600380	606880	models that everyone has trained. Not all of these models are available. For example, the GPT-4 base
606880	611480	model was never released. The GPT-4 model that you might be interacting with over API is not a base
611480	616900	model. It's an assistant model. And we're going to cover how to get those in a bit. GPT-3 base
616900	620020	model is available via the API under the name DaVinci.
620020	625300	And GPT-2 base model is available even as weights on our GitHub repo. But currently,
625420	630660	the best available base model probably is the Lama series from Meta, although it is not
630660	637100	commercially licensed. Now, one thing to point out is base models are not assistants. They don't
637100	644200	want to make answers to your questions. They just want to complete documents. So if you tell them,
644480	649540	write a poem about the bread and cheese, it will answer questions with more questions. It's just
649540	654280	completing what it thinks is a document. However, you can prompt them in a specific way for base
654280	660060	models that is more likely to work. So as an example, here's a poem about bread and cheese. And in
660060	666400	that case, it will autocomplete correctly. You can even trick base models into being assistants. And
666400	670860	the way you would do this is you would create like a specific few-shot prompt that makes it look like
670860	674680	there's some kind of a document between a human and assistant, and they're exchanging sort of
674680	679160	information. And then at the bottom, you sort of put your query at the end.
679540	686140	And the base model will sort of like condition itself into being like a helpful assistant and kind of
686140	690100	answer. But this is not very reliable and doesn't work super well in practice, although it can be
690100	696020	done. So instead, we have a different path to make actual GPT assistants, not just base model document
696020	700840	completers. And so that takes us into supervised fine-tuning. So in the supervised fine-tuning
700840	706660	stage, we are going to collect small but high-quality data sets. And in this case, we're going to ask
709540	714160	you to collect a set of the form prompt and ideal response. And we're going to collect lots of these,
714160	718480	typically tens of thousands or something like that. And then we're going to still do language
718480	722320	modeling on this data. So nothing changed algorithmically. We're just swapping out a
722320	728080	training set. So it used to be internet documents, which is a high-quantity, low-quality, for
728080	734740	basically a QA prompt response kind of data. And that is low-quantity, high-quality. So we
734740	739060	still do language modeling, and then after training, we get an SFD model. And you can
739540	744100	see these models, and they are actual assistants, and they work to some extent. Let me show you what
744100	747640	an example demonstration might look like. So here's something that a human contractor might
747640	752380	come up with. Here's some random prompt. Can you write a short introduction about the relevance of
752380	756700	the term monopsony or something like that? And then the contractor also writes out an ideal
756700	761260	response. And when they write out these responses, they are following extensive labeling documentations,
761260	767440	and they are being asked to be helpful, truthful, and harmless. And this is labeling instructions
767440	769520	here. You probably can't read it.
769540	774100	Neither can I. But they're long, and this is just people following instructions and trying to
774100	778360	complete these prompts. So that's what the data set looks like, and you can train these models,
778360	783760	and this works to some extent. Now, you can actually continue the pipeline from here on and
783760	789100	go into RLHF, reinforcement learning from human feedback, that consists of both reward modeling
789100	793300	and reinforcement learning. So let me cover that, and then I'll come back to why you may want to go
793300	798040	through the extra steps and how that compares to just SFD models. So in the reward modeling step,
799540	803860	we're now going to shift our data collection to be of the form of comparisons. So here's an example
803860	808180	of what our data set will look like. I have the same prompt, identical prompt on the top,
808180	814240	which is asking the assistant to write a program or a function that checks if a given string is
814240	819340	a palindrome. And then what we do is we take the SFD model, which we've already trained,
819340	823060	and we create multiple completions. So in this case, we have three completions that the model
823060	829120	has created. And then we ask people to rank these completions. So if you stare at this for a while,
829540	833680	these are very difficult things to do to compare some of these predictions. And this can take
833680	840400	people even hours for a single prompt completion pairs. But let's say we decided that one of these
840400	844900	is much better than the others and so on. So we rank them. Then we can follow that with something
844900	848680	that looks very much kind of like a binary classification on all the possible pairs
848680	854080	between these completions. So what we do now is we lay out our prompt in rows, and the prompts
854080	858940	is identical across all three rows here. So it's all the same prompt, but the completion does vary.
859540	863920	So the yellow tokens are coming from the SFD model. Then what we do is we append another
863920	870460	special reward readout token at the end. And we basically only supervise the transformer at this
870460	876640	single green token. And the transformer will predict some reward for how good that completion
876640	882580	is for that prompt. And so basically, it makes a guess about the quality of each completion. And
882580	886720	then once it makes a guess for every one of them, we also have the ground truth, which is telling
886720	889480	us the ranking of them. And so we can actually enforce that. And so we can actually enforce that,
889480	893620	some of these numbers should be much higher than others and so on. We formulate this into a loss
893620	898060	function, and we train our model to make reward predictions that are consistent with the ground
898060	902440	truth coming from the comparisons from all these contractors. So that's how we train our reward
902440	908320	model. And that allows us to score how good a completion is for a prompt. Once we have a reward
908320	913780	model, we can't deploy this because this is not very useful as an assistant by itself, but it's
913780	918400	very useful for the reinforcement learning stage that follows now. Because we have a reward model,
918400	919180	we can score
919480	924280	the quality of any arbitrary completion for any given prompt. So what we do during reinforcement
924280	929080	learning is we basically get, again, a large collection of prompts. And now we do reinforcement
929080	933940	learning with respect to the reward model. So here's what that looks like. We take a single
933940	939520	prompt, we lay it out in rows, and now we use basically the model we'd like to train,
939520	944680	which is initialized at SFT model, to create some completions in yellow. And then we append
944680	949420	the reward token again, and we read off the reward according to the reward model, which
949480	954820	is now kept fixed. It doesn't change anymore. And now the reward model tells us the quality of every
954820	959740	single completion for these prompts. And so what we can do is we can now just basically apply the
959740	965920	same language modeling loss function, but we're currently training on the yellow tokens, and we
965920	971740	are weighing the language modeling objective by the rewards indicated by the reward model. So as
971740	976960	an example, in the first row, the reward model said that this is a fairly high scoring completion,
976960	979260	and so all of the tokens that we happened to
979480	983640	on the first row are going to get reinforced and they're going to get higher probabilities
983640	988700	for the future. Conversely, on the second row, the reward model really did not like this completion,
988920	993940	negative 1.2. And so therefore, every single token that we sampled in that second row is going to
993940	998200	get a slightly higher probability for the future. And we do this over and over on many prompts,
998200	1004520	on many batches. And basically, we get a policy which creates yellow tokens here. And it basically,
1004720	1008540	all of them, all of the completions here will score high according to the reward model that
1008540	1014640	we trained in the previous stage. So that's how we train. That's what the RLHF pipeline is.
1015840	1019780	Now, and then at the end, you get a model that you could deploy. And so as an example,
1020160	1025040	ChatGPT is an RLHF model. But some other models that you might come across, like for example,
1025040	1031100	the Kuna 13b and so on, these are SFT models. So we have base models, SFT models, and RLHF models.
1031900	1036500	And that's kind of like the state of things there. Now, why would you want to do RLHF?
1036920	1038400	So one answer that is kind of...
1038540	1042440	not that exciting, is that it just works better. So this comes from the InstructGPT paper.
1042840	1047760	According to these experiments a while ago now, these PPO models are RLHF.
1047760	1051720	And we see that they are basically just preferred in a lot of comparisons
1052300	1054880	when we give them to humans. So humans just prefer out
1055460	1061800	basically tokens that come from RLHF models compared to SFT models, compared to base model that is prompted to be an assistant.
1061800	1065160	And so it just works better. But you might ask why?
1065800	1068380	Why does it work better? And I don't think that there's a single
1068540	1075440	like amazing answer that the community has really like agreed on, but I will just offer one reason, potentially.
1075440	1081800	And it has to do with the asymmetry between how easy computationally it is to compare versus generate.
1082300	1087920	So let's take an example of generating a haiku. Suppose I ask a model to write a haiku about paperclips.
1087920	1094160	If you're a contractor trying to give training data, then imagine being a contractor collecting basically data for the SFT stage.
1094160	1098380	How are you supposed to create a nice haiku for a paperclip? You might just not be very good at that.
1098540	1103660	But if I give you a few examples of haikus, you might be able to appreciate some of these haikus a lot more than others.
1103660	1106780	And so judging which one of these is good is a much easier task.
1106780	1113160	And so basically this asymmetry makes it so that comparisons are a better way to potentially
1113340	1117040	leverage yourself as a human and your judgment to create a slightly better model.
1117740	1123040	Now, RLHF models are not strictly an improvement on the base models in some cases.
1123360	1126580	So in particular, we've noticed, for example, that they lose some entropy.
1126580	1128420	So that means that they give more
1128540	1136120	peaky results. They can output lower variations, like they can output samples with lower variation than base model.
1136120	1140240	So base model has lots of entropy and will give lots of diverse outputs.
1140240	1153740	So, for example, one kind of place where I still prefer to use a base model is in a setup where you basically have n things and you want to generate more things like it.
1153740	1156700	And so here is an example that I just cooked up.
1156700	1158540	I want to generate cool Pokemon names.
1158540	1164160	I gave it seven Pokemon names, and I asked the base model to complete the document, and it gave me a lot more Pokemon names.
1164460	1168540	These are fictitious. I tried to look them up. I don't believe they're actual Pokemons.
1169420	1173260	And this is the kind of task that I think base model would be good at, because it still has lots of entropy.
1173260	1178100	It will give you lots of diverse, cool, kind of more things that look like whatever you give it before.
1180220	1184860	So this is what, this is number, having said all that, these are kind of like the assistant models
1184860	1186860	that are probably available to you at this point.
1187260	1188380	There's a team at Berkeley,
1188380	1193260	that ranked a lot of the available assistant models and gave them basically ELO ratings.
1193260	1199500	So currently some of the best models, of course, are GPT-4, by far, I would say, followed by Clawed, GPT-3.5,
1199500	1204140	and then a number of models, some of these might be available as weights, like the Kuna, Koala, etc.
1204700	1213100	And the first three rows here, they're all RLHF models, and all of the other models, to my knowledge, are SFT models, I believe.
1216060	1218300	Okay, so that's how we train these models.
1218300	1219340	On the high level.
1219340	1225580	Now I'm going to switch gears, and let's look at how we can best apply a GPT assistant model to your problems.
1226220	1229900	Now, I would like to work in a setting of a concrete example.
1229900	1233020	So let's work with a concrete example here.
1233020	1237980	Let's say that you are working on an article or a blog post, and you're going to write this sentence at the end.
1238620	1241020	California's population is 53 times that of Alaska.
1241020	1244060	So for some reason, you want to compare the populations of these two states.
1245180	1248060	Think about the rich internal monologue and tool use,
1248300	1253260	and how much work actually goes computationally in your brain to generate this one final sentence.
1253260	1255100	So here's maybe what that could look like in your brain.
1255740	1260380	Okay, for this next step, let me blog, or my blog, let me compare these two populations.
1261020	1264540	Okay, first I'm going to obviously need to get both of these populations.
1265180	1268940	Now, I know that I probably don't know these populations off the top of my head.
1268940	1272380	So I'm kind of like aware of what I know or don't know of my self-knowledge, right?
1273180	1278140	So I go, I do some tool use, and I go to Wikipedia, and I look up California's population.
1278300	1279260	And Alaska's population.
1280140	1282140	Now I know that I should divide the two.
1282140	1286780	But again, I know that dividing 39.2 by 0.74 is very unlikely to succeed.
1286780	1289580	That's not the kind of thing that I can do in my head.
1289580	1292300	And so therefore, I'm going to rely on the calculator.
1292300	1296140	So I'm going to use a calculator, punch it in, and see that the output is roughly 53.
1297180	1300700	And then maybe I do some reflection and sanity checks in my brain.
1300700	1302540	So does 53 make sense?
1302540	1306220	Well, that's quite a large fraction, but then California is the most populous state.
1306220	1307260	So maybe that looks okay.
1307260	1307420	Okay.
1307420	1307660	Okay.
1307660	1307740	Okay.
1307740	1308060	Okay.
1308060	1308140	Okay.
1308140	1308220	Okay.
1308220	1308300	Okay.
1308300	1309980	So then I have all the information I might need.
1309980	1312700	And now I get to the sort of creative portion of writing.
1312700	1317100	So I might start to write something like, California has 53x times greater.
1317100	1320220	And then I think to myself, that's actually like really awkward phrasing.
1320220	1322780	So let me actually delete that, and let me try again.
1323420	1328300	And so as I'm writing, I have this separate process almost inspecting what I'm writing
1328300	1330140	and judging whether it looks good or not.
1330940	1335100	And then maybe I delete, and maybe I reframe it, and then maybe I'm happy with what comes out.
1335740	1338060	So basically, long story short, a ton happens.
1338060	1340380	So I'm writing this sentence under the hood in terms of your internal monologue when you
1340380	1341420	create sentences like this.
1341980	1345980	But what does a sentence like this look like when we are training a GPT on it?
1347340	1349900	From GPT's perspective, this is just a sequence of tokens.
1350620	1355180	So a GPT, when it's reading or generating these tokens, it just goes chunk, chunk,
1355180	1356220	chunk, chunk, chunk.
1356220	1359820	And each chunk is roughly the same amount of computational work for each token.
1360380	1363260	And these transformers are not very shallow networks.
1363260	1365340	They have about 80 layers of reasoning.
1365340	1366940	But 80 is still not like too much.
1367500	1371340	And so this transformer is going to do its best to imitate.
1371340	1375260	But of course, the process here looks very, very different from the process that you took.
1376460	1381020	So in particular, in our final artifacts, in the data sets that we create and then eventually feed
1381020	1384220	to LLMs, all of that internal dialogue is completely stripped.
1384780	1390380	And unlike you, the GPT will look at every single token and spend the same amount of
1390380	1392060	compute on every one of them.
1392060	1396140	And so you can't expect it to actually like, well, you can't expect it to do,
1396940	1398540	sort of do too much work per token.
1399660	1403740	And also in particular, basically these transformers are just like token simulators.
1403740	1405660	So they don't know what they don't know.
1405660	1407900	Like they just imitate the next token.
1407900	1409660	They don't know what they're good at or not good at.
1409660	1411660	They just tried their best to imitate the next token.
1412300	1413980	They don't reflect in the loop.
1413980	1415420	They don't sanity check anything.
1415420	1417900	They don't correct their mistakes along the way by default.
1417900	1419980	They just sample token sequences.
1420860	1423660	They don't have separate inner monologue streams in their head, right?
1423660	1424940	They're evaluating what's happening.
1425580	1426540	Now, they do have some.
1426540	1431260	A sort of cognitive advantages, I would say, and that is that they do actually have very
1431260	1435980	large fact based knowledge across a vast number of areas because they have, say, several 10
1435980	1436860	billion parameters.
1436860	1439020	So that's a lot of storage for a lot of facts.
1439900	1444620	But and they also, I think, have a relatively large and perfect working memory.
1444620	1449260	So whatever fixed into the whatever fits into the context window is immediately available
1449260	1452380	to the transformer through its internal self attention mechanism.
1452380	1456060	And so it's kind of like perfect memory, but it's got a finite size.
1456540	1460860	The transformer has a very direct access to it, and so it can like a losslessly remember
1460860	1463100	anything that is inside its context window.
1463980	1465820	So that's kind of how I would compare those two.
1465820	1470460	And the reason I bring all of this up is because I think to a large extent, prompting is just
1470460	1477340	making up for this sort of cognitive difference between these two kind of architectures like
1477340	1479500	our brains here and LLM brains.
1479500	1480780	You can look at it that way almost.
1481980	1485900	So here's one thing that people found, for example, works pretty well in practice, especially
1485900	1488140	if your tasks require reasoning.
1488140	1492220	You can't expect the transformer to make to do too much reasoning per token.
1492220	1495900	And so you have to really spread out the reasoning across more and more tokens.
1495900	1499420	So, for example, you can't give a transformer a very complicated question and expect it
1499420	1500780	to get the answer in a single token.
1500780	1502060	There's just not enough time for it.
1502700	1506300	These transformers need tokens to think, quote unquote, I like to say sometimes.
1506860	1508860	And so this is some of the things that work well.
1508860	1512380	You may, for example, have a few shot prompt that shows the transformer that it should
1512380	1515660	like show its work when it's answering the question when it's answering a question.
1515900	1520700	And if you give a few examples, the transformer will imitate that template and it will just
1520700	1523740	end up working out better in terms of its evaluation.
1524540	1528220	Additionally, you can elicit this kind of behavior from the transformer by saying, let's
1528220	1532860	think step by step, because this conditioned the transformer into sort of like showing
1532860	1533580	its work.
1533580	1537900	And because it kind of snaps into a mode of showing its work, it's going to do less
1537900	1539500	computational work per token.
1540060	1545180	And so it's more likely to succeed as a result because it's making slower reasoning over
1545180	1545580	time.
1546460	1547420	Here's another example.
1547420	1548860	This one is called self-consistency.
1549820	1554540	We saw that we had the ability to start writing and then if it didn't work out, I can try
1554540	1560540	again and I can try multiple times and and maybe select the one that worked best.
1560540	1564700	So in these kinds of approaches, you may sample not just once, but you may sample multiple
1564700	1569260	times and then have some process for finding the ones that are good and then keeping just
1569260	1571900	those samples or doing a majority vote or something like that.
1571900	1575580	So basically, these transformers in the process as they predict the next token.
1575900	1579900	Just like you, they can get unlucky and they could they could sample and not a very good
1579900	1583900	token and they can go down sort of like a blind alley in terms of reasoning.
1583900	1587180	And so unlike you, they cannot recover from that.
1587180	1590940	They are stuck with every single token they sample, and so they will continue the sequence
1590940	1594060	even if they even know that this sequence is not going to work out.
1594060	1599820	So give them the ability to look back, inspect or try to find, try to basically sample around
1599820	1599980	it.
1601180	1601980	Here's one technique.
1601980	1602620	Also, you could.
1603580	1604540	It turns out that actually, LLMs.
1604540	1605500	Like, they know they're going to be able to do it.
1605500	1605740	They know they're going to be able to do it.
1605740	1605820	They know they're going to be able to do it.
1605820	1607340	They know when they've screwed up.
1607340	1613580	So as an example, say you asked the model to generate a poem that does not rhyme, and
1613580	1615900	it might give you a poem, but it actually rhymes.
1615900	1619900	But it turns out that especially for the bigger models like GPT-4, you can just ask it, did
1619900	1621020	you meet the assignment?
1621020	1624780	And actually, GPT-4 knows very well that it did not meet the assignment.
1624780	1627100	It just kind of got unlucky in its sampling.
1627100	1629580	And so it will tell you, no, I didn't actually meet the assignment here.
1629580	1630300	Let me try again.
1630940	1635660	But without you prompting it, it doesn't even like it doesn't know it doesn't know
1635660	1637820	to revisit and and so on.
1637820	1639980	So you have to make up for that in your prompts.
1639980	1641900	You have to get it to check.
1641900	1644140	If you don't ask it to check, it's not going to check by itself.
1644140	1645260	It's just a token simulator.
1649020	1653420	I think more generally, a lot of these techniques fall into the bucket of what I would say recreating
1653420	1654540	our system, too.
1654540	1657820	So you might be familiar with the system one system to thinking for humans.
1657820	1661980	System one is a fast automatic process, and I think kind of corresponds to like an LLM
1661980	1665500	just sampling tokens and system two is the slower, the
1665500	1668460	deliberate planning sort of part of your brain.
1669260	1673180	And so this is a paper actually from just last week because this space is pretty quickly
1673180	1673740	evolving.
1673740	1678700	It's called Tree of Thought and in Tree of Thought, the authors of this paper proposed
1678700	1684140	maintaining multiple completions for any given prompt, and then they are also scoring them
1684140	1688060	along the way and keeping the ones that are going well, if that makes sense.
1688060	1693740	And so a lot of people are like really playing around with kind of prompt engineering to
1694780	1695260	basically.
1695260	1699020	Bring back some of these abilities that we sort of have in our brain for LLMs.
1699820	1702780	Now, one thing I would like to note here is that this is not just a prompt.
1702780	1707980	This is actually prompts that are together used with some Python glue code because you
1707980	1711180	don't you actually have to maintain multiple prompts and you also have to do some tree
1711180	1715340	search algorithm here to figure out which prompts to expand, etc.
1715340	1720220	So it's a symbiosis of Python glue code and individual prompts that are called in a while
1720220	1721500	loop or in a bigger algorithm.
1722380	1724540	I also think there's a really cool parallel here to AlphaGo.
1724540	1729660	AlphaGo has a policy for placing the next stone when it plays go, and this policy was
1729660	1731900	trained originally by imitating humans.
1732460	1737180	But in addition to this policy, it also does multi-color tree search, and basically it
1737180	1740620	will play out a number of possibilities in its head and evaluate all of them and only
1740620	1741820	keep the ones that work well.
1741820	1747020	And so I think this is kind of an equivalent of AlphaGo, but for text, if that makes sense.
1748780	1753100	So just like Tree of Thought, I think more generally people are starting to really explore
1753100	1757900	more general techniques of not just the simple question answer prompts, but something
1757900	1761980	that looks a lot more like Python glue code stringing together many prompts.
1761980	1767500	So on the right, I have an example from this paper called React, where they structure the
1767500	1774140	answer to a prompt as a sequence of thought, action, observation, thought, action, observation,
1774140	1777660	and it's a full rollout, a kind of a thinking process to answer the query.
1778300	1781500	And in these actions, the model is also allowed to tool use.
1782220	1782860	On the left.
1783100	1785340	I have an example of AutoGPT.
1785340	1792460	And now AutoGPT, by the way, is a project that I think got a lot of hype recently, but
1792460	1794860	I think I still find it kind of inspirationally interesting.
1795980	1800940	It's a project that allows an LLM to sort of keep a task list and continue to recursively
1800940	1802060	break down tasks.
1802060	1805420	And I don't think this currently works very well, and I would not advise people to use
1805420	1807020	it in practical applications.
1807020	1810060	I just think it's something to generally take inspiration from in terms of where this is
1810060	1811180	going, I think, over time.
1811180	1816220	So that's kind of like giving our model system to thinking.
1816220	1820940	The next thing that I find kind of interesting is this following sort of, I would say, almost
1820940	1825500	psychological quirk of LLMs is that LLMs don't want to succeed.
1826540	1827500	They want to imitate.
1828460	1830380	You want to succeed, and you should ask for it.
1831180	1836620	So what I mean by that is when Transformers are trained, they have training sets, and
1837500	1841180	there can be an entire spectrum of performance qualities in their training data.
1841180	1844780	So, for example, there could be some kind of a prompt for some physics question or something
1844780	1848380	like that, and there could be a student solution that is completely wrong, but there can also
1848380	1850460	be an expert answer that is extremely right.
1851020	1855980	And Transformers can't tell the difference between like, I mean, they know about low
1855980	1859740	quality solutions and high quality solutions, but by default, they want to imitate all of
1859740	1862300	it because they're just trained on language modeling.
1862300	1866060	And so at test time, you actually have to ask for a good performance.
1866060	1870300	So in this example, in this paper, they tried various prompts.
1871180	1874700	Let's think step by step was very powerful because it sort of like spread out the reasoning
1874700	1875660	over many tokens.
1875660	1879740	But what worked even better is let's work this out in a step by step way to be sure we
1879740	1880860	have the right answer.
1880860	1883740	And so it's kind of like conditioning on getting the right answer.
1883740	1887180	And this actually makes the Transformer work better because the Transformer doesn't have
1887180	1891100	to now hedge its probability mass on low quality solutions.
1891100	1892860	As ridiculous as that sounds.
1892860	1897260	And so basically, don't feel free to ask for a strong solution.
1897260	1899740	Say something like you are a leading expert on this topic.
1899740	1901020	Pretend you have IQ 120.
1901180	1901980	Et cetera.
1901980	1907020	But don't try to ask for too much IQ because if you ask for IQ like 400, you might be out
1907020	1912060	of data distribution or even worse, you could be in data distribution for some like sci-fi
1912060	1916380	stuff and it will start to like take on some sci-fi like role playing or something like
1916380	1916940	that.
1916940	1918780	So you have to find like the right amount of IQ.
1919580	1921660	I think it's got some U-shaped curve there.
1923100	1928860	Next up, as we saw when we are trying to solve problems, we know what we are good at and
1928860	1929660	what we're not good at.
1929660	1931020	And we lean on tools.
1931020	1931980	Computationally.
1931980	1934460	You want to do the same potentially with your LLMs.
1935100	1941420	So in particular, we may want to give them calculators, code interpreters, and so on.
1941980	1943260	The ability to do search.
1943820	1946620	And there's a lot of techniques for doing that.
1947180	1951580	One thing to keep in mind again is that these Transformers by default may not know what
1951580	1952780	they don't know.
1952780	1956700	So you may even want to tell the Transformer in a prompt, you are not very good at mental
1956700	1957500	arithmetic.
1957500	1961020	Whenever you need to do very large number addition, multiplication, or whatever, you're
1961020	1962460	going to use this calculator.
1962460	1963740	Here's how you use the calculator.
1963740	1966380	Use this token combination, et cetera, et cetera.
1966380	1969580	So you have to actually like spell it out because the model by default doesn't know
1969580	1971660	what it's good at or not good at necessarily.
1971660	1973500	Just like you and I might be.
1975500	1980540	Next up, I think something that is very interesting is we went from a world that was retrieval
1980540	1985420	only all the way the pendulum has swung to the other extreme where it's memory only in
1985420	1986060	LLMs.
1986060	1990380	But actually, there's this entire space in between of these retrieval augmented models.
1990380	1990940	And this was a very interesting thing.
1990940	1992380	This works extremely well in practice.
1993180	1997100	As I mentioned, the context window of a Transformer is its working memory.
1997100	2001260	If you can load the working memory with any information that is relevant to the task,
2001260	2005660	the model will work extremely well because it can immediately access all that memory.
2006380	2012060	And so I think a lot of people are really interested in basically retrieval augmented
2012060	2013020	generation.
2013020	2017180	And on the bottom, I have like an example of LLMA index, which is one sort of data connector
2017180	2018940	to lots of different types of data.
2018940	2020540	And you can make it.
2020940	2024220	You can index all of that data, and you can make it accessible to LLMs.
2024220	2028460	And the emerging recipe there is you take relevant documents, you split them up into
2028460	2032860	chunks, you embed all of them, and you basically get embedding vectors that represent that
2032860	2033420	data.
2033420	2037420	You store that in the vector store, and then at test time, you make some kind of a query
2037420	2041980	to your vector store, and you fetch chunks that might be relevant to your task, and you
2041980	2044060	stuff them into the prompt, and then you generate.
2044060	2046300	So this can work quite well in practice.
2046300	2050380	So this is, I think, similar to when you and I solve problems, you can do everything from
2050380	2053660	your memory, and transformers have very large and extensive memory.
2053660	2057580	But also, it really helps to reference some primary documents.
2057580	2061420	So whenever you find yourself going back to a textbook to find something, or whenever
2061420	2066140	you find yourself going back to documentation of a library to look something up, the transformers
2066140	2067660	definitely want to do that too.
2067660	2072540	You have some memory over how some documentation of a library works, but it's much better to
2072540	2073180	look it up.
2073180	2074780	So the same applies here.
2076860	2079580	Next, I wanted to briefly talk about constraint prompting.
2079580	2080300	I also find this very useful.
2080300	2080860	Very interesting.
2082140	2090220	This is basically techniques for forcing a certain template in the outputs of LLMs.
2090220	2092780	So guidance is one example from Microsoft, actually.
2093340	2097260	And here we are enforcing that the output from the LLM will be JSON.
2097820	2101980	And this will actually guarantee that the output will take on this form, because they
2101980	2104860	go in and they mess with the probabilities of all the different tokens that come out
2104860	2107500	of the transformer, and they clamp those tokens.
2107500	2109980	And then the transformer is only filling in the blanks here.
2109980	2113340	And then you can enforce additional restrictions on what could go into those blanks.
2113340	2114700	So this might be really helpful.
2114700	2117340	And I think this kind of constraint sampling is also extremely interesting.
2120060	2122380	I also wanted to say a few words about fine tuning.
2122380	2127260	It is the case that you can get really far with prompt engineering, but it's also possible
2127260	2128940	to think about fine tuning your models.
2129580	2133020	Now, fine tuning models means that you are actually going to change the weights of the
2133020	2133340	model.
2134220	2138780	It is becoming a lot more accessible to do this in practice, and that's because of a
2138780	2139820	number of techniques that have been developed.
2139820	2143020	And I have libraries for very recently.
2143020	2146780	So, for example, parameter efficient fine tuning techniques like LoRa make sure that
2147660	2150940	you're only training small, sparse pieces of your model.
2150940	2155260	So most of the model is kept clamped at the base model, and some pieces of it are allowed
2155260	2155820	to change.
2155820	2159900	And this still works pretty well empirically, and makes it much cheaper to sort of tune
2159900	2161100	only small pieces of your model.
2163020	2167100	It also means that because most of your model is clamped, you can use very low precision
2167100	2169740	inference for computing those parts, because they are
2169740	2171660	not going to be updated by gradient descent.
2171660	2173580	And so that makes everything a lot more efficient as well.
2174220	2177420	And in addition, we have a number of open sourced, high quality based models.
2177420	2181580	Currently, as I mentioned, I think LAMA is quite nice, although it is not commercially
2181580	2182700	licensed, I believe, right now.
2184300	2189580	Something to keep in mind is that basically fine tuning is a lot more technically involved.
2189580	2192700	It requires a lot more, I think, technical expertise to do right.
2192700	2197020	It requires human data contractors for data sets and or synthetic data pipelines that
2197020	2197980	can be pretty complicated.
2198540	2201260	This will definitely slow down your iteration cycle by a lot.
2201820	2207420	And I would say on a high level, SFT is achievable, because it is just your continuing the language
2207420	2208140	modeling task.
2208140	2209660	It's relatively straightforward.
2209660	2215020	But RLHF, I would say, is very much research territory, and is even much harder to get
2215020	2215740	to work.
2215740	2220620	And so I would probably not advise that someone just tries to roll their own RLHF implementation.
2220620	2224540	These things are pretty unstable, very difficult to train, not something that is, I think,
2224540	2225900	very beginner friendly right now.
2225900	2227900	And it's also potentially likely, also.
2228460	2230220	To change pretty rapidly still.
2232140	2234940	So I think these are my sort of default recommendations right now.
2235660	2238220	I would break up your task into two major parts.
2238220	2240460	Number one, achieve your top performance.
2240460	2243260	And number two, optimize your performance, in that order.
2244220	2247500	Number one, the best performance will currently come from GFT4 model.
2247500	2249020	It is the most capable model by far.
2249900	2251820	Use prompts that are very detailed.
2251820	2255660	They have lots of task contents, relevant information and instructions.
2256460	2257740	Think along the lines of, what would you do?
2257740	2260860	Would you tell a task contractor if they can't email you back?
2260860	2264860	But then also keep in mind that a task contractor is a human, and they have inner monologue,
2264860	2266300	and they're very clever, et cetera.
2266300	2268620	LLMs do not possess those qualities.
2268620	2275580	So make sure to think through the psychology of the LLM, almost, and cater prompts to that.
2275580	2280620	Retrieve and add any relevant context and information to these prompts.
2280620	2283100	Basically refer to a lot of the prompt engineering techniques.
2283100	2285100	Some of them I've highlighted in the slides above.
2285100	2287340	But also, this is a very large space.
2287340	2291740	And I would just advise you to look for prompt engineering techniques online.
2291740	2292860	There's a lot to cover there.
2293740	2295820	Experiment with few-shot examples.
2295820	2297820	What this refers to is, you don't just want to tell.
2297820	2299980	You want to show, whenever it's possible.
2299980	2302060	So give it examples of everything.
2302060	2304380	That helps it really understand what you mean, if you can.
2305820	2309660	Experiment with tools and plug-ins to offload a task that are difficult for LLMs natively.
2311180	2315820	And then think about not just a single prompt and answer, think about potential chains and reflection,
2315820	2317020	and how you glue them together.
2317340	2319420	How you could potentially make multiple samples, and so on.
2320700	2323420	Finally, if you think you've squeezed out prompt engineering,
2323420	2325260	which I think you should stick with for a while,
2325900	2331420	look at some potentially fine-tuning a model to your application.
2331420	2334060	But expect this to be a lot more slower and involved.
2334060	2338300	And then there's an expert fragile research zone here, and I would say that is RLHF,
2338300	2342220	which currently does work a bit better than SFT, if you can get it to work.
2342220	2344700	But again, this is pretty involved, I would say.
2345260	2347340	And to optimize your costs, try to explore,
2347340	2351100	look for lower capacity models, or shorter prompts, and so on.
2353420	2355900	I also wanted to say a few words about the use cases,
2355900	2358940	in which I think LLMs are currently well-suited for.
2358940	2362860	So in particular, note that there's a large number of limitations to LLMs today.
2362860	2366540	And so I would keep that definitely in mind for all your applications.
2366540	2368780	Models, and this, by the way, could be an entire talk,
2368780	2370940	so I don't have time to cover it in full detail.
2370940	2374220	Models may be biased, they may fabricate, hallucinate information.
2374220	2375340	They may have reasoning errors.
2375340	2376540	They may struggle.
2376540	2377980	LLMs can run entire classes of applications.
2378540	2379980	They have knowledge cut-offs,
2379980	2383820	so they might not know any information above, say, September 2021.
2383820	2386380	They are susceptible to a large range of attacks,
2386380	2388780	which are sort of like coming out on Twitter daily,
2388780	2392380	including prompt injection, jailbreak attacks, data poisoning attacks, and so on.
2392940	2397580	So my recommendation right now is use LLMs in low-stakes applications,
2397580	2400220	combine them with always with human oversight,
2400220	2402860	use them as a source of inspiration and suggestions,
2402860	2405740	and think co-pilots instead of completely autonomous agents,
2405740	2407340	that are just like performing a task somewhere.
2407900	2410300	It's just not clear that the models are there right now.
2412300	2415020	So I wanted to close by saying that GPT-4 is an amazing artifact.
2415020	2416300	I'm very thankful that it exists.
2416860	2418220	And it's beautiful.
2418220	2420220	It has a ton of knowledge across so many areas.
2420220	2423020	It can do math, code, and so on.
2423020	2425980	And in addition, there's this thriving ecosystem of everything else
2425980	2428700	that is being built and incorporated into the ecosystem.
2429260	2431340	Some of these things I've talked about.
2432060	2435180	And all of this power is accessible at your fingertips.
2435740	2438540	So here's everything that's needed in terms of code
2438540	2441740	to ask GPT-4 a question, to prompt it, and get a response.
2442460	2443420	In this case, I said,
2444060	2447260	can you say something to inspire the audience of Microsoft Build 2023?
2447900	2453020	And I just punched this into Python, and verbatim, GPT-4 said the following.
2454060	2457340	And by the way, I did not know that they used this trick in the keynote.
2457340	2462700	So I thought I was being clever, but it is really good at this.
2462700	2465660	It says, ladies and gentlemen, innovators and trailblazers, Microsoft.
2465740	2467340	Build 2023.
2467340	2469980	Welcome to the gathering of brilliant minds like no other.
2469980	2471740	You are the architects of the future,
2471740	2475580	the visionaries molding the digital realm in which humanity thrives.
2475580	2477820	Embrace the limitless possibilities of technologies
2477820	2480380	and let your ideas soar as high as your imagination.
2480380	2482860	Together, let's create a more connected, remarkable,
2482860	2485260	and inclusive world for generations to come.
2485260	2487260	Get ready to unleash your creativity,
2487260	2490300	canvass the unknown, and turn dreams into reality.
2490300	2492140	Your journey begins today.
2495740	2500220	www.microsoft.com
2525740	2527740	www.microsoft.com
2555740	2557740	www.microsoft.com
