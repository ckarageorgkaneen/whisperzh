1
00:00:00,000 --> 00:00:04,400
Hi everyone. Today we are continuing our implementation of MakeMore, our favorite

2
00:00:04,400 --> 00:00:09,000
character-level language model. Now, you'll notice that the background behind me is different. That's

3
00:00:09,000 --> 00:00:14,380
because I am in Kyoto, and it is awesome. So I'm in a hotel room here. Now, over the last few

4
00:00:14,380 --> 00:00:19,620
lectures, we've built up to this architecture that is a multi-layer perceptron character-level

5
00:00:19,620 --> 00:00:23,700
language model. So we see that it receives three previous characters and tries to predict the

6
00:00:23,700 --> 00:00:28,360
fourth character in a sequence using a very simple multi-layer perceptron using one hidden

7
00:00:28,360 --> 00:00:33,460
layer of neurons with tenational neuralities. So what we'd like to do now in this lecture is I'd

8
00:00:33,460 --> 00:00:37,400
like to complexify this architecture. In particular, we would like to take more characters

9
00:00:37,400 --> 00:00:42,660
in a sequence as an input, not just three. And in addition to that, we don't just want to feed

10
00:00:42,660 --> 00:00:47,020
them all into a single hidden layer because that squashes too much information too quickly.

11
00:00:47,680 --> 00:00:52,760
Instead, we would like to make a deeper model that progressively fuses this information to make

12
00:00:52,760 --> 00:00:57,960
its guess about the next character in a sequence. And so we'll see that as we make this architecture

13
00:00:57,960 --> 00:00:58,340
more complex, we'll be able to make a more complex model that progressively fuses this

14
00:00:58,340 --> 00:01:01,440
information to make it more complex. We're actually going to arrive at something that looks

15
00:01:01,440 --> 00:01:06,520
very much like a WaveNet. So WaveNet is this paper published by DeepMind in 2016.

16
00:01:07,480 --> 00:01:13,500
And it is also a language model, basically, but it tries to predict audio sequences instead of

17
00:01:13,500 --> 00:01:19,280
character-level sequences or word-level sequences. But fundamentally, the modeling setup is

18
00:01:19,280 --> 00:01:24,280
identical. It is an autoregressive model, and it tries to predict the next character in a sequence.

19
00:01:24,280 --> 00:01:27,940
And the architecture actually takes this interesting hierarchical

20
00:01:27,940 --> 00:01:33,920
sort of approach to predicting the next character in a sequence with this tree-like structure.

21
00:01:34,760 --> 00:01:38,720
And this is the architecture, and we're going to implement it in the course of this video.

22
00:01:39,040 --> 00:01:44,260
So let's get started. So the starter code for part five is very similar to where we ended up in

23
00:01:44,260 --> 00:01:49,180
in part three. Recall that part four was the manual dot propagation exercise. That is kind

24
00:01:49,180 --> 00:01:54,120
of an aside. So we are coming back to part three, copy-pasting chunks out of it. And that is our

25
00:01:54,120 --> 00:01:56,920
starter code for part five. I've changed very few things otherwise.

26
00:01:57,940 --> 00:02:02,400
However, both should look familiar to if you've gone through part three. So in particular, very

27
00:02:02,400 --> 00:02:08,880
briefly, we are doing imports. We are reading our data set of words. And we are processing the

28
00:02:08,880 --> 00:02:14,040
dataset of words into individual examples. And none of this data generation code has changed.

29
00:02:14,040 --> 00:02:20,060
And basically we have lots and lots of examples. In particular, we have 182,000 examples

30
00:02:20,060 --> 00:02:26,200
of three characters try to predict the fourth one. And we've broken up every one of these words into

31
00:02:26,200 --> 00:02:27,060
little problems.

32
00:02:27,060 --> 00:02:30,660
given three characters predict the fourth one so this is our data set and this is what we're

33
00:02:30,660 --> 00:02:37,220
trying to get the neural net to do now in part three we started to develop our code around these

34
00:02:37,220 --> 00:02:43,220
layer modules that are for example a class linear and we're doing this because we want to think of

35
00:02:43,220 --> 00:02:48,980
these modules as building blocks and like a lego building block bricks that we can sort of like

36
00:02:48,980 --> 00:02:54,260
stack up into neural networks and we can feed data between these layers and stack them up into

37
00:02:54,260 --> 00:03:01,700
sort of graphs now we also developed these layers to have apis and signatures very similar to those

38
00:03:01,700 --> 00:03:06,740
that are found in pytorch so we have torch.nn and it's got all these layer building blocks that you

39
00:03:06,740 --> 00:03:11,940
would use in practice and we were developing all these to mimic the apis of these so for example

40
00:03:11,940 --> 00:03:17,780
we have linear so there will also be a torch.nn.linear and its signature will be very

41
00:03:17,780 --> 00:03:22,580
similar to our signature and the functionality will be also quite identical as far as i'm aware

42
00:03:22,580 --> 00:03:23,620
so we have the linear layer

43
00:03:24,260 --> 00:03:30,740
with the batchnorm1d layer and the 10h layer that we developed previously and linear just does a

44
00:03:30,740 --> 00:03:36,500
matrix multiply in the forward pass of this module batchnorm of course is this crazy layer that we

45
00:03:36,500 --> 00:03:41,300
developed in the previous lecture and what's crazy about it is well there's many things

46
00:03:42,020 --> 00:03:47,220
number one it has these running mean and variances that are trained outside of back propagation they

47
00:03:47,220 --> 00:03:53,540
are trained using exponential moving average inside this layer when we call the forward pass

48
00:03:54,420 --> 00:03:59,460
uh in addition to that there's this training flag because the behavior of bastion is different

49
00:03:59,460 --> 00:04:03,380
during train time and evaluation time and so suddenly we have to be very careful that

50
00:04:03,380 --> 00:04:07,860
bastion is in its correct state that it's in the evaluation state or training state so that's

51
00:04:07,860 --> 00:04:12,740
something to now keep track of something that sometimes introduces bugs because you forget to

52
00:04:12,740 --> 00:04:18,020
put it into the right mode and finally we saw that bastion couples the statistics or the the

53
00:04:18,020 --> 00:04:24,020
activations across the examples in the batch so normally we thought of the batch as just an efficiency thing

54
00:04:24,820 --> 00:04:30,900
but now we are coupling the computation across batch elements and it's done for the purposes of

55
00:04:30,900 --> 00:04:36,340
controlling the activation statistics as we saw in the previous video so it's a very weird layer

56
00:04:36,340 --> 00:04:41,300
at least a lot of bugs um partly for example because you have to modulate the training and

57
00:04:41,300 --> 00:04:48,740
eval phase and so on um in addition for example you have to wait for uh the mean and the variance

58
00:04:48,740 --> 00:04:53,780
to settle and to actually reach a steady state and so um you have to make sure that you

59
00:04:54,260 --> 00:05:01,540
if you write the problem invalid you can more simply to you know drag and drop um

60
00:05:02,260 --> 00:05:22,900
to get whatever range you want uh and i want to show you the behavior of bastion

61
00:05:24,260 --> 00:05:25,940
And then we have a list of layers.

62
00:05:25,940 --> 00:05:29,840
And it's a linear, feeds to BatchNorm, feeds to 10H,

63
00:05:29,840 --> 00:05:31,780
and then a linear output layer.

64
00:05:31,780 --> 00:05:33,200
And its weights are scaled down,

65
00:05:33,200 --> 00:05:36,720
so we are not confidently wrong at initialization.

66
00:05:36,720 --> 00:05:39,260
We see that this is about 12,000 parameters.

67
00:05:39,260 --> 00:05:42,860
We're telling PyTorch that the parameters require gradients.

68
00:05:42,860 --> 00:05:45,700
The optimization is, as far as I'm aware, identical,

69
00:05:45,700 --> 00:05:47,940
and should look very, very familiar.

70
00:05:47,940 --> 00:05:48,980
Nothing changed here.

71
00:05:50,000 --> 00:05:52,580
Loss function looks very crazy.

72
00:05:52,580 --> 00:05:54,020
We should probably fix this.

73
00:05:54,020 --> 00:05:57,280
And that's because 32 batch elements are too few.

74
00:05:57,280 --> 00:05:59,880
And so you can get very lucky or unlucky

75
00:05:59,880 --> 00:06:01,200
in any one of these batches,

76
00:06:01,200 --> 00:06:04,240
and it creates a very thick loss function.

77
00:06:04,240 --> 00:06:06,440
So we're gonna fix that soon.

78
00:06:06,440 --> 00:06:09,240
Now, once we want to evaluate the trained neural network,

79
00:06:09,240 --> 00:06:11,400
we need to remember, because of the BatchNorm layers,

80
00:06:11,400 --> 00:06:14,360
to set all the layers to be training equals false.

81
00:06:14,360 --> 00:06:17,240
This only matters for the BatchNorm layer so far.

82
00:06:17,240 --> 00:06:18,860
And then we evaluate.

83
00:06:20,340 --> 00:06:23,740
We see that currently we have validation loss of 2.10,

84
00:06:23,740 --> 00:06:27,200
which is fairly good, but there's still a ways to go.

85
00:06:27,200 --> 00:06:30,500
But even at 2.10, we see that when we sample from the model,

86
00:06:30,500 --> 00:06:33,380
we actually get relatively name-like results

87
00:06:33,380 --> 00:06:35,100
that do not exist in a training set.

88
00:06:35,100 --> 00:06:40,100
So for example, Yvonne, Kilo, Pros, Alaya, et cetera.

89
00:06:41,600 --> 00:06:46,200
So certainly not reasonable, not unreasonable, I would say,

90
00:06:46,200 --> 00:06:47,320
but not amazing.

91
00:06:47,320 --> 00:06:49,760
And we can still push this validation loss even lower

92
00:06:49,760 --> 00:06:52,860
and get much better samples that are even more name-like.

93
00:06:53,740 --> 00:06:56,640
So let's improve this model now.

94
00:06:56,640 --> 00:06:58,200
Okay, first, let's fix this graph,

95
00:06:58,200 --> 00:06:59,700
because it is daggers in my eyes,

96
00:06:59,700 --> 00:07:02,000
and I just can't take it anymore.

97
00:07:02,000 --> 00:07:06,900
So lossI, if you recall, is a Python list of floats.

98
00:07:06,900 --> 00:07:10,020
So for example, the first 10 elements look like this.

99
00:07:11,060 --> 00:07:12,340
Now, what we'd like to do, basically,

100
00:07:12,340 --> 00:07:15,180
is we need to average up some of these values

101
00:07:15,180 --> 00:07:19,780
to get a more sort of representative value along the way.

102
00:07:19,780 --> 00:07:21,860
So one way to do this is the following.

103
00:07:21,860 --> 00:07:23,040
In PyTorch, if I create, for example, this,

104
00:07:23,040 --> 00:07:23,680
I'm gonna do this.

105
00:07:23,680 --> 00:07:27,480
If I create, for example, a tensor of the first 10 numbers,

106
00:07:27,480 --> 00:07:29,920
then this is currently a one-dimensional array.

107
00:07:29,920 --> 00:07:32,840
But recall that I can view this array as two-dimensional.

108
00:07:32,840 --> 00:07:35,800
So for example, I can view it as a two-by-five array,

109
00:07:35,800 --> 00:07:39,140
and this is a 2D tensor now, two-by-five.

110
00:07:39,140 --> 00:07:41,460
And you see what PyTorch has done is that the first row

111
00:07:41,460 --> 00:07:43,800
of this tensor is the first five elements,

112
00:07:43,800 --> 00:07:46,840
and the second row is the second five elements.

113
00:07:46,840 --> 00:07:50,180
I can also view it as a five-by-two as an example.

114
00:07:50,180 --> 00:07:53,680
And then recall that I can also use negative one

115
00:07:53,680 --> 00:07:55,920
in place of one of these numbers.

116
00:07:55,920 --> 00:07:58,640
And PyTorch will calculate what that number must be

117
00:07:58,640 --> 00:08:01,080
in order to make the number of elements work out.

118
00:08:01,080 --> 00:08:04,720
So this can be this, or like that.

119
00:08:04,720 --> 00:08:05,520
Both will work.

120
00:08:05,520 --> 00:08:06,720
Of course, this would not work.

121
00:08:09,220 --> 00:08:12,680
OK, so this allows it to spread out some of the consecutive values

122
00:08:12,680 --> 00:08:13,720
into rows.

123
00:08:13,720 --> 00:08:15,720
So that's very helpful, because what we can do now

124
00:08:15,720 --> 00:08:19,600
is, first of all, we're going to create a Torch.tensor out

125
00:08:19,600 --> 00:08:22,480
of the list of floats.

126
00:08:22,480 --> 00:08:23,520
And then we're going to view it.

127
00:08:23,520 --> 00:08:26,520
As whatever it is, but we're going

128
00:08:26,520 --> 00:08:30,640
to stretch it out into rows of 1,000 consecutive elements.

129
00:08:30,640 --> 00:08:34,560
So the shape of this now becomes 200 by 1,000.

130
00:08:34,560 --> 00:08:39,520
And each row is 1,000 consecutive elements in this list.

131
00:08:39,520 --> 00:08:41,020
So that's very helpful, because now we

132
00:08:41,020 --> 00:08:44,040
can do a mean along the rows.

133
00:08:44,040 --> 00:08:47,240
And the shape of this will just be 200.

134
00:08:47,240 --> 00:08:49,840
And so we've taken basically the mean on every row.

135
00:08:49,840 --> 00:08:53,060
So plt.plot of that should be something nicer.

136
00:08:53,060 --> 00:08:55,180
Much better.

137
00:08:55,180 --> 00:08:57,740
So we see that we've basically made a lot of progress.

138
00:08:57,740 --> 00:09:00,900
And then here, this is the learning rate decay.

139
00:09:00,900 --> 00:09:03,180
So here we see that the learning rate decay subtracted

140
00:09:03,180 --> 00:09:04,900
a ton of energy out of the system,

141
00:09:04,900 --> 00:09:07,820
and allowed us to settle into the local minimum

142
00:09:07,820 --> 00:09:09,500
in this optimization.

143
00:09:09,500 --> 00:09:11,420
So this is a much nicer plot.

144
00:09:11,420 --> 00:09:14,680
Let me come up and delete the monster.

145
00:09:14,680 --> 00:09:16,700
And we're going to be using this going forward.

146
00:09:16,700 --> 00:09:18,240
Now, next up, what I'm bothered by

147
00:09:18,240 --> 00:09:21,180
is that you see our forward pass is a little bit gnarly, and takes a lot of time.

148
00:09:21,180 --> 00:09:21,940
So we're going to go ahead and do that.

149
00:09:21,940 --> 00:09:22,060
And we're going to go ahead and do that.

150
00:09:22,060 --> 00:09:23,060
And we're going to go ahead and do that.

151
00:09:23,060 --> 00:09:24,740
And we're going to explain too many lines of code.

152
00:09:24,740 --> 00:09:27,380
So in particular, we see that we've organized some of the layers

153
00:09:27,380 --> 00:09:31,900
inside the layers list, but not all of them for no reason.

154
00:09:31,900 --> 00:09:34,780
So in particular, we see that we still have the embedding table special

155
00:09:34,780 --> 00:09:37,020
cased outside of the layers.

156
00:09:37,020 --> 00:09:39,780
And in addition to that, the viewing operation here

157
00:09:39,780 --> 00:09:41,680
is also outside of our layers.

158
00:09:41,680 --> 00:09:43,640
Let's create layers for these, and then we

159
00:09:43,640 --> 00:09:46,600
can add those layers to just our list.

160
00:09:46,600 --> 00:09:49,540
So in particular, the two things that we need is here,

161
00:09:49,540 --> 00:09:52,580
we have this embedding table, and we are indexing

162
00:09:52,580 --> 00:09:58,700
at the integers inside the batch xb, inside the tensor xb.

163
00:09:58,700 --> 00:10:02,580
So that's an embedding table lookup just done with indexing.

164
00:10:02,580 --> 00:10:04,700
And then here, we see that we have this view operation,

165
00:10:04,700 --> 00:10:06,620
which if you recall from the previous video,

166
00:10:06,620 --> 00:10:11,000
simply rearranges the character embeddings

167
00:10:11,000 --> 00:10:13,080
and stretches them out into a row.

168
00:10:13,080 --> 00:10:16,420
And effectively, what that does is the concatenation operation,

169
00:10:16,420 --> 00:10:19,100
basically, except it's free because viewing

170
00:10:19,100 --> 00:10:21,180
is very cheap in PyTorch.

171
00:10:21,180 --> 00:10:23,420
And no memory is being copied.

172
00:10:23,420 --> 00:10:26,040
We're just re-representing how we view that tensor.

173
00:10:26,040 --> 00:10:30,780
So let's create modules for both of these operations,

174
00:10:30,780 --> 00:10:34,060
the embedding operation and the flattening operation.

175
00:10:34,060 --> 00:10:38,860
So I actually wrote the code just to save some time.

176
00:10:38,860 --> 00:10:41,860
So we have a module embedding and a module flatten.

177
00:10:41,860 --> 00:10:44,880
And both of them simply do the indexing operation

178
00:10:44,880 --> 00:10:49,620
in a forward pass and the flattening operation here.

179
00:10:49,620 --> 00:10:51,060
And this.

180
00:10:51,180 --> 00:10:56,880
C now will just become a self.weight inside an embedding module.

181
00:10:56,880 --> 00:10:59,760
And I'm calling these layers specifically embedding and flatten

182
00:10:59,760 --> 00:11:03,080
because it turns out that both of them actually exist in PyTorch.

183
00:11:03,080 --> 00:11:05,820
So in PyTorch, we have n and dot embedding.

184
00:11:05,820 --> 00:11:07,260
And it also takes the number of embeddings

185
00:11:07,260 --> 00:11:10,960
and the dimensionality of the embedding, just like we have here.

186
00:11:10,960 --> 00:11:13,340
But in addition, PyTorch takes in a lot of other keyword arguments

187
00:11:13,340 --> 00:11:18,080
that we are not using for our purposes yet.

188
00:11:18,080 --> 00:11:20,720
And for flatten, that also exists in PyTorch.

189
00:11:20,720 --> 00:11:23,840
But it also takes additional keyword arguments that we are not using.

190
00:11:23,840 --> 00:11:26,860
So we have a very simple flatten.

191
00:11:26,860 --> 00:11:27,860
But both of them exist in PyTorch.

192
00:11:27,860 --> 00:11:30,180
They're just a bit more simpler.

193
00:11:30,180 --> 00:11:38,500
And now that we have these, we can simply take out some of these special cased things.

194
00:11:38,500 --> 00:11:45,820
So instead of C, we're just going to have an embedding and vocab size and n embed.

195
00:11:45,820 --> 00:11:49,120
And then after the embedding, we are going to flatten.

196
00:11:49,120 --> 00:11:50,620
So let's construct those modules.

197
00:11:50,720 --> 00:11:53,300
And now I can take out this C.

198
00:11:53,300 --> 00:11:55,600
And here, I don't have to special case it anymore.

199
00:11:55,600 --> 00:11:59,180
Because now, C is the embedding's weight.

200
00:11:59,180 --> 00:12:02,060
And it's inside layers.

201
00:12:02,060 --> 00:12:04,340
So this should just work.

202
00:12:04,340 --> 00:12:07,960
And then here, our forward pass simplifies substantially.

203
00:12:07,960 --> 00:12:13,460
Because we don't need to do these now outside of these layers, outside and explicitly.

204
00:12:13,460 --> 00:12:15,360
They're now inside layers.

205
00:12:15,360 --> 00:12:17,320
So we can delete those.

206
00:12:17,320 --> 00:12:20,600
But now to kick things off, we want this little x,

207
00:12:20,600 --> 00:12:23,100
which in the beginning is just xb,

208
00:12:23,100 --> 00:12:27,920
the tensor of integers specifying the identities of these characters at the input.

209
00:12:27,920 --> 00:12:31,100
And so these characters can now directly feed into the first layer.

210
00:12:31,100 --> 00:12:32,740
And this should just work.

211
00:12:32,740 --> 00:12:35,480
So let me come here and insert a break.

212
00:12:35,480 --> 00:12:37,980
Because I just want to make sure that the first iteration of this runs,

213
00:12:37,980 --> 00:12:39,840
and that there's no mistake.

214
00:12:39,840 --> 00:12:41,720
So that ran properly.

215
00:12:41,720 --> 00:12:44,840
And basically, we've substantially simplified the forward pass here.

216
00:12:44,840 --> 00:12:46,740
Okay, I'm sorry, I changed my microphone.

217
00:12:46,740 --> 00:12:49,720
So hopefully, the audio is a little bit better.

218
00:12:49,720 --> 00:12:50,480
Now, one last thing.

219
00:12:50,480 --> 00:12:54,100
One more thing that I would like to do in order to PyTorchify our code even further,

220
00:12:54,100 --> 00:12:58,440
is that right now we are maintaining all of our modules in a naked list of layers.

221
00:12:58,440 --> 00:13:04,180
And we can also simplify this, because we can introduce the concept of PyTorch containers.

222
00:13:04,180 --> 00:13:07,480
So in torch.nn, which we are basically rebuilding from scratch here,

223
00:13:07,480 --> 00:13:09,340
there's a concept of containers.

224
00:13:09,340 --> 00:13:14,860
And these containers are basically a way of organizing layers into lists or dicts and

225
00:13:14,860 --> 00:13:15,860
so on.

226
00:13:15,860 --> 00:13:20,480
So in particular, there's a sequential, which maintains a list of layers, and there's a

227
00:13:20,480 --> 00:13:22,760
module class in PyTorch.

228
00:13:22,760 --> 00:13:26,940
And it basically just passes a given input through all the layers sequentially, exactly

229
00:13:26,940 --> 00:13:28,980
as we are doing here.

230
00:13:28,980 --> 00:13:31,200
So let's write our own sequential.

231
00:13:31,200 --> 00:13:32,840
I've written a code here.

232
00:13:32,840 --> 00:13:36,000
And basically, the code for sequential is quite straightforward.

233
00:13:36,000 --> 00:13:39,140
We pass in a list of layers, which we keep here.

234
00:13:39,140 --> 00:13:43,180
And then given any input in a forward pass, we just call all the layers sequentially and

235
00:13:43,180 --> 00:13:44,180
return the result.

236
00:13:44,180 --> 00:13:48,400
And in terms of the parameters, it's just all the parameters of the child modules.

237
00:13:48,400 --> 00:13:49,600
So we can run this.

238
00:13:49,600 --> 00:13:50,100
Okay.

239
00:13:50,480 --> 00:13:54,520
And again, simplify this substantially, because we don't maintain this naked list of layers.

240
00:13:54,520 --> 00:14:01,000
We now have a notion of a model, which is a module, and in particular, is a sequential

241
00:14:01,000 --> 00:14:05,040
of all these layers.

242
00:14:05,040 --> 00:14:09,700
And now parameters are simply just model.parameters.

243
00:14:09,700 --> 00:14:14,100
And so that list comprehension now lives here.

244
00:14:14,100 --> 00:14:18,100
And then here we are doing all the things we used to do.

245
00:14:18,100 --> 00:14:19,600
Now here, the code again simplifies substantially.

246
00:14:19,600 --> 00:14:25,320
Because we don't have to do this forwarding here, instead we just call the model on the

247
00:14:25,320 --> 00:14:26,320
input data.

248
00:14:26,320 --> 00:14:29,600
And the input data here are the integers inside xb.

249
00:14:29,600 --> 00:14:33,900
So we can simply do logits, which are the outputs of our model, are simply the model

250
00:14:33,900 --> 00:14:37,040
called on xb.

251
00:14:37,040 --> 00:14:41,520
And then the cross entropy here takes the logits and the targets.

252
00:14:41,520 --> 00:14:44,120
So this simplifies substantially.

253
00:14:44,120 --> 00:14:45,840
And then this looks good.

254
00:14:45,840 --> 00:14:47,440
So let's just make sure this runs.

255
00:14:47,440 --> 00:14:48,440
That looks good.

256
00:14:48,440 --> 00:14:49,440
Okay.

257
00:14:49,440 --> 00:14:53,760
Now here, we actually have some work to do still here, but I'm going to come back later.

258
00:14:53,760 --> 00:14:55,160
For now, there's no more layers.

259
00:14:55,160 --> 00:14:57,040
There's a model that layers.

260
00:14:57,040 --> 00:15:00,900
But it's naughty to access attributes of these classes directly.

261
00:15:00,900 --> 00:15:03,320
So we'll come back and fix this later.

262
00:15:03,320 --> 00:15:07,620
And then here, of course, this simplifies substantially as well, because logits are

263
00:15:07,620 --> 00:15:10,840
the model called on x.

264
00:15:10,840 --> 00:15:14,260
And then these logits come here.

265
00:15:14,260 --> 00:15:18,120
So we can evaluate the train and validation loss, which currently is terrible, because

266
00:15:18,120 --> 00:15:19,280
we just initialized it in neural net.

267
00:15:19,280 --> 00:15:21,800
And then we can also sample from the model.

268
00:15:21,800 --> 00:15:27,100
And this simplifies dramatically as well, because we just want to call the model onto

269
00:15:27,100 --> 00:15:30,520
the context and outcome logits.

270
00:15:30,520 --> 00:15:34,600
And then these logits go into softmax and get the probabilities, et cetera.

271
00:15:34,600 --> 00:15:38,400
So we can sample from this model.

272
00:15:38,400 --> 00:15:39,400
What did I screw up?

273
00:15:39,400 --> 00:15:40,400
Okay.

274
00:15:40,400 --> 00:15:47,160
So I fixed the issue and we now get the result that we expect, which is gibberish, because

275
00:15:47,160 --> 00:15:48,440
the model is not trained.

276
00:15:48,440 --> 00:15:49,160
Okay.

277
00:15:49,160 --> 00:15:50,800
So we initialize it from scratch.

278
00:15:50,800 --> 00:15:55,080
The problem was that when I fixed this cell to be modeled out layers instead of just layers,

279
00:15:55,080 --> 00:15:57,420
I did not actually run the cell.

280
00:15:57,420 --> 00:16:00,460
And so our neural net was in a training mode.

281
00:16:00,460 --> 00:16:04,000
And what caused the issue here is the batch norm layer, as batch norm layer often likes

282
00:16:04,000 --> 00:16:07,280
to do, because batch norm was in the training mode.

283
00:16:07,280 --> 00:16:11,520
And here we are passing in an input, which is a batch of just a single example made up

284
00:16:11,520 --> 00:16:13,220
of the context.

285
00:16:13,220 --> 00:16:16,840
And so if you are trying to pass in a single example into a batch norm that is in the training

286
00:16:16,840 --> 00:16:17,840
mode.

287
00:16:17,840 --> 00:16:18,960
You're going to end up estimating the variance.

288
00:16:18,960 --> 00:16:24,600
Using the input and the variance of a single number is not a number because it is a measure

289
00:16:24,600 --> 00:16:25,900
of a spread.

290
00:16:25,900 --> 00:16:30,780
So for example, the variance of just a single number five, you can see is not a number.

291
00:16:30,780 --> 00:16:32,980
And so that's what happened.

292
00:16:32,980 --> 00:16:35,020
And batch norm basically caused an issue.

293
00:16:35,020 --> 00:16:38,160
And then that polluted all of the further processing.

294
00:16:38,160 --> 00:16:41,360
So all that we had to do was make sure that this runs.

295
00:16:41,360 --> 00:16:46,660
And we basically made the issue of, again, we didn't actually see the issue with the

296
00:16:46,660 --> 00:16:47,660
loss.

297
00:16:47,660 --> 00:16:48,660
We could have evaluated the loss.

298
00:16:48,960 --> 00:16:53,460
We got the wrong result because batch norm was in the training mode.

299
00:16:53,460 --> 00:16:54,460
And so we still get a result.

300
00:16:54,460 --> 00:16:59,540
It's just the wrong result because it's using the sample statistics of the batch.

301
00:16:59,540 --> 00:17:03,180
Whereas we want to use the running mean and running variance inside the batch norm.

302
00:17:03,180 --> 00:17:09,620
And so again, an example of introducing a bug in line because we did not properly maintain

303
00:17:09,620 --> 00:17:11,400
the state of what is training or not.

304
00:17:11,400 --> 00:17:12,400
Okay.

305
00:17:12,400 --> 00:17:13,480
So I rerun everything.

306
00:17:13,480 --> 00:17:14,520
And here's where we are.

307
00:17:14,520 --> 00:17:17,800
As a reminder, we have the training loss of 2.05 and validation of 2.10.

308
00:17:17,800 --> 00:17:18,800
Now, let's go back.

309
00:17:18,800 --> 00:17:22,700
Now, because these losses are very similar to each other, we have a sense that we are

310
00:17:22,700 --> 00:17:25,240
not overfitting too much on this task.

311
00:17:25,240 --> 00:17:28,880
And we can make additional progress in our performance by scaling up the size of the

312
00:17:28,880 --> 00:17:31,980
neural network and making everything bigger and deeper.

313
00:17:31,980 --> 00:17:36,600
Now, currently, we are using this architecture here, where we are taking in some number of

314
00:17:36,600 --> 00:17:40,040
characters, going into a single hidden layer, and then going to the prediction of the next

315
00:17:40,040 --> 00:17:41,420
character.

316
00:17:41,420 --> 00:17:46,400
The problem here is we don't have a naive way of making this bigger in a productive

317
00:17:46,400 --> 00:17:47,400
way.

318
00:17:47,400 --> 00:17:48,600
We could, of course, use our neural network.

319
00:17:48,600 --> 00:17:52,780
We could use our layers, sort of building blocks and materials to introduce additional

320
00:17:52,780 --> 00:17:55,280
layers here and make the network deeper.

321
00:17:55,280 --> 00:17:59,420
But it is still the case that we are crushing all of the characters into a single layer

322
00:17:59,420 --> 00:18:01,260
all the way at the beginning.

323
00:18:01,260 --> 00:18:04,980
And even if we make this a bigger layer and add neurons, it's still kind of like silly

324
00:18:04,980 --> 00:18:10,040
to squash all that information so fast in a single step.

325
00:18:10,040 --> 00:18:13,580
So what we'd like to do instead is we'd like our network to look a lot more like this in

326
00:18:13,580 --> 00:18:14,960
the WaveNet case.

327
00:18:14,960 --> 00:18:18,220
So you see in the WaveNet, when we are trying to make the prediction for the next character

328
00:18:18,220 --> 00:18:24,940
in a sequence, it is a function of the previous characters that feed in, but not all of these

329
00:18:24,940 --> 00:18:30,000
different characters are not just crushed to a single layer and then you have a sandwich.

330
00:18:30,000 --> 00:18:32,280
They are crushed slowly.

331
00:18:32,280 --> 00:18:37,800
So in particular, we take two characters and we fuse them into sort of like a bigram representation.

332
00:18:37,800 --> 00:18:40,480
And we do that for all these characters consecutively.

333
00:18:40,480 --> 00:18:47,000
And then we take the bigrams and we fuse those into four character level chunks.

334
00:18:47,000 --> 00:18:48,100
And then we fuse that again.

335
00:18:48,220 --> 00:18:52,060
And so we do that in this tree-like hierarchical manner.

336
00:18:52,060 --> 00:18:57,220
So we fuse the information from the previous context slowly into the network as it gets

337
00:18:57,220 --> 00:18:58,220
deeper.

338
00:18:58,220 --> 00:19:01,000
And so this is the kind of architecture that we want to implement.

339
00:19:01,000 --> 00:19:06,740
Now in the WaveNet's case, this is a visualization of a stack of dilated causal convolution layers.

340
00:19:06,740 --> 00:19:10,180
And this makes it sound very scary, but actually the idea is very simple.

341
00:19:10,180 --> 00:19:14,340
And the fact that it's a dilated causal convolution layer is really just an implementation detail

342
00:19:14,340 --> 00:19:15,660
to make everything fast.

343
00:19:15,660 --> 00:19:17,220
We're going to see that later.

344
00:19:17,220 --> 00:19:18,220
But for now, let's just keep going.

345
00:19:18,220 --> 00:19:21,780
We're going to keep the basic idea of it, which is this progressive fusion.

346
00:19:21,780 --> 00:19:26,220
So we want to make the network deeper, and at each level, we want to fuse only two consecutive

347
00:19:26,220 --> 00:19:27,260
elements.

348
00:19:27,260 --> 00:19:31,820
Two characters, then two bigrams, then two fourgrams, and so on.

349
00:19:31,820 --> 00:19:32,820
So let's implement this.

350
00:19:32,820 --> 00:19:36,100
Okay, so first up, let me scroll to where we built the dataset, and let's change the

351
00:19:36,100 --> 00:19:38,520
block size from three to eight.

352
00:19:38,520 --> 00:19:43,360
So we're going to be taking eight characters of context to predict the ninth character.

353
00:19:43,360 --> 00:19:45,260
So the dataset now looks like this.

354
00:19:45,260 --> 00:19:48,220
We have a lot more context feeding in to predict any next character.

355
00:19:48,220 --> 00:19:49,220
So we're going to have a sequence.

356
00:19:49,220 --> 00:19:53,660
And these eight characters are going to be processed in this tree-like structure.

357
00:19:53,660 --> 00:19:57,660
Now if we scroll here, everything here should just be able to work.

358
00:19:57,660 --> 00:19:59,960
So we should be able to redefine the network.

359
00:19:59,960 --> 00:20:04,460
You see that the number of parameters has increased by 10,000, and that's because the

360
00:20:04,460 --> 00:20:05,720
block size has grown.

361
00:20:05,720 --> 00:20:08,140
So this first linear layer is much, much bigger.

362
00:20:08,140 --> 00:20:12,740
Our linear layer now takes eight characters into this middle layer.

363
00:20:12,740 --> 00:20:14,500
So there's a lot more parameters there.

364
00:20:14,500 --> 00:20:16,260
But this should just run.

365
00:20:16,260 --> 00:20:18,140
Let me just break right after this.

366
00:20:18,140 --> 00:20:19,140
This is the very first iteration.

367
00:20:19,140 --> 00:20:21,500
So you see that this runs just fine.

368
00:20:21,500 --> 00:20:23,580
It's just that this network doesn't make too much sense.

369
00:20:23,580 --> 00:20:27,140
We're crushing way too much information way too fast.

370
00:20:27,140 --> 00:20:32,120
So let's now come in and see how we could try to implement the hierarchical scheme.

371
00:20:32,120 --> 00:20:35,800
Now before we dive into the detail of the re-implementation here, I was just curious

372
00:20:35,800 --> 00:20:39,640
to actually run it and see where we are in terms of the baseline performance of just

373
00:20:39,640 --> 00:20:42,500
lazily scaling up the context length.

374
00:20:42,500 --> 00:20:43,500
So I let it run.

375
00:20:43,500 --> 00:20:45,000
We get a nice loss curve.

376
00:20:45,000 --> 00:20:48,040
And then evaluating the loss, we actually see quite a bit of improvement.

377
00:20:48,140 --> 00:20:51,040
This is from increasing the context length.

378
00:20:51,040 --> 00:20:53,240
So I started a little bit of a performance log here.

379
00:20:53,240 --> 00:20:59,180
And previously where we were is we were getting a performance of 2.10 on the validation loss.

380
00:20:59,180 --> 00:21:05,080
And now simply scaling up the context length from 3 to 8 gives us a performance of 2.02.

381
00:21:05,080 --> 00:21:07,060
So quite a bit of an improvement here.

382
00:21:07,060 --> 00:21:10,900
And also when you sample from the model, you see that the names are definitely improving

383
00:21:10,900 --> 00:21:13,260
qualitatively as well.

384
00:21:13,260 --> 00:21:18,140
So we could, of course, spend a lot of time here tuning things and making it even bigger

385
00:21:18,140 --> 00:21:23,880
and scaling up the network further, even with the simple sort of setup here.

386
00:21:23,880 --> 00:21:27,580
But let's continue and let's implement the hierarchical model and treat this as just

387
00:21:27,580 --> 00:21:29,960
a rough baseline performance.

388
00:21:29,960 --> 00:21:34,360
But there's a lot of optimization left on the table in terms of some of the hyperparameters

389
00:21:34,360 --> 00:21:36,260
that you're hopefully getting a sense of now.

390
00:21:36,260 --> 00:21:40,400
Okay, so let's scroll up now and come back up.

391
00:21:40,400 --> 00:21:44,360
And what I've done here is I've created a bit of a scratch space for us to just look

392
00:21:44,360 --> 00:21:47,040
at the forward pass of the neural net and inspect the shape of the network.

393
00:21:47,040 --> 00:21:48,040
So let's go ahead and do that.

394
00:21:48,040 --> 00:21:50,000
So let's go ahead and look at the shape of the tensors along the way as the neural net

395
00:21:50,000 --> 00:21:52,360
forwards.

396
00:21:52,360 --> 00:21:57,580
So here I'm just temporarily for debugging, creating a batch of just, say, four examples.

397
00:21:57,580 --> 00:21:59,320
So four random integers.

398
00:21:59,320 --> 00:22:02,460
Then I'm plucking out those rows from our training set.

399
00:22:02,460 --> 00:22:06,720
And then I'm passing into the model the input XB.

400
00:22:06,720 --> 00:22:11,040
Now the shape of XB here, because we have only four examples, is four by eight.

401
00:22:11,040 --> 00:22:14,480
And this eight is now the current block size.

402
00:22:14,480 --> 00:22:17,980
So inspecting XB, we just see that we have four examples.

403
00:22:17,980 --> 00:22:21,720
Each one of them is a row of XB.

404
00:22:21,720 --> 00:22:24,620
And we have eight characters here.

405
00:22:24,620 --> 00:22:29,740
And this integer tensor just contains the identities of those characters.

406
00:22:29,740 --> 00:22:32,380
So the first layer of our neural net is the embedding layer.

407
00:22:32,380 --> 00:22:37,020
So passing XB, this integer tensor, through the embedding layer creates an output that

408
00:22:37,020 --> 00:22:39,360
is four by eight by 10.

409
00:22:39,360 --> 00:22:45,020
So our embedding table has, for each character, a 10-dimensional vector that we are trying

410
00:22:45,020 --> 00:22:46,020
to learn.

411
00:22:46,020 --> 00:22:47,300
And so what the embedding layer does here...

412
00:22:47,300 --> 00:22:52,360
What the layer does here is it blocks out the embedding vector for each one of these

413
00:22:52,360 --> 00:22:58,760
integers and organizes it all in a four by eight by 10 tensor now.

414
00:22:58,760 --> 00:23:03,160
So all of these integers are translated into 10-dimensional vectors inside this three-dimensional

415
00:23:03,160 --> 00:23:04,980
tensor now.

416
00:23:04,980 --> 00:23:09,380
Now passing that through the flatten layer, as you recall, what this does is it views

417
00:23:09,380 --> 00:23:12,520
this tensor as just a four by 80 tensor.

418
00:23:12,520 --> 00:23:16,800
And what that effectively does is that all these 10-dimensional embeddings for all these

419
00:23:16,800 --> 00:23:21,800
eight characters just end up being stretched out into a long row.

420
00:23:21,800 --> 00:23:24,820
And that looks kind of like a concatenation operation, basically.

421
00:23:24,820 --> 00:23:29,100
So by viewing the tensor differently, we now have a four by 80.

422
00:23:29,100 --> 00:23:36,380
And inside this 80, it's all the 10-dimensional vectors just concatenated next to each other.

423
00:23:36,380 --> 00:23:43,480
And the linear layer, of course, takes 80 and creates 200 channels just via matrix multiplication.

424
00:23:43,480 --> 00:23:44,480
So so far, so good.

425
00:23:44,480 --> 00:23:45,720
Now I'd like to show you something surprising.

426
00:23:45,720 --> 00:23:46,720
Let's see.

427
00:23:46,800 --> 00:23:52,740
Let's look at the insides of the linear layer and remind ourselves how it works.

428
00:23:52,740 --> 00:23:57,740
The linear layer here in a forward pass takes the input x, multiplies it with a weight,

429
00:23:57,740 --> 00:23:59,760
and then optionally adds bias.

430
00:23:59,760 --> 00:24:03,060
And the weight here is two-dimensional, as defined here, and the bias is one-dimensional

431
00:24:03,060 --> 00:24:04,620
here.

432
00:24:04,620 --> 00:24:08,680
So effectively, in terms of the shapes involved, what's happening inside this linear layer

433
00:24:08,680 --> 00:24:11,240
looks like this right now.

434
00:24:11,240 --> 00:24:15,800
And I'm using random numbers here, but I'm just illustrating the shapes and what happens.

435
00:24:15,800 --> 00:24:20,620
Basically, a four by 80 input comes into the linear layer, gets multiplied by this

436
00:24:20,620 --> 00:24:24,840
80 by 200 weight matrix inside, and then there's a plus 200 bias.

437
00:24:24,840 --> 00:24:28,780
And the shape of the whole thing that comes out of the linear layer is four by 200, as

438
00:24:28,780 --> 00:24:30,780
we see here.

439
00:24:30,780 --> 00:24:36,180
Now notice here, by the way, that this here will create a four by 200 tensor, and then

440
00:24:36,180 --> 00:24:42,360
plus 200, there's a broadcasting happening here, but four by 200 broadcasts with 200,

441
00:24:42,360 --> 00:24:43,680
so everything works here.

442
00:24:43,680 --> 00:24:44,640
So now the surprising thing that I want to show you is this.

443
00:24:44,640 --> 00:24:45,640
I'm going to show you how this works.

444
00:24:45,640 --> 00:24:49,460
One thing that I'd like to show you that you may not expect is that this input here

445
00:24:49,460 --> 00:24:53,640
that is being multiplied doesn't actually have to be two-dimensional.

446
00:24:53,640 --> 00:24:58,180
This matrix multiply operator in PyTorch is quite powerful, and in fact, you can actually

447
00:24:58,180 --> 00:25:02,020
pass in higher dimensional arrays or tensors, and everything works fine.

448
00:25:02,020 --> 00:25:05,960
So for example, this could be four by five by 80, and the result in that case will become

449
00:25:05,960 --> 00:25:08,360
four by five by 200.

450
00:25:08,360 --> 00:25:11,720
You can add as many dimensions as you like on the left here.

451
00:25:11,720 --> 00:25:15,640
And so effectively, what's happening is that the matrix multiplication only works on a

452
00:25:15,640 --> 00:25:19,180
matrix multiplication on the last dimension, and the dimensions before it in the input

453
00:25:19,180 --> 00:25:24,780
tensor are left unchanged.

454
00:25:24,780 --> 00:25:31,740
So basically, these dimensions on the left are all treated as just a batch dimension.

455
00:25:31,740 --> 00:25:36,580
So we can have multiple batch dimensions, and then in parallel over all those dimensions,

456
00:25:36,580 --> 00:25:39,620
we are doing the matrix multiplication on the last dimension.

457
00:25:39,620 --> 00:25:44,520
So this is quite convenient because we can use that in our network now.

458
00:25:44,520 --> 00:25:45,520
Because remember that.

459
00:25:45,520 --> 00:25:49,380
We have these eight characters coming in.

460
00:25:49,380 --> 00:25:55,120
And we don't want to now flatten all of it out into a large eight-dimensional vector

461
00:25:55,120 --> 00:26:01,840
because we don't want to matrix multiply 80 into a weight matrix multiply immediately.

462
00:26:01,840 --> 00:26:07,140
Instead, we want to group these like this.

463
00:26:07,140 --> 00:26:11,380
So every consecutive two elements, one and two and three and four and five and six and

464
00:26:11,380 --> 00:26:14,900
seven and eight, all of these should be now basically flattened.

465
00:26:14,900 --> 00:26:15,520
Okay.

466
00:26:15,520 --> 00:26:16,600
So we can.

467
00:26:16,600 --> 00:26:18,380
End out and multiply by weight matrix.

468
00:26:18,380 --> 00:26:22,240
But all of these four groups here, we'd like to process in parallel.

469
00:26:22,240 --> 00:26:26,080
So it's kind of like a batch dimension that we can introduce.

470
00:26:26,080 --> 00:26:33,820
And then we can, in parallel, basically process all of these bigram groups in the four batch

471
00:26:33,820 --> 00:26:40,020
dimensions of an individual example, and also over the actual batch dimension of the four

472
00:26:40,020 --> 00:26:41,840
examples in our example here.

473
00:26:41,840 --> 00:26:43,380
So let's see how that works.

474
00:26:43,380 --> 00:26:44,880
Effectively, what we want is.

475
00:26:44,880 --> 00:26:45,360
Right now.

476
00:26:45,360 --> 00:26:51,760
Now we take a 4 by 80 and multiply it by 80 by 200 in the linear layer.

477
00:26:51,840 --> 00:26:52,500
This is what happens.

478
00:26:53,560 --> 00:26:58,660
But instead what we want is we don't want 80 characters or 80 numbers to come in.

479
00:26:59,020 --> 00:27:01,840
We only want two characters to come in on the very first layer,

480
00:27:02,020 --> 00:27:03,700
and those two characters should be fused.

481
00:27:04,820 --> 00:27:08,320
So in other words, we just want 20 to come in, right?

482
00:27:09,020 --> 00:27:10,280
20 numbers would come in.

483
00:27:10,900 --> 00:27:14,140
And here we don't want a 4 by 80 to feed into the linear layer.

484
00:27:14,140 --> 00:27:17,220
We actually want these groups of 2 to feed in.

485
00:27:17,620 --> 00:27:21,700
So instead of 4 by 80, we want this to be a 4 by 4 by 20.

486
00:27:23,260 --> 00:27:28,700
So these are the four groups of 2, and each one of them is a 10-dimensional vector.

487
00:27:29,420 --> 00:27:32,380
So what we want now is we need to change the flatten layer

488
00:27:32,380 --> 00:27:36,400
so it doesn't output a 4 by 80, but it outputs a 4 by 4 by 20,

489
00:27:37,000 --> 00:27:44,120
where basically every two consecutive characters are packed in

490
00:27:44,120 --> 00:27:45,400
on the very last dimension.

491
00:27:45,980 --> 00:27:48,400
And then these four is the first batch dimension,

492
00:27:48,880 --> 00:27:51,020
and this four is the second batch dimension,

493
00:27:51,400 --> 00:27:54,380
referring to the four groups inside every one of these examples.

494
00:27:55,380 --> 00:27:57,580
And then this will just multiply like this.

495
00:27:57,700 --> 00:27:59,300
So this is what we want to get to.

496
00:27:59,880 --> 00:28:01,360
So we're going to have to change the linear layer

497
00:28:01,360 --> 00:28:03,280
in terms of how many inputs it expects.

498
00:28:03,420 --> 00:28:05,440
It shouldn't expect 80.

499
00:28:05,520 --> 00:28:06,720
It should just expect 20 numbers.

500
00:28:07,040 --> 00:28:08,700
And we have to change our flatten layer

501
00:28:08,700 --> 00:28:11,880
so it doesn't just fully flatten out this entire example.

502
00:28:12,280 --> 00:28:14,100
It needs to create a 4 by 4.

503
00:28:14,100 --> 00:28:16,580
It needs to create a 4 by 20 instead of a 4 by 80.

504
00:28:17,060 --> 00:28:18,500
So let's see how this could be implemented.

505
00:28:19,240 --> 00:28:22,740
Basically right now we have an input that is a 4 by 8 by 10

506
00:28:22,740 --> 00:28:24,680
that feeds into the flatten layer,

507
00:28:25,100 --> 00:28:28,140
and currently the flatten layer just stretches it out.

508
00:28:28,500 --> 00:28:30,420
So if you remember the implementation of flatten,

509
00:28:31,260 --> 00:28:35,220
it takes our x and it just views it as whatever the batch dimension is,

510
00:28:35,320 --> 00:28:36,100
and then negative 1.

511
00:28:36,940 --> 00:28:42,000
So effectively what it does right now is it does e.view of 4, negative 1,

512
00:28:42,000 --> 00:28:44,080
and the shape of this, of course, is 4 by 80.

513
00:28:44,100 --> 00:28:47,500
So that's what currently happens,

514
00:28:47,660 --> 00:28:50,260
and we instead want this to be a 4 by 4 by 20,

515
00:28:50,500 --> 00:28:53,280
where these consecutive 10-dimensional vectors get concatenated.

516
00:28:54,180 --> 00:28:58,500
So you know how in Python you can take a list of range of 10?

517
00:28:59,780 --> 00:29:02,280
So we have numbers from 0 to 9,

518
00:29:02,640 --> 00:29:05,740
and we can index like this to get all the even parts,

519
00:29:06,280 --> 00:29:08,740
and we can also index like starting at 1

520
00:29:08,740 --> 00:29:11,720
and going in steps of 2 to get all the odd parts.

521
00:29:13,060 --> 00:29:14,080
So one way to implement this is to take a list of range of 10,

522
00:29:14,080 --> 00:29:16,380
and one way to implement this, it would be as follows.

523
00:29:16,380 --> 00:29:21,200
We can take e, and we can index into it for all the batch elements,

524
00:29:21,720 --> 00:29:24,440
and then just even elements in this dimension,

525
00:29:25,020 --> 00:29:28,100
so at indexes 0, 2, 4, and 8,

526
00:29:28,840 --> 00:29:32,200
and then all the parts here from this last dimension,

527
00:29:33,480 --> 00:29:36,800
and this gives us the even characters,

528
00:29:37,300 --> 00:29:41,180
and then here this gives us all the odd characters.

529
00:29:41,500 --> 00:29:43,800
And basically what we want to do is we want to make sure

530
00:29:43,800 --> 00:29:46,140
that these get concatenated in PyTorch,

531
00:29:46,380 --> 00:29:49,420
and then we want to concatenate these two tensors

532
00:29:49,420 --> 00:29:51,200
along the second dimension.

533
00:29:53,020 --> 00:29:56,280
So this and the shape of it would be 4 by 4 by 20.

534
00:29:56,440 --> 00:29:58,160
This is definitely the result we want.

535
00:29:58,360 --> 00:30:02,080
We are explicitly grabbing the even parts and the odd parts,

536
00:30:02,220 --> 00:30:05,220
and we're arranging those 4 by 4 by 10

537
00:30:05,220 --> 00:30:07,240
right next to each other and concatenate.

538
00:30:08,240 --> 00:30:11,140
So this works, but it turns out that what also works

539
00:30:11,140 --> 00:30:13,700
is you can simply use a view again,

540
00:30:13,800 --> 00:30:16,040
and just request the right shape.

541
00:30:16,380 --> 00:30:18,020
And it just so happens that in this case,

542
00:30:18,540 --> 00:30:21,480
those vectors will again end up being arranged

543
00:30:21,480 --> 00:30:22,640
exactly the way we want.

544
00:30:23,260 --> 00:30:24,640
So in particular, if we take e,

545
00:30:24,760 --> 00:30:26,920
and we just view it as a 4 by 4 by 20,

546
00:30:27,060 --> 00:30:27,900
which is what we want,

547
00:30:28,600 --> 00:30:30,960
we can check that this is exactly equal to,

548
00:30:31,680 --> 00:30:35,020
let me call this, this is the explicit concatenation, I suppose.

549
00:30:36,760 --> 00:30:39,540
So explicit dot shape is 4 by 4 by 20.

550
00:30:40,100 --> 00:30:41,820
If you just view it as 4 by 4 by 20,

551
00:30:41,820 --> 00:30:43,120
you can check that,

552
00:30:43,120 --> 00:30:45,120
when you compare it to explicit,

553
00:30:46,220 --> 00:30:48,580
you get a big, this is element-wise operation,

554
00:30:48,840 --> 00:30:50,340
so making sure that all of them are true,

555
00:30:51,140 --> 00:30:51,720
values to true.

556
00:30:52,620 --> 00:30:54,160
So basically, long story short,

557
00:30:54,240 --> 00:30:56,880
we don't need to make an explicit call to concatenate, etc.

558
00:30:57,200 --> 00:31:01,220
We can simply take this input tensor to flatten,

559
00:31:01,680 --> 00:31:04,140
and we can just view it in whatever way we want.

560
00:31:05,020 --> 00:31:06,400
And in particular,

561
00:31:06,520 --> 00:31:08,740
we don't want to stretch things out with negative 1.

562
00:31:08,980 --> 00:31:11,100
We want to actually create a three-dimensional array,

563
00:31:11,400 --> 00:31:12,800
and depending on how many,

564
00:31:12,800 --> 00:31:14,800
vectors that are consecutive,

565
00:31:15,280 --> 00:31:17,700
we want to fuse,

566
00:31:17,920 --> 00:31:18,920
like for example, 2,

567
00:31:19,580 --> 00:31:21,960
then we can just simply ask for this dimension to be 20,

568
00:31:22,520 --> 00:31:25,320
and use a negative 1 here,

569
00:31:25,660 --> 00:31:27,940
and PyTorch will figure out how many groups it needs to pack

570
00:31:28,020 --> 00:31:29,940
into this additional batch dimension.

571
00:31:30,780 --> 00:31:33,160
So let's now go into flatten and implement this.

572
00:31:33,420 --> 00:31:35,040
Okay, so I scrolled up here to flatten,

573
00:31:35,580 --> 00:31:37,900
and what we'd like to do is we'd like to change it now.

574
00:31:38,140 --> 00:31:39,360
So let me create a constructor,

575
00:31:39,680 --> 00:31:42,200
and take the number of elements that are consecutive,

576
00:31:42,300 --> 00:31:42,800
that we would like to use,

577
00:31:42,880 --> 00:31:44,800
and then we'd like to concatenate now

578
00:31:44,880 --> 00:31:46,880
in the last dimension of the output.

579
00:31:47,400 --> 00:31:49,400
So here we're just going to remember,

580
00:31:49,480 --> 00:31:50,480
self.n equals n.

581
00:31:51,120 --> 00:31:53,120
And then I want to be careful here,

582
00:31:53,200 --> 00:31:56,200
because PyTorch actually has a torch.flatten,

583
00:31:56,280 --> 00:31:58,280
and its keyword arguments are different,

584
00:31:58,360 --> 00:32:00,360
and they kind of like function differently.

585
00:32:00,440 --> 00:32:03,440
So our flatten is going to start to depart from PyTorch flatten.

586
00:32:03,520 --> 00:32:06,200
So let me call it flatten consecutive,

587
00:32:06,280 --> 00:32:07,280
or something like that,

588
00:32:07,360 --> 00:32:10,160
just to make sure that our APIs are about equal.

589
00:32:10,520 --> 00:32:11,520
So this,

590
00:32:11,520 --> 00:32:15,000
basically flattens only some n consecutive elements,

591
00:32:15,080 --> 00:32:17,080
and puts them into the last dimension.

592
00:32:17,720 --> 00:32:21,400
Now here, the shape of x is b by t by c.

593
00:32:21,480 --> 00:32:25,480
So let me pop those out into variables.

594
00:32:25,560 --> 00:32:27,680
And recall that in our example down below,

595
00:32:27,760 --> 00:32:30,760
b was 4, t was 8, and c was 10.

596
00:32:33,680 --> 00:32:37,680
Now, instead of doing x.view of b by negative 1,

597
00:32:39,600 --> 00:32:41,480
right, this is what we had before.

598
00:32:41,560 --> 00:32:48,560
We want this to be b by negative 1 by,

599
00:32:48,640 --> 00:32:52,640
and basically here, we want c times n.

600
00:32:52,720 --> 00:32:55,720
That's how many consecutive elements we want.

601
00:32:55,800 --> 00:32:58,040
And here, instead of negative 1,

602
00:32:58,120 --> 00:32:59,920
I don't super love the use of negative 1,

603
00:33:00,000 --> 00:33:01,920
because I like to be very explicit,

604
00:33:02,000 --> 00:33:03,000
so that you get error messages

605
00:33:03,080 --> 00:33:05,320
when things don't go according to your expectation.

606
00:33:05,400 --> 00:33:06,880
So what do we expect here?

607
00:33:06,960 --> 00:33:10,360
We expect this to become t divide n,

608
00:33:10,440 --> 00:33:11,480
using integer division here.

609
00:33:11,560 --> 00:33:13,680
So that's what I expect to happen.

610
00:33:13,760 --> 00:33:16,120
And then one more thing I want to do here is,

611
00:33:16,200 --> 00:33:18,600
remember previously, all the way in the beginning,

612
00:33:18,680 --> 00:33:22,560
n was 3, and basically we're concatenating

613
00:33:22,640 --> 00:33:25,360
all the three characters that existed there.

614
00:33:25,440 --> 00:33:29,040
So we basically concatenated everything.

615
00:33:29,120 --> 00:33:30,960
And so sometimes that can create

616
00:33:31,040 --> 00:33:32,680
a spurious dimension of 1 here.

617
00:33:32,760 --> 00:33:36,480
So if it is the case that x.shapeAt1 is 1,

618
00:33:36,560 --> 00:33:38,520
then it's kind of like a spurious dimension.

619
00:33:38,600 --> 00:33:40,440
So we don't want to return a 3,

620
00:33:40,480 --> 00:33:41,440
so we don't want to return a 3,

621
00:33:41,480 --> 00:33:44,600
so we don't want to return a 3-dimensional tensor with a 1 here.

622
00:33:44,680 --> 00:33:46,480
We just want to return a 2-dimensional tensor

623
00:33:46,560 --> 00:33:48,640
exactly as we did before.

624
00:33:48,720 --> 00:33:53,560
So in this case, basically, we will just say x equals x.squeeze,

625
00:33:53,640 --> 00:33:57,560
that is a PyTorch function.

626
00:33:57,640 --> 00:34:01,920
And squeeze takes a dimension that it either squeezes out

627
00:34:02,000 --> 00:34:04,720
all the dimensions of a tensor that are 1,

628
00:34:04,800 --> 00:34:07,760
or you can specify the exact dimension

629
00:34:07,840 --> 00:34:09,360
that you want to be squeezed.

630
00:34:09,440 --> 00:34:11,400
And again, I like to be as explicit as possible,

631
00:34:11,480 --> 00:34:15,480
always, so I expect to squeeze out the first dimension only

632
00:34:15,560 --> 00:34:18,880
of this tensor, this 3-dimensional tensor.

633
00:34:18,960 --> 00:34:20,400
And if this dimension here is 1,

634
00:34:20,480 --> 00:34:24,120
then I just want to return b by c times n.

635
00:34:24,200 --> 00:34:29,120
And so self.out will be x, and then we return self.out.

636
00:34:29,200 --> 00:34:31,000
So that's the candidate implementation.

637
00:34:31,080 --> 00:34:34,840
And of course, this should be self.in instead of just n.

638
00:34:34,920 --> 00:34:40,520
So let's run, and let's come here now and take it for a spin.

639
00:34:40,560 --> 00:34:46,080
So flattened consecutive, and in the beginning,

640
00:34:46,160 --> 00:34:47,640
let's just use 8.

641
00:34:47,720 --> 00:34:50,120
So this should recover the previous behavior.

642
00:34:50,200 --> 00:34:55,680
So flattened consecutive of 8, which is the current block size,

643
00:34:55,760 --> 00:34:59,320
we can do this, that should recover the previous behavior.

644
00:34:59,400 --> 00:35:02,600
So we should be able to run the model.

645
00:35:02,680 --> 00:35:06,920
And here we can inspect, I have a little code snippet here,

646
00:35:07,000 --> 00:35:10,480
where I iterate over all the layers, I print the name,

647
00:35:10,560 --> 00:35:14,680
of this class, and the shape.

648
00:35:14,760 --> 00:35:17,720
And so we see the shapes as we expect them

649
00:35:17,800 --> 00:35:20,600
after every single layer in its output.

650
00:35:20,680 --> 00:35:24,640
So now let's try to restructure it using our flattened consecutive

651
00:35:24,720 --> 00:35:26,600
and do it hierarchically.

652
00:35:26,680 --> 00:35:29,520
So in particular, we want to flatten consecutive,

653
00:35:29,600 --> 00:35:33,000
not just block size, but just 2.

654
00:35:33,080 --> 00:35:35,280
And then we want to process this with linear.

655
00:35:35,360 --> 00:35:37,960
Now, the number of inputs to this linear will not be

656
00:35:38,040 --> 00:35:40,400
n embed times block size, it will now only be

657
00:35:40,440 --> 00:35:44,240
n embed times 2, 20.

658
00:35:44,320 --> 00:35:46,240
This goes through the first layer.

659
00:35:46,320 --> 00:35:49,800
And now we can, in principle, just copy paste this.

660
00:35:49,880 --> 00:35:54,000
Now, the next linear layer should expect n hidden times 2.

661
00:35:54,080 --> 00:36:01,560
And the last piece of it should expect n hidden times 2 again.

662
00:36:01,640 --> 00:36:05,400
So this is sort of like the naive version of it.

663
00:36:05,480 --> 00:36:09,040
So running this, we now have a much, much bigger model.

664
00:36:09,120 --> 00:36:10,360
And we should be able to basically just

665
00:36:10,440 --> 00:36:13,640
just forward the model.

666
00:36:13,720 --> 00:36:17,720
And now we can inspect the numbers in between.

667
00:36:17,800 --> 00:36:23,240
So 4x8x20 was flattened consecutively into 4x4x20.

668
00:36:23,320 --> 00:36:26,640
This was projected into 4x4x200.

669
00:36:26,720 --> 00:36:30,240
And then BatchNorm just worked out of the box.

670
00:36:30,320 --> 00:36:32,400
We have to verify that BatchNorm does the correct thing,

671
00:36:32,480 --> 00:36:33,960
even though it takes a three-dimensional input

672
00:36:34,040 --> 00:36:36,480
instead of two-dimensional input.

673
00:36:36,560 --> 00:36:38,800
Then we have 10H, which is element-wise.

674
00:36:38,880 --> 00:36:40,280
Then we crushed it again.

675
00:36:40,320 --> 00:36:44,400
So we flattened consecutively and ended up with a 4x2x400 now.

676
00:36:44,480 --> 00:36:48,360
Then linear brought it back down to 200, BatchNorm 10H.

677
00:36:48,440 --> 00:36:50,800
And lastly, we get a 4x400.

678
00:36:50,880 --> 00:36:53,840
And we see that the flattened consecutive for the last flatten here,

679
00:36:53,920 --> 00:36:56,800
it squeezed out that dimension of 1.

680
00:36:56,880 --> 00:36:58,920
So we only ended up with 4x400.

681
00:36:59,000 --> 00:37:04,440
And then linear BatchNorm 10H and the last linear layer to get our logits.

682
00:37:04,520 --> 00:37:07,600
And so the logits end up in the same shape as they were before.

683
00:37:07,680 --> 00:37:09,360
But now we actually have a nice three-layer,

684
00:37:09,360 --> 00:37:11,400
neural net.

685
00:37:11,480 --> 00:37:14,600
And it basically corresponds to, whoops, sorry.

686
00:37:14,680 --> 00:37:17,600
It basically corresponds exactly to this network now,

687
00:37:17,680 --> 00:37:21,000
except only this piece here because we only have three layers.

688
00:37:21,080 --> 00:37:24,760
Whereas here in this example, there's four layers

689
00:37:24,840 --> 00:37:28,480
with a total receptive field size of 16 characters

690
00:37:28,560 --> 00:37:30,360
instead of just eight characters.

691
00:37:30,440 --> 00:37:32,640
So the block size here is 16.

692
00:37:32,720 --> 00:37:36,960
So this piece of it is basically implemented here.

693
00:37:37,040 --> 00:37:39,320
Now we just have to kind of figure out some good,

694
00:37:39,400 --> 00:37:41,360
channel numbers to use here.

695
00:37:41,440 --> 00:37:45,320
Now in particular, I changed the number of hidden units to be 68

696
00:37:45,400 --> 00:37:47,560
in this architecture because when I use 68,

697
00:37:47,640 --> 00:37:50,200
the number of parameters comes out to be 22,000.

698
00:37:50,280 --> 00:37:52,720
So that's exactly the same that we had before.

699
00:37:52,800 --> 00:37:55,600
And we have the same amount of capacity at this neural net

700
00:37:55,680 --> 00:37:57,360
in terms of the number of parameters.

701
00:37:57,440 --> 00:37:59,560
But the question is whether we are utilizing those parameters

702
00:37:59,640 --> 00:38:01,480
in a more efficient architecture.

703
00:38:01,560 --> 00:38:05,640
So what I did then is I got rid of a lot of the debugging cells here

704
00:38:05,720 --> 00:38:07,360
and I rerun the optimization.

705
00:38:07,440 --> 00:38:09,080
And scrolling down to the result,

706
00:38:09,120 --> 00:38:12,760
we see that we get the identical performance roughly.

707
00:38:12,840 --> 00:38:17,680
So our validation loss now is 2.029 and previously it was 2.027.

708
00:38:17,760 --> 00:38:19,640
So controlling for the number of parameters,

709
00:38:19,720 --> 00:38:23,480
changing from the flat to hierarchical is not giving us anything yet.

710
00:38:23,560 --> 00:38:26,600
That said, there are two things to point out.

711
00:38:26,680 --> 00:38:30,080
Number one, we didn't really torture the architecture here very much.

712
00:38:30,160 --> 00:38:31,480
This is just my first guess.

713
00:38:31,560 --> 00:38:34,000
And there's a bunch of hyperparameter search that we could do

714
00:38:34,080 --> 00:38:38,920
in terms of how we allocate our budget of parameters to what layers.

715
00:38:39,080 --> 00:38:43,720
Number two, we still may have a bug inside the BatchNorm1D layer.

716
00:38:43,800 --> 00:38:50,640
So let's take a look at that because it runs but doesn't do the right thing.

717
00:38:50,720 --> 00:38:54,440
So I pulled up the layer inspector sort of that we have here

718
00:38:54,520 --> 00:38:56,200
and printed out the shape along the way.

719
00:38:56,280 --> 00:38:58,960
And currently it looks like the BatchNorm is receiving an input

720
00:38:59,040 --> 00:39:02,480
that is 32 by 4 by 68, right?

721
00:39:02,560 --> 00:39:05,120
And here on the right, I have the current implementation of BatchNorm

722
00:39:05,200 --> 00:39:06,520
that we have right now.

723
00:39:06,600 --> 00:39:08,920
Now, this BatchNorm assumed, in the way

724
00:39:08,920 --> 00:39:12,040
we wrote it and at the time, that X is two-dimensional.

725
00:39:12,120 --> 00:39:16,040
So it was N by D, where N was the batch size.

726
00:39:16,120 --> 00:39:20,560
So that's why we only reduced the mean and the variance over the zeroth dimension.

727
00:39:20,640 --> 00:39:23,080
But now X will basically become three-dimensional.

728
00:39:23,160 --> 00:39:25,000
So what's happening inside the BatchNorm layer right now?

729
00:39:25,080 --> 00:39:28,200
And how come it's working at all and not giving any errors?

730
00:39:28,280 --> 00:39:31,560
The reason for that is basically because everything broadcasts properly,

731
00:39:31,640 --> 00:39:35,560
but the BatchNorm is not doing what we want it to do.

732
00:39:35,640 --> 00:39:38,200
So in particular, let's basically think through what's happening

733
00:39:38,280 --> 00:39:38,520
inside the BatchNorm.

734
00:39:38,920 --> 00:39:43,760
I'm looking at what's happening here.

735
00:39:43,840 --> 00:39:45,440
I have the code here.

736
00:39:45,520 --> 00:39:49,600
So we're receiving an input of 32 by 4 by 68.

737
00:39:49,680 --> 00:39:52,720
And then we are doing here, X dot mean.

738
00:39:52,800 --> 00:39:54,600
Here I have E instead of X.

739
00:39:54,680 --> 00:39:57,160
But we're doing the mean over zero.

740
00:39:57,240 --> 00:39:59,680
And that's actually giving us 1 by 4 by 68.

741
00:39:59,760 --> 00:40:02,640
So we're doing the mean only over the very first dimension.

742
00:40:02,720 --> 00:40:07,880
And it's giving us a mean and a variance that still maintain this dimension here.

743
00:40:07,880 --> 00:40:12,160
So these means are only taken over 32 numbers in the first dimension.

744
00:40:12,240 --> 00:40:17,000
And then when we perform this, everything broadcasts correctly still.

745
00:40:17,080 --> 00:40:26,200
But basically what ends up happening is when we also look at the running mean,

746
00:40:26,280 --> 00:40:26,880
the shape of it.

747
00:40:26,960 --> 00:40:28,480
So I'm looking at the model that layers the three,

748
00:40:28,560 --> 00:40:29,920
which is the first BatchNorm layer,

749
00:40:30,000 --> 00:40:34,120
and then looking at whatever the running mean became and its shape.

750
00:40:34,200 --> 00:40:37,640
The shape of this running mean now is 1 by 4 by 68.

751
00:40:37,880 --> 00:40:44,480
Instead of it being just size of dimension, because we have 68 channels,

752
00:40:44,560 --> 00:40:48,320
we expect to have 68 means and variances that we're maintaining.

753
00:40:48,400 --> 00:40:50,960
But actually we have an array of 4 by 68.

754
00:40:51,040 --> 00:40:55,120
And so basically what this is telling us is this BatchNorm is only...

755
00:40:55,200 --> 00:41:05,960
This BatchNorm is currently working in parallel over 4 times 68 instead of just 68 channels.

756
00:41:06,040 --> 00:41:07,200
So basically we are maintaining this.

757
00:41:07,880 --> 00:41:13,680
We are maintaining statistics for every one of these four positions individually and independently.

758
00:41:13,760 --> 00:41:17,880
And instead what we want to do is we want to treat this 4 as a Batch dimension,

759
00:41:17,960 --> 00:41:19,840
just like the 0th dimension.

760
00:41:19,920 --> 00:41:22,800
So as far as the BatchNorm is concerned,

761
00:41:22,880 --> 00:41:24,440
it doesn't want to average...

762
00:41:24,520 --> 00:41:26,440
We don't want to average over 32 numbers.

763
00:41:26,520 --> 00:41:32,720
We want to now average over 32 times 4 numbers for every single one of these 68 channels.

764
00:41:32,800 --> 00:41:37,080
And so let me now remove this.

765
00:41:37,080 --> 00:41:42,320
It turns out that when you look at the documentation of Torch.mean...

766
00:41:42,400 --> 00:41:49,600
So let's go to Torch.mean.

767
00:41:49,680 --> 00:41:53,200
In one of its signatures, when we specify the dimension,

768
00:41:53,280 --> 00:41:55,200
we see that the dimension here is not just...

769
00:41:55,280 --> 00:41:58,400
It can be int or it can also be a tuple of ints.

770
00:41:58,480 --> 00:42:01,880
So we can reduce over multiple integers at the same time,

771
00:42:01,960 --> 00:42:03,760
over multiple dimensions at the same time.

772
00:42:03,840 --> 00:42:06,840
So instead of just reducing over 0, we can pass in a tuple,

773
00:42:06,840 --> 00:42:10,400
0, 1, and here 0, 1 as well.

774
00:42:10,480 --> 00:42:13,840
And then what's going to happen is the output, of course, is going to be the same.

775
00:42:13,920 --> 00:42:17,240
But now what's going to happen is because we reduce over 0 and 1,

776
00:42:17,320 --> 00:42:22,440
if we look at inmean.shape, we see that now we've reduced.

777
00:42:22,520 --> 00:42:26,840
We took the mean over both the 0th and the first dimension.

778
00:42:26,920 --> 00:42:30,920
So we're just getting 68 numbers and a bunch of spurious dimensions here.

779
00:42:31,000 --> 00:42:33,640
So now this becomes 1 by 1 by 68.

780
00:42:33,720 --> 00:42:36,160
And the running mean and the running variance,

781
00:42:36,160 --> 00:42:38,800
analogously, will become 1 by 1 by 68.

782
00:42:38,880 --> 00:42:41,040
So even though there are the spurious dimensions,

783
00:42:41,120 --> 00:42:46,200
the correct thing will happen in that we are only maintaining means and variances

784
00:42:46,280 --> 00:42:49,640
for 68 channels.

785
00:42:49,720 --> 00:42:54,200
And we're now calculating the mean and variance across 32 times 4 dimensions.

786
00:42:54,280 --> 00:42:56,040
So that's exactly what we want.

787
00:42:56,120 --> 00:42:59,720
And let's change the implementation of BatchNorm1D that we have

788
00:42:59,800 --> 00:43:03,400
so that it can take in two-dimensional or three-dimensional inputs

789
00:43:03,480 --> 00:43:05,240
and perform accordingly.

790
00:43:05,320 --> 00:43:05,960
So at the end of the day,

791
00:43:05,960 --> 00:43:08,000
the fix is relatively straightforward.

792
00:43:08,080 --> 00:43:12,240
Basically, the dimension we want to reduce over is either 0

793
00:43:12,320 --> 00:43:15,400
or the tuple 0 and 1, depending on the dimensionality of x.

794
00:43:15,480 --> 00:43:19,280
So if x.ndim is 2, so it's a two-dimensional tensor,

795
00:43:19,360 --> 00:43:22,520
then the dimension we want to reduce over is just the integer 0.

796
00:43:22,600 --> 00:43:25,840
And if x.ndim is 3, so it's a three-dimensional tensor,

797
00:43:25,920 --> 00:43:31,440
then the dims we're going to assume are 0 and 1 that we want to reduce over.

798
00:43:31,520 --> 00:43:33,880
And then here, we just pass in dim.

799
00:43:33,960 --> 00:43:35,680
And if the dimensionality of x is anything else,

800
00:43:35,680 --> 00:43:37,840
we're going to get an error, which is good.

801
00:43:37,920 --> 00:43:40,560
So that should be the fix.

802
00:43:40,640 --> 00:43:42,320
Now, I want to point out one more thing.

803
00:43:42,400 --> 00:43:45,720
We're actually departing from the API of PyTorch here a little bit,

804
00:43:45,800 --> 00:43:48,560
because when you come to BatchNorm1D in PyTorch,

805
00:43:48,640 --> 00:43:51,720
you can scroll down and you can see that the input to this layer

806
00:43:51,800 --> 00:43:54,680
can either be n by c, where n is the batch size

807
00:43:54,760 --> 00:43:56,840
and c is the number of features or channels,

808
00:43:56,920 --> 00:43:59,600
or it actually does accept three-dimensional inputs,

809
00:43:59,680 --> 00:44:02,720
but it expects it to be n by c by l,

810
00:44:02,800 --> 00:44:05,520
where l is, say, like the sequence length or something like that.

811
00:44:05,680 --> 00:44:11,040
So this is a problem because you see how c is nested here in the middle.

812
00:44:11,120 --> 00:44:14,000
And so when it gets three-dimensional inputs,

813
00:44:14,080 --> 00:44:19,120
this BatchNorm layer will reduce over 0 and 2 instead of 0 and 1.

814
00:44:19,200 --> 00:44:26,400
So basically, PyTorch BatchNorm1D layer assumes that c will always be the first dimension,

815
00:44:26,480 --> 00:44:30,240
whereas we assume here that c is the last dimension,

816
00:44:30,320 --> 00:44:32,560
and there are some number of batch dimensions beforehand.

817
00:44:32,640 --> 00:44:35,440
And so,

818
00:44:35,440 --> 00:44:37,440
it expects n by c or n by c by l.

819
00:44:37,520 --> 00:44:39,440
We expect n by c or n by l by c.

820
00:44:39,520 --> 00:44:41,520
And so, it's a deviation.

821
00:44:41,600 --> 00:44:43,600
I think it's okay.

822
00:44:43,680 --> 00:44:45,680
I prefer it this way, honestly,

823
00:44:45,760 --> 00:44:48,400
so this is the way that we will keep it for our purposes.

824
00:44:48,480 --> 00:44:51,200
So I redefined the layers, reinitialized the neural net,

825
00:44:51,280 --> 00:44:54,480
and did a single forward pass with a break just for one step.

826
00:44:54,560 --> 00:44:57,600
Looking at the shapes along the way, they're, of course, identical.

827
00:44:57,680 --> 00:44:59,280
All the shapes are the same,

828
00:44:59,360 --> 00:45:02,560
but the way we see that things are actually working as we want them to,

829
00:45:02,640 --> 00:45:04,880
is that we can actually do the same thing.

830
00:45:04,880 --> 00:45:06,880
So the way we see that things are actually working as we want them to now

831
00:45:06,960 --> 00:45:08,880
is that when we look at the BatchNorm layer,

832
00:45:08,960 --> 00:45:10,880
the running mean shape is now 1 by 1 by 68.

833
00:45:10,960 --> 00:45:14,800
So we're only maintaining 68 means for every one of our channels,

834
00:45:14,880 --> 00:45:18,800
and we're treating both the 0th and the first dimension as a batch dimension,

835
00:45:18,880 --> 00:45:20,800
which is exactly what we want.

836
00:45:20,880 --> 00:45:22,800
So let me retrain the neural net now.

837
00:45:22,880 --> 00:45:24,800
Okay, so I've retrained the neural net with the bug fix.

838
00:45:24,880 --> 00:45:26,800
We get a nice curve.

839
00:45:26,880 --> 00:45:28,800
And when we look at the validation performance,

840
00:45:28,880 --> 00:45:30,800
we do actually see a slight improvement.

841
00:45:30,880 --> 00:45:32,800
So it went from 2.029 to 2.022.

842
00:45:32,880 --> 00:45:34,800
So basically, the bug inside the BatchNorm was holding us back, like,

843
00:45:34,880 --> 00:45:36,800
a little bit, it looks like.

844
00:45:36,880 --> 00:45:38,800
And we are getting a tiny improvement now,

845
00:45:38,880 --> 00:45:42,800
but it's not clear if this is statistically significant.

846
00:45:42,880 --> 00:45:44,800
And the reason we slightly expect an improvement

847
00:45:44,880 --> 00:45:48,800
is because we're not maintaining so many different means and variances

848
00:45:48,880 --> 00:45:50,800
that are only estimated using 32 numbers, effectively.

849
00:45:50,880 --> 00:45:54,800
Now we are estimating them using 32 times 4 numbers.

850
00:45:54,880 --> 00:45:56,800
So you just have a lot more numbers

851
00:45:56,880 --> 00:45:58,800
that go into any one estimate of the mean and variance.

852
00:45:58,880 --> 00:46:02,800
And it allows things to be a bit more stable and less wiggly

853
00:46:02,880 --> 00:46:04,800
inside those estimates of the BatchNorm.

854
00:46:04,880 --> 00:46:06,800
So pretty nice.

855
00:46:06,880 --> 00:46:08,800
With this more general architecture in place,

856
00:46:08,880 --> 00:46:10,800
we are now set up to push the performance further

857
00:46:10,880 --> 00:46:12,800
by increasing the size of the network.

858
00:46:12,880 --> 00:46:14,800
So, for example,

859
00:46:14,880 --> 00:46:16,800
I've bumped up the number of embeddings to 24 instead of 10,

860
00:46:16,880 --> 00:46:18,800
and also increased the number of hidden units.

861
00:46:18,880 --> 00:46:20,800
But using the exact same architecture,

862
00:46:20,880 --> 00:46:22,800
we now have 76,000 parameters,

863
00:46:22,880 --> 00:46:24,800
and the training takes a lot longer,

864
00:46:24,880 --> 00:46:26,800
but we do get a nice curve.

865
00:46:26,880 --> 00:46:28,800
And then when you actually evaluate the performance,

866
00:46:28,880 --> 00:46:30,800
we are now getting validation performance of 1.993.

867
00:46:30,880 --> 00:46:32,800
So we've crossed over 1.993.

868
00:46:32,880 --> 00:46:34,800
So we've crossed over 1.993.

869
00:46:34,880 --> 00:46:36,800
We've crossed over the 2.0 sort of territory.

870
00:46:36,880 --> 00:46:38,800
And we're at about 1.99.

871
00:46:38,880 --> 00:46:40,800
But we are starting to have to wait quite a bit longer.

872
00:46:40,880 --> 00:46:42,800
But we are starting to have to wait quite a bit longer.

873
00:46:42,880 --> 00:46:44,800
And we're a little bit in the dark

874
00:46:44,880 --> 00:46:46,800
with respect to the correct setting of the hyperparameters here

875
00:46:46,880 --> 00:46:48,800
and the learning rates and so on,

876
00:46:48,880 --> 00:46:50,800
because the experiments are starting to take longer to train.

877
00:46:50,880 --> 00:46:52,800
And so we are missing sort of like an experimental harness

878
00:46:52,880 --> 00:46:54,800
on which we could run a number of experiments

879
00:46:54,880 --> 00:46:56,800
on which we could run a number of experiments

880
00:46:56,880 --> 00:46:58,800
and really tune this architecture very well.

881
00:46:58,880 --> 00:47:00,800
So I'd like to conclude now with a few notes.

882
00:47:00,880 --> 00:47:02,800
We basically improved our performance

883
00:47:02,880 --> 00:47:04,800
from a starting of 2.1

884
00:47:04,800 --> 00:47:06,720
to 2.9.

885
00:47:06,800 --> 00:47:08,720
But I don't want that to be the focus

886
00:47:08,800 --> 00:47:10,720
because honestly we're kind of in the dark.

887
00:47:10,800 --> 00:47:12,720
We have no experimental harness.

888
00:47:12,800 --> 00:47:14,720
We're just guessing and checking.

889
00:47:14,800 --> 00:47:16,720
And this whole thing is terrible.

890
00:47:16,800 --> 00:47:18,720
We're just looking at the training loss.

891
00:47:18,800 --> 00:47:20,720
Normally you want to look at both the training

892
00:47:20,800 --> 00:47:22,720
and the validation loss together.

893
00:47:22,800 --> 00:47:24,720
The whole thing looks different

894
00:47:24,800 --> 00:47:26,720
if you're actually trying to squeeze out numbers.

895
00:47:26,800 --> 00:47:28,720
That said, we did implement this architecture

896
00:47:28,800 --> 00:47:30,720
from the WaveNet paper.

897
00:47:30,800 --> 00:47:32,720
But we did not implement this specific forward pass of it

898
00:47:32,800 --> 00:47:34,720
where you have a more complicated

899
00:47:34,720 --> 00:47:36,640
structure that is this gated

900
00:47:36,720 --> 00:47:38,640
linear layer kind of.

901
00:47:38,720 --> 00:47:40,640
And there's residual connections and skip connections

902
00:47:40,720 --> 00:47:42,640
and so on. So we did not implement that.

903
00:47:42,720 --> 00:47:44,640
We just implemented this structure.

904
00:47:44,720 --> 00:47:46,640
I would like to briefly hint or preview

905
00:47:46,720 --> 00:47:48,640
how what we've done here relates

906
00:47:48,720 --> 00:47:50,640
to convolutional neural networks

907
00:47:50,720 --> 00:47:52,640
as used in the WaveNet paper.

908
00:47:52,720 --> 00:47:54,640
And basically the use of convolutions

909
00:47:54,720 --> 00:47:56,640
is strictly for efficiency.

910
00:47:56,720 --> 00:47:58,640
It doesn't actually change the model we've implemented.

911
00:47:58,720 --> 00:48:00,640
So here for example,

912
00:48:00,720 --> 00:48:02,640
let me look at a specific name

913
00:48:02,720 --> 00:48:04,640
to work with an example.

914
00:48:04,640 --> 00:48:06,560
So we have a name in our training set

915
00:48:06,640 --> 00:48:08,560
and it's D'Andre.

916
00:48:08,640 --> 00:48:10,560
And it has seven letters.

917
00:48:10,640 --> 00:48:12,560
So that is eight independent examples in our model.

918
00:48:12,640 --> 00:48:14,560
So all these rows here

919
00:48:14,640 --> 00:48:16,560
are independent examples of D'Andre.

920
00:48:16,640 --> 00:48:18,560
Now you can forward of course

921
00:48:18,640 --> 00:48:20,560
any one of these rows independently.

922
00:48:20,640 --> 00:48:22,560
So I can take my model

923
00:48:22,640 --> 00:48:24,560
and call it on

924
00:48:24,640 --> 00:48:26,560
any individual index.

925
00:48:26,640 --> 00:48:28,560
Notice by the way here

926
00:48:28,640 --> 00:48:30,560
I'm being a little bit tricky.

927
00:48:30,640 --> 00:48:32,560
The reason for this is that

928
00:48:32,560 --> 00:48:36,480
it's a one dimensional array of eight.

929
00:48:36,560 --> 00:48:38,480
So you can't actually call the model on it.

930
00:48:38,560 --> 00:48:40,480
You're going to get an error

931
00:48:40,560 --> 00:48:42,480
because there's no batch dimension.

932
00:48:42,560 --> 00:48:44,480
So when you do extra at

933
00:48:44,560 --> 00:48:46,480
a list of seven

934
00:48:46,560 --> 00:48:48,480
then the shape of this becomes one by eight.

935
00:48:48,560 --> 00:48:50,480
So I get an extra batch dimension

936
00:48:50,560 --> 00:48:52,480
of one and then we can forward the model.

937
00:48:52,560 --> 00:48:54,480
So

938
00:48:54,560 --> 00:48:56,480
that forwards a single example

939
00:48:56,560 --> 00:48:58,480
and you might imagine that you actually

940
00:48:58,560 --> 00:49:00,480
may want to forward all of these eight

941
00:49:00,560 --> 00:49:02,480
at the same time.

942
00:49:02,480 --> 00:49:04,400
So pre-allocating some memory

943
00:49:04,480 --> 00:49:06,400
and then doing a for loop

944
00:49:06,480 --> 00:49:08,400
eight times and forwarding all of those

945
00:49:08,480 --> 00:49:10,400
eight here will give us

946
00:49:10,480 --> 00:49:12,400
all the logits in all these different cases.

947
00:49:12,480 --> 00:49:14,400
Now for us with the model

948
00:49:14,480 --> 00:49:16,400
as we've implemented it right now

949
00:49:16,480 --> 00:49:18,400
this is eight independent calls to our model.

950
00:49:18,480 --> 00:49:20,400
But what convolutions allow you to do

951
00:49:20,480 --> 00:49:22,400
is it allow you to basically slide

952
00:49:22,480 --> 00:49:24,400
this model efficiently

953
00:49:24,480 --> 00:49:26,400
over the input sequence.

954
00:49:26,480 --> 00:49:28,400
And so this for loop can be done

955
00:49:28,480 --> 00:49:30,400
not outside in Python

956
00:49:30,480 --> 00:49:32,400
but inside of kernels in CUDA.

957
00:49:32,480 --> 00:49:34,400
And so this for loop gets hidden into the convolution.

958
00:49:34,480 --> 00:49:36,400
So the convolution

959
00:49:36,480 --> 00:49:38,400
basically you can think of it as

960
00:49:38,480 --> 00:49:40,400
it's a for loop applying a little linear

961
00:49:40,480 --> 00:49:42,400
filter over space

962
00:49:42,480 --> 00:49:44,400
of some input sequence.

963
00:49:44,480 --> 00:49:46,400
And in our case the space we're interested in is one dimensional

964
00:49:46,480 --> 00:49:48,400
and we're interested in sliding these filters

965
00:49:48,480 --> 00:49:50,400
over the input data.

966
00:49:50,480 --> 00:49:52,400
So this diagram

967
00:49:52,480 --> 00:49:54,400
actually is fairly good as well.

968
00:49:54,480 --> 00:49:56,400
Basically what we've done is

969
00:49:56,480 --> 00:49:58,400
here they are highlighting in black

970
00:49:58,480 --> 00:50:00,400
one single sort of like tree

971
00:50:00,480 --> 00:50:02,400
of this calculation.

972
00:50:02,400 --> 00:50:04,320
So just calculating the single output

973
00:50:04,400 --> 00:50:06,320
example here.

974
00:50:06,400 --> 00:50:08,320
And so this is basically

975
00:50:08,400 --> 00:50:10,320
what we've implemented here.

976
00:50:10,400 --> 00:50:12,320
We've implemented a single, this black structure

977
00:50:12,400 --> 00:50:14,320
we've implemented that

978
00:50:14,400 --> 00:50:16,320
and calculated a single output, like a single example.

979
00:50:16,400 --> 00:50:18,320
But what convolutions

980
00:50:18,400 --> 00:50:20,320
allow you to do is it allows you to take

981
00:50:20,400 --> 00:50:22,320
this black structure and

982
00:50:22,400 --> 00:50:24,320
kind of like slide it over the input sequence

983
00:50:24,400 --> 00:50:26,320
here and calculate

984
00:50:26,400 --> 00:50:28,320
all of these orange

985
00:50:28,400 --> 00:50:30,320
outputs at the same time.

986
00:50:30,400 --> 00:50:32,320
Or here that corresponds to calculating

987
00:50:32,320 --> 00:50:34,240
all of these outputs of

988
00:50:34,320 --> 00:50:36,240
at all the positions of

989
00:50:36,320 --> 00:50:38,240
deandre at the same time.

990
00:50:38,320 --> 00:50:40,240
And the reason that

991
00:50:40,320 --> 00:50:42,240
this is much more efficient is because

992
00:50:42,320 --> 00:50:44,240
number one, as I mentioned, the for loop

993
00:50:44,320 --> 00:50:46,240
is inside the CUDA kernels in the

994
00:50:46,320 --> 00:50:48,240
sliding. So that makes

995
00:50:48,320 --> 00:50:50,240
it efficient. But number two, notice

996
00:50:50,320 --> 00:50:52,240
the variable reuse here. For example

997
00:50:52,320 --> 00:50:54,240
if we look at this circle, this node here,

998
00:50:54,320 --> 00:50:56,240
this node here is the right child

999
00:50:56,320 --> 00:50:58,240
of this node, but it's also

1000
00:50:58,320 --> 00:51:00,240
the left child of the node here.

1001
00:51:00,320 --> 00:51:02,240
And so basically this

1002
00:51:02,240 --> 00:51:04,160
node and its value is used

1003
00:51:04,240 --> 00:51:06,160
twice. And so

1004
00:51:06,240 --> 00:51:08,160
right now, in this naive way,

1005
00:51:08,240 --> 00:51:10,160
we'd have to recalculate it.

1006
00:51:10,240 --> 00:51:12,160
But here we are allowed to reuse it.

1007
00:51:12,240 --> 00:51:14,160
So in the convolutional neural network,

1008
00:51:14,240 --> 00:51:16,160
you think of these linear layers that we have

1009
00:51:16,240 --> 00:51:18,160
up above as filters.

1010
00:51:18,240 --> 00:51:20,160
And we take these filters

1011
00:51:20,240 --> 00:51:22,160
and they're linear filters, and you slide them over

1012
00:51:22,240 --> 00:51:24,160
input sequence, and we calculate

1013
00:51:24,240 --> 00:51:26,160
the first layer, and then the second layer,

1014
00:51:26,240 --> 00:51:28,160
and then the third layer, and then the output layer

1015
00:51:28,240 --> 00:51:30,160
of the sandwich, and it's all done very

1016
00:51:30,240 --> 00:51:32,160
efficiently using these convolutions.

1017
00:51:32,240 --> 00:51:34,160
So we're going to cover that in a future video.

1018
00:51:34,240 --> 00:51:36,160
The second thing I hope you took away from this video

1019
00:51:36,240 --> 00:51:38,160
is you've seen me basically implement

1020
00:51:38,240 --> 00:51:40,160
all of these layer

1021
00:51:40,240 --> 00:51:42,160
Lego building blocks, or module

1022
00:51:42,240 --> 00:51:44,160
building blocks. And I'm

1023
00:51:44,240 --> 00:51:46,160
implementing them over here, and we've implemented

1024
00:51:46,240 --> 00:51:48,160
a number of layers together, and we're also

1025
00:51:48,240 --> 00:51:50,160
implementing these containers.

1026
00:51:50,240 --> 00:51:52,160
And we've overall

1027
00:51:52,240 --> 00:51:54,160
PyTorchified our code quite a bit more.

1028
00:51:54,240 --> 00:51:56,160
Now, basically what we're doing

1029
00:51:56,240 --> 00:51:58,160
here is we're reimplementing Torch.nn,

1030
00:51:58,240 --> 00:52:00,160
which is the neural network's

1031
00:52:00,240 --> 00:52:02,160
library on top of

1032
00:52:02,240 --> 00:52:04,080
Torch.tensor. And it looks very much

1033
00:52:04,160 --> 00:52:06,080
like this, except it is much better

1034
00:52:06,160 --> 00:52:08,080
because it's in PyTorch

1035
00:52:08,160 --> 00:52:10,080
instead of jinkling my Jupyter

1036
00:52:10,160 --> 00:52:12,080
notebook. So I think going forward

1037
00:52:12,160 --> 00:52:14,080
I will probably have considered us having

1038
00:52:14,160 --> 00:52:16,080
unlocked Torch.nn.

1039
00:52:16,160 --> 00:52:18,080
We understand roughly what's in there,

1040
00:52:18,160 --> 00:52:20,080
how these modules work, how they're nested,

1041
00:52:20,160 --> 00:52:22,080
and what they're doing on top of

1042
00:52:22,160 --> 00:52:24,080
Torch.tensor. So hopefully we'll just

1043
00:52:24,160 --> 00:52:26,080
switch over and continue

1044
00:52:26,160 --> 00:52:28,080
and start using Torch.nn directly.

1045
00:52:28,160 --> 00:52:30,080
The next thing I hope you got a bit of a sense of

1046
00:52:30,160 --> 00:52:32,080
is what the development process

1047
00:52:32,080 --> 00:52:34,000
of building deep neural networks looks like.

1048
00:52:34,080 --> 00:52:36,000
Which I think was relatively representative

1049
00:52:36,080 --> 00:52:38,000
to some extent. So number one,

1050
00:52:38,080 --> 00:52:40,000
we are spending a lot of time

1051
00:52:40,080 --> 00:52:42,000
in the documentation page of PyTorch.

1052
00:52:42,080 --> 00:52:44,000
And we're reading through all the layers,

1053
00:52:44,080 --> 00:52:46,000
looking at documentations,

1054
00:52:46,080 --> 00:52:48,000
what are the shapes of the inputs,

1055
00:52:48,080 --> 00:52:50,000
what can they be, what does the layer do,

1056
00:52:50,080 --> 00:52:52,000
and so on. Unfortunately,

1057
00:52:52,080 --> 00:52:54,000
I have to say the PyTorch documentation

1058
00:52:54,080 --> 00:52:56,000
is not very good.

1059
00:52:56,080 --> 00:52:58,000
They spend a ton of time on hardcore

1060
00:52:58,080 --> 00:53:00,000
engineering of all kinds of distributed primitives,

1061
00:53:00,080 --> 00:53:02,000
etc. But as far as I can tell,

1062
00:53:02,000 --> 00:53:03,920
no one is maintaining documentation.

1063
00:53:04,000 --> 00:53:05,920
It will lie to you,

1064
00:53:06,000 --> 00:53:07,920
it will be wrong, it will be incomplete,

1065
00:53:08,000 --> 00:53:09,920
it will be unclear.

1066
00:53:10,000 --> 00:53:11,920
So unfortunately, it is what it is

1067
00:53:12,000 --> 00:53:13,920
and you just kind of do your best

1068
00:53:14,000 --> 00:53:15,920
with what they've

1069
00:53:16,000 --> 00:53:17,920
given us.

1070
00:53:18,000 --> 00:53:19,920
Number two,

1071
00:53:20,000 --> 00:53:21,920
the other thing that I hope you got

1072
00:53:22,000 --> 00:53:23,920
a sense of is there's a ton of

1073
00:53:24,000 --> 00:53:25,920
trying to make the shapes work.

1074
00:53:26,000 --> 00:53:27,920
And there's a lot of gymnastics around these multi-dimensional

1075
00:53:28,000 --> 00:53:29,920
arrays. And are they two-dimensional,

1076
00:53:30,000 --> 00:53:31,920
three-dimensional, four-dimensional?

1077
00:53:31,920 --> 00:53:33,840
Do the layers take what shapes?

1078
00:53:33,920 --> 00:53:35,840
Is it NCL or NLC?

1079
00:53:35,920 --> 00:53:37,840
And you're permuting and viewing,

1080
00:53:37,920 --> 00:53:39,840
and it just gets pretty messy.

1081
00:53:39,920 --> 00:53:41,840
And so that brings me to number three.

1082
00:53:41,920 --> 00:53:43,840
I very often prototype these layers

1083
00:53:43,920 --> 00:53:45,840
and implementations in Jupyter Notebooks

1084
00:53:45,920 --> 00:53:47,840
and make sure that all the shapes work out.

1085
00:53:47,920 --> 00:53:49,840
And I'm spending a lot of time basically

1086
00:53:49,920 --> 00:53:51,840
babysitting the shapes and making sure

1087
00:53:51,920 --> 00:53:53,840
everything is correct. And then once I'm

1088
00:53:53,920 --> 00:53:55,840
satisfied with the functionality in a Jupyter Notebook,

1089
00:53:55,920 --> 00:53:57,840
I will take that code and copy-paste it into

1090
00:53:57,920 --> 00:53:59,840
my repository of actual code

1091
00:53:59,920 --> 00:54:01,840
that I'm training with. And so

1092
00:54:01,840 --> 00:54:03,760
then I'm working with VS Code on the side.

1093
00:54:03,840 --> 00:54:05,760
So I usually have Jupyter Notebook and VS Code.

1094
00:54:05,840 --> 00:54:07,760
I develop in Jupyter Notebook, I paste

1095
00:54:07,840 --> 00:54:09,760
into VS Code, and then I kick off experiments

1096
00:54:09,840 --> 00:54:11,760
from the repo, of course,

1097
00:54:11,840 --> 00:54:13,760
from the code repository.

1098
00:54:13,840 --> 00:54:15,760
So that's roughly some notes on the

1099
00:54:15,840 --> 00:54:17,760
development process of working with neural nets.

1100
00:54:17,840 --> 00:54:19,760
Lastly, I think this lecture unlocks a lot

1101
00:54:19,840 --> 00:54:21,760
of potential further lectures

1102
00:54:21,840 --> 00:54:23,760
because, number one, we have to convert our

1103
00:54:23,840 --> 00:54:25,760
neural network to actually use these dilated

1104
00:54:25,840 --> 00:54:27,760
causal convolutional layers,

1105
00:54:27,840 --> 00:54:29,760
so implementing the comnet.

1106
00:54:29,840 --> 00:54:31,760
Number two, I potentially start

1107
00:54:31,760 --> 00:54:33,680
to get into what this means,

1108
00:54:33,760 --> 00:54:35,680
where are residual connections and

1109
00:54:35,760 --> 00:54:37,680
skip connections and why are they useful.

1110
00:54:37,760 --> 00:54:39,680
Number three,

1111
00:54:39,760 --> 00:54:41,680
as I mentioned, we don't have any experimental harness.

1112
00:54:41,760 --> 00:54:43,680
So right now I'm just guessing, checking

1113
00:54:43,760 --> 00:54:45,680
everything. This is not representative of

1114
00:54:45,760 --> 00:54:47,680
typical deep learning workflows. You have to

1115
00:54:47,760 --> 00:54:49,680
set up your evaluation harness.

1116
00:54:49,760 --> 00:54:51,680
You can kick off experiments. You have lots of arguments

1117
00:54:51,760 --> 00:54:53,680
that your script can take.

1118
00:54:53,760 --> 00:54:55,680
You're kicking off a lot of experimentation.

1119
00:54:55,760 --> 00:54:57,680
You're looking at a lot of plots of training and validation

1120
00:54:57,760 --> 00:54:59,680
losses, and you're looking at what is working

1121
00:54:59,760 --> 00:55:01,680
and what is not working. And you're working on this

1122
00:55:01,680 --> 00:55:03,600
like population level, and you're doing

1123
00:55:03,680 --> 00:55:05,600
all these hyperparameter searches.

1124
00:55:05,680 --> 00:55:07,600
And so we've done none of that so far.

1125
00:55:07,680 --> 00:55:09,600
So how to set that up

1126
00:55:09,680 --> 00:55:11,600
and how to make it good, I think

1127
00:55:11,680 --> 00:55:13,600
is a whole another topic.

1128
00:55:13,680 --> 00:55:15,600
And number three, we should probably cover

1129
00:55:15,680 --> 00:55:17,600
recurring neural networks. RNNs, LSTMs,

1130
00:55:17,680 --> 00:55:19,600
Grooves, and of course Transformers.

1131
00:55:19,680 --> 00:55:21,600
So many

1132
00:55:21,680 --> 00:55:23,600
places to go,

1133
00:55:23,680 --> 00:55:25,600
and we'll cover that in the future.

1134
00:55:25,680 --> 00:55:27,600
For now, bye. Sorry, I forgot to say that

1135
00:55:27,680 --> 00:55:29,600
if you are interested, I think

1136
00:55:29,680 --> 00:55:31,600
it is kind of interesting to try to beat this number

1137
00:55:31,600 --> 00:55:33,520
1.993, because

1138
00:55:33,600 --> 00:55:35,520
I really haven't tried a lot of experimentation

1139
00:55:35,600 --> 00:55:37,520
here, and there's quite a bit of longing for it potentially,

1140
00:55:37,600 --> 00:55:39,520
to still push this further.

1141
00:55:39,600 --> 00:55:41,520
So I haven't tried any other

1142
00:55:41,600 --> 00:55:43,520
ways of allocating these channels in this

1143
00:55:43,600 --> 00:55:45,520
neural net. Maybe the number of

1144
00:55:45,600 --> 00:55:47,520
dimensions for the embedding is all

1145
00:55:47,600 --> 00:55:49,520
wrong. Maybe it's possible to actually

1146
00:55:49,600 --> 00:55:51,520
take the original network with just one hidden layer

1147
00:55:51,600 --> 00:55:53,520
and make it big enough and actually

1148
00:55:53,600 --> 00:55:55,520
beat my fancy hierarchical

1149
00:55:55,600 --> 00:55:57,520
network. It's not obvious.

1150
00:55:57,600 --> 00:55:59,520
That would be kind of embarrassing if this

1151
00:55:59,600 --> 00:56:01,520
did not do better, even once you torture

1152
00:56:01,520 --> 00:56:03,440
it a little bit. Maybe you can read the

1153
00:56:03,520 --> 00:56:05,440
WaveNet paper and try to figure out how some of these

1154
00:56:05,520 --> 00:56:07,440
layers work and implement them yourselves using

1155
00:56:07,520 --> 00:56:09,440
what we have. And of course

1156
00:56:09,520 --> 00:56:11,440
you can always tune some of the initialization

1157
00:56:11,520 --> 00:56:13,440
or some of the optimization

1158
00:56:13,520 --> 00:56:15,440
and see if you can improve it that way.

1159
00:56:15,520 --> 00:56:17,440
So I'd be curious if people can come up with some

1160
00:56:17,520 --> 00:56:19,440
ways to beat this.

1161
00:56:19,520 --> 00:56:21,440
And yeah, that's it for now. Bye.

