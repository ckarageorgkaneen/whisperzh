{"text": " Hi everyone. Today we are continuing our implementation of MakeMore, our favorite character-level language model. Now, you'll notice that the background behind me is different. That's because I am in Kyoto, and it is awesome. So I'm in a hotel room here. Now, over the last few lectures, we've built up to this architecture that is a multi-layer perceptron character-level language model. So we see that it receives three previous characters and tries to predict the fourth character in a sequence using a very simple multi-layer perceptron using one hidden layer of neurons with tenational neuralities. So what we'd like to do now in this lecture is I'd like to complexify this architecture. In particular, we would like to take more characters in a sequence as an input, not just three. And in addition to that, we don't just want to feed them all into a single hidden layer because that squashes too much information too quickly. Instead, we would like to make a deeper model that progressively fuses this information to make its guess about the next character in a sequence. And so we'll see that as we make this architecture more complex, we'll be able to make a more complex model that progressively fuses this information to make it more complex. We're actually going to arrive at something that looks very much like a WaveNet. So WaveNet is this paper published by DeepMind in 2016. And it is also a language model, basically, but it tries to predict audio sequences instead of character-level sequences or word-level sequences. But fundamentally, the modeling setup is identical. It is an autoregressive model, and it tries to predict the next character in a sequence. And the architecture actually takes this interesting hierarchical sort of approach to predicting the next character in a sequence with this tree-like structure. And this is the architecture, and we're going to implement it in the course of this video. So let's get started. So the starter code for part five is very similar to where we ended up in in part three. Recall that part four was the manual dot propagation exercise. That is kind of an aside. So we are coming back to part three, copy-pasting chunks out of it. And that is our starter code for part five. I've changed very few things otherwise. However, both should look familiar to if you've gone through part three. So in particular, very briefly, we are doing imports. We are reading our data set of words. And we are processing the dataset of words into individual examples. And none of this data generation code has changed. And basically we have lots and lots of examples. In particular, we have 182,000 examples of three characters try to predict the fourth one. And we've broken up every one of these words into little problems. given three characters predict the fourth one so this is our data set and this is what we're trying to get the neural net to do now in part three we started to develop our code around these layer modules that are for example a class linear and we're doing this because we want to think of these modules as building blocks and like a lego building block bricks that we can sort of like stack up into neural networks and we can feed data between these layers and stack them up into sort of graphs now we also developed these layers to have apis and signatures very similar to those that are found in pytorch so we have torch.nn and it's got all these layer building blocks that you would use in practice and we were developing all these to mimic the apis of these so for example we have linear so there will also be a torch.nn.linear and its signature will be very similar to our signature and the functionality will be also quite identical as far as i'm aware so we have the linear layer with the batchnorm1d layer and the 10h layer that we developed previously and linear just does a matrix multiply in the forward pass of this module batchnorm of course is this crazy layer that we developed in the previous lecture and what's crazy about it is well there's many things number one it has these running mean and variances that are trained outside of back propagation they are trained using exponential moving average inside this layer when we call the forward pass uh in addition to that there's this training flag because the behavior of bastion is different during train time and evaluation time and so suddenly we have to be very careful that bastion is in its correct state that it's in the evaluation state or training state so that's something to now keep track of something that sometimes introduces bugs because you forget to put it into the right mode and finally we saw that bastion couples the statistics or the the activations across the examples in the batch so normally we thought of the batch as just an efficiency thing but now we are coupling the computation across batch elements and it's done for the purposes of controlling the activation statistics as we saw in the previous video so it's a very weird layer at least a lot of bugs um partly for example because you have to modulate the training and eval phase and so on um in addition for example you have to wait for uh the mean and the variance to settle and to actually reach a steady state and so um you have to make sure that you if you write the problem invalid you can more simply to you know drag and drop um to get whatever range you want uh and i want to show you the behavior of bastion And then we have a list of layers. And it's a linear, feeds to BatchNorm, feeds to 10H, and then a linear output layer. And its weights are scaled down, so we are not confidently wrong at initialization. We see that this is about 12,000 parameters. We're telling PyTorch that the parameters require gradients. The optimization is, as far as I'm aware, identical, and should look very, very familiar. Nothing changed here. Loss function looks very crazy. We should probably fix this. And that's because 32 batch elements are too few. And so you can get very lucky or unlucky in any one of these batches, and it creates a very thick loss function. So we're gonna fix that soon. Now, once we want to evaluate the trained neural network, we need to remember, because of the BatchNorm layers, to set all the layers to be training equals false. This only matters for the BatchNorm layer so far. And then we evaluate. We see that currently we have validation loss of 2.10, which is fairly good, but there's still a ways to go. But even at 2.10, we see that when we sample from the model, we actually get relatively name-like results that do not exist in a training set. So for example, Yvonne, Kilo, Pros, Alaya, et cetera. So certainly not reasonable, not unreasonable, I would say, but not amazing. And we can still push this validation loss even lower and get much better samples that are even more name-like. So let's improve this model now. Okay, first, let's fix this graph, because it is daggers in my eyes, and I just can't take it anymore. So lossI, if you recall, is a Python list of floats. So for example, the first 10 elements look like this. Now, what we'd like to do, basically, is we need to average up some of these values to get a more sort of representative value along the way. So one way to do this is the following. In PyTorch, if I create, for example, this, I'm gonna do this. If I create, for example, a tensor of the first 10 numbers, then this is currently a one-dimensional array. But recall that I can view this array as two-dimensional. So for example, I can view it as a two-by-five array, and this is a 2D tensor now, two-by-five. And you see what PyTorch has done is that the first row of this tensor is the first five elements, and the second row is the second five elements. I can also view it as a five-by-two as an example. And then recall that I can also use negative one in place of one of these numbers. And PyTorch will calculate what that number must be in order to make the number of elements work out. So this can be this, or like that. Both will work. Of course, this would not work. OK, so this allows it to spread out some of the consecutive values into rows. So that's very helpful, because what we can do now is, first of all, we're going to create a Torch.tensor out of the list of floats. And then we're going to view it. As whatever it is, but we're going to stretch it out into rows of 1,000 consecutive elements. So the shape of this now becomes 200 by 1,000. And each row is 1,000 consecutive elements in this list. So that's very helpful, because now we can do a mean along the rows. And the shape of this will just be 200. And so we've taken basically the mean on every row. So plt.plot of that should be something nicer. Much better. So we see that we've basically made a lot of progress. And then here, this is the learning rate decay. So here we see that the learning rate decay subtracted a ton of energy out of the system, and allowed us to settle into the local minimum in this optimization. So this is a much nicer plot. Let me come up and delete the monster. And we're going to be using this going forward. Now, next up, what I'm bothered by is that you see our forward pass is a little bit gnarly, and takes a lot of time. So we're going to go ahead and do that. And we're going to go ahead and do that. And we're going to go ahead and do that. And we're going to explain too many lines of code. So in particular, we see that we've organized some of the layers inside the layers list, but not all of them for no reason. So in particular, we see that we still have the embedding table special cased outside of the layers. And in addition to that, the viewing operation here is also outside of our layers. Let's create layers for these, and then we can add those layers to just our list. So in particular, the two things that we need is here, we have this embedding table, and we are indexing at the integers inside the batch xb, inside the tensor xb. So that's an embedding table lookup just done with indexing. And then here, we see that we have this view operation, which if you recall from the previous video, simply rearranges the character embeddings and stretches them out into a row. And effectively, what that does is the concatenation operation, basically, except it's free because viewing is very cheap in PyTorch. And no memory is being copied. We're just re-representing how we view that tensor. So let's create modules for both of these operations, the embedding operation and the flattening operation. So I actually wrote the code just to save some time. So we have a module embedding and a module flatten. And both of them simply do the indexing operation in a forward pass and the flattening operation here. And this. C now will just become a self.weight inside an embedding module. And I'm calling these layers specifically embedding and flatten because it turns out that both of them actually exist in PyTorch. So in PyTorch, we have n and dot embedding. And it also takes the number of embeddings and the dimensionality of the embedding, just like we have here. But in addition, PyTorch takes in a lot of other keyword arguments that we are not using for our purposes yet. And for flatten, that also exists in PyTorch. But it also takes additional keyword arguments that we are not using. So we have a very simple flatten. But both of them exist in PyTorch. They're just a bit more simpler. And now that we have these, we can simply take out some of these special cased things. So instead of C, we're just going to have an embedding and vocab size and n embed. And then after the embedding, we are going to flatten. So let's construct those modules. And now I can take out this C. And here, I don't have to special case it anymore. Because now, C is the embedding's weight. And it's inside layers. So this should just work. And then here, our forward pass simplifies substantially. Because we don't need to do these now outside of these layers, outside and explicitly. They're now inside layers. So we can delete those. But now to kick things off, we want this little x, which in the beginning is just xb, the tensor of integers specifying the identities of these characters at the input. And so these characters can now directly feed into the first layer. And this should just work. So let me come here and insert a break. Because I just want to make sure that the first iteration of this runs, and that there's no mistake. So that ran properly. And basically, we've substantially simplified the forward pass here. Okay, I'm sorry, I changed my microphone. So hopefully, the audio is a little bit better. Now, one last thing. One more thing that I would like to do in order to PyTorchify our code even further, is that right now we are maintaining all of our modules in a naked list of layers. And we can also simplify this, because we can introduce the concept of PyTorch containers. So in torch.nn, which we are basically rebuilding from scratch here, there's a concept of containers. And these containers are basically a way of organizing layers into lists or dicts and so on. So in particular, there's a sequential, which maintains a list of layers, and there's a module class in PyTorch. And it basically just passes a given input through all the layers sequentially, exactly as we are doing here. So let's write our own sequential. I've written a code here. And basically, the code for sequential is quite straightforward. We pass in a list of layers, which we keep here. And then given any input in a forward pass, we just call all the layers sequentially and return the result. And in terms of the parameters, it's just all the parameters of the child modules. So we can run this. Okay. And again, simplify this substantially, because we don't maintain this naked list of layers. We now have a notion of a model, which is a module, and in particular, is a sequential of all these layers. And now parameters are simply just model.parameters. And so that list comprehension now lives here. And then here we are doing all the things we used to do. Now here, the code again simplifies substantially. Because we don't have to do this forwarding here, instead we just call the model on the input data. And the input data here are the integers inside xb. So we can simply do logits, which are the outputs of our model, are simply the model called on xb. And then the cross entropy here takes the logits and the targets. So this simplifies substantially. And then this looks good. So let's just make sure this runs. That looks good. Okay. Now here, we actually have some work to do still here, but I'm going to come back later. For now, there's no more layers. There's a model that layers. But it's naughty to access attributes of these classes directly. So we'll come back and fix this later. And then here, of course, this simplifies substantially as well, because logits are the model called on x. And then these logits come here. So we can evaluate the train and validation loss, which currently is terrible, because we just initialized it in neural net. And then we can also sample from the model. And this simplifies dramatically as well, because we just want to call the model onto the context and outcome logits. And then these logits go into softmax and get the probabilities, et cetera. So we can sample from this model. What did I screw up? Okay. So I fixed the issue and we now get the result that we expect, which is gibberish, because the model is not trained. Okay. So we initialize it from scratch. The problem was that when I fixed this cell to be modeled out layers instead of just layers, I did not actually run the cell. And so our neural net was in a training mode. And what caused the issue here is the batch norm layer, as batch norm layer often likes to do, because batch norm was in the training mode. And here we are passing in an input, which is a batch of just a single example made up of the context. And so if you are trying to pass in a single example into a batch norm that is in the training mode. You're going to end up estimating the variance. Using the input and the variance of a single number is not a number because it is a measure of a spread. So for example, the variance of just a single number five, you can see is not a number. And so that's what happened. And batch norm basically caused an issue. And then that polluted all of the further processing. So all that we had to do was make sure that this runs. And we basically made the issue of, again, we didn't actually see the issue with the loss. We could have evaluated the loss. We got the wrong result because batch norm was in the training mode. And so we still get a result. It's just the wrong result because it's using the sample statistics of the batch. Whereas we want to use the running mean and running variance inside the batch norm. And so again, an example of introducing a bug in line because we did not properly maintain the state of what is training or not. Okay. So I rerun everything. And here's where we are. As a reminder, we have the training loss of 2.05 and validation of 2.10. Now, let's go back. Now, because these losses are very similar to each other, we have a sense that we are not overfitting too much on this task. And we can make additional progress in our performance by scaling up the size of the neural network and making everything bigger and deeper. Now, currently, we are using this architecture here, where we are taking in some number of characters, going into a single hidden layer, and then going to the prediction of the next character. The problem here is we don't have a naive way of making this bigger in a productive way. We could, of course, use our neural network. We could use our layers, sort of building blocks and materials to introduce additional layers here and make the network deeper. But it is still the case that we are crushing all of the characters into a single layer all the way at the beginning. And even if we make this a bigger layer and add neurons, it's still kind of like silly to squash all that information so fast in a single step. So what we'd like to do instead is we'd like our network to look a lot more like this in the WaveNet case. So you see in the WaveNet, when we are trying to make the prediction for the next character in a sequence, it is a function of the previous characters that feed in, but not all of these different characters are not just crushed to a single layer and then you have a sandwich. They are crushed slowly. So in particular, we take two characters and we fuse them into sort of like a bigram representation. And we do that for all these characters consecutively. And then we take the bigrams and we fuse those into four character level chunks. And then we fuse that again. And so we do that in this tree-like hierarchical manner. So we fuse the information from the previous context slowly into the network as it gets deeper. And so this is the kind of architecture that we want to implement. Now in the WaveNet's case, this is a visualization of a stack of dilated causal convolution layers. And this makes it sound very scary, but actually the idea is very simple. And the fact that it's a dilated causal convolution layer is really just an implementation detail to make everything fast. We're going to see that later. But for now, let's just keep going. We're going to keep the basic idea of it, which is this progressive fusion. So we want to make the network deeper, and at each level, we want to fuse only two consecutive elements. Two characters, then two bigrams, then two fourgrams, and so on. So let's implement this. Okay, so first up, let me scroll to where we built the dataset, and let's change the block size from three to eight. So we're going to be taking eight characters of context to predict the ninth character. So the dataset now looks like this. We have a lot more context feeding in to predict any next character. So we're going to have a sequence. And these eight characters are going to be processed in this tree-like structure. Now if we scroll here, everything here should just be able to work. So we should be able to redefine the network. You see that the number of parameters has increased by 10,000, and that's because the block size has grown. So this first linear layer is much, much bigger. Our linear layer now takes eight characters into this middle layer. So there's a lot more parameters there. But this should just run. Let me just break right after this. This is the very first iteration. So you see that this runs just fine. It's just that this network doesn't make too much sense. We're crushing way too much information way too fast. So let's now come in and see how we could try to implement the hierarchical scheme. Now before we dive into the detail of the re-implementation here, I was just curious to actually run it and see where we are in terms of the baseline performance of just lazily scaling up the context length. So I let it run. We get a nice loss curve. And then evaluating the loss, we actually see quite a bit of improvement. This is from increasing the context length. So I started a little bit of a performance log here. And previously where we were is we were getting a performance of 2.10 on the validation loss. And now simply scaling up the context length from 3 to 8 gives us a performance of 2.02. So quite a bit of an improvement here. And also when you sample from the model, you see that the names are definitely improving qualitatively as well. So we could, of course, spend a lot of time here tuning things and making it even bigger and scaling up the network further, even with the simple sort of setup here. But let's continue and let's implement the hierarchical model and treat this as just a rough baseline performance. But there's a lot of optimization left on the table in terms of some of the hyperparameters that you're hopefully getting a sense of now. Okay, so let's scroll up now and come back up. And what I've done here is I've created a bit of a scratch space for us to just look at the forward pass of the neural net and inspect the shape of the network. So let's go ahead and do that. So let's go ahead and look at the shape of the tensors along the way as the neural net forwards. So here I'm just temporarily for debugging, creating a batch of just, say, four examples. So four random integers. Then I'm plucking out those rows from our training set. And then I'm passing into the model the input XB. Now the shape of XB here, because we have only four examples, is four by eight. And this eight is now the current block size. So inspecting XB, we just see that we have four examples. Each one of them is a row of XB. And we have eight characters here. And this integer tensor just contains the identities of those characters. So the first layer of our neural net is the embedding layer. So passing XB, this integer tensor, through the embedding layer creates an output that is four by eight by 10. So our embedding table has, for each character, a 10-dimensional vector that we are trying to learn. And so what the embedding layer does here... What the layer does here is it blocks out the embedding vector for each one of these integers and organizes it all in a four by eight by 10 tensor now. So all of these integers are translated into 10-dimensional vectors inside this three-dimensional tensor now. Now passing that through the flatten layer, as you recall, what this does is it views this tensor as just a four by 80 tensor. And what that effectively does is that all these 10-dimensional embeddings for all these eight characters just end up being stretched out into a long row. And that looks kind of like a concatenation operation, basically. So by viewing the tensor differently, we now have a four by 80. And inside this 80, it's all the 10-dimensional vectors just concatenated next to each other. And the linear layer, of course, takes 80 and creates 200 channels just via matrix multiplication. So so far, so good. Now I'd like to show you something surprising. Let's see. Let's look at the insides of the linear layer and remind ourselves how it works. The linear layer here in a forward pass takes the input x, multiplies it with a weight, and then optionally adds bias. And the weight here is two-dimensional, as defined here, and the bias is one-dimensional here. So effectively, in terms of the shapes involved, what's happening inside this linear layer looks like this right now. And I'm using random numbers here, but I'm just illustrating the shapes and what happens. Basically, a four by 80 input comes into the linear layer, gets multiplied by this 80 by 200 weight matrix inside, and then there's a plus 200 bias. And the shape of the whole thing that comes out of the linear layer is four by 200, as we see here. Now notice here, by the way, that this here will create a four by 200 tensor, and then plus 200, there's a broadcasting happening here, but four by 200 broadcasts with 200, so everything works here. So now the surprising thing that I want to show you is this. I'm going to show you how this works. One thing that I'd like to show you that you may not expect is that this input here that is being multiplied doesn't actually have to be two-dimensional. This matrix multiply operator in PyTorch is quite powerful, and in fact, you can actually pass in higher dimensional arrays or tensors, and everything works fine. So for example, this could be four by five by 80, and the result in that case will become four by five by 200. You can add as many dimensions as you like on the left here. And so effectively, what's happening is that the matrix multiplication only works on a matrix multiplication on the last dimension, and the dimensions before it in the input tensor are left unchanged. So basically, these dimensions on the left are all treated as just a batch dimension. So we can have multiple batch dimensions, and then in parallel over all those dimensions, we are doing the matrix multiplication on the last dimension. So this is quite convenient because we can use that in our network now. Because remember that. We have these eight characters coming in. And we don't want to now flatten all of it out into a large eight-dimensional vector because we don't want to matrix multiply 80 into a weight matrix multiply immediately. Instead, we want to group these like this. So every consecutive two elements, one and two and three and four and five and six and seven and eight, all of these should be now basically flattened. Okay. So we can. End out and multiply by weight matrix. But all of these four groups here, we'd like to process in parallel. So it's kind of like a batch dimension that we can introduce. And then we can, in parallel, basically process all of these bigram groups in the four batch dimensions of an individual example, and also over the actual batch dimension of the four examples in our example here. So let's see how that works. Effectively, what we want is. Right now. Now we take a 4 by 80 and multiply it by 80 by 200 in the linear layer. This is what happens. But instead what we want is we don't want 80 characters or 80 numbers to come in. We only want two characters to come in on the very first layer, and those two characters should be fused. So in other words, we just want 20 to come in, right? 20 numbers would come in. And here we don't want a 4 by 80 to feed into the linear layer. We actually want these groups of 2 to feed in. So instead of 4 by 80, we want this to be a 4 by 4 by 20. So these are the four groups of 2, and each one of them is a 10-dimensional vector. So what we want now is we need to change the flatten layer so it doesn't output a 4 by 80, but it outputs a 4 by 4 by 20, where basically every two consecutive characters are packed in on the very last dimension. And then these four is the first batch dimension, and this four is the second batch dimension, referring to the four groups inside every one of these examples. And then this will just multiply like this. So this is what we want to get to. So we're going to have to change the linear layer in terms of how many inputs it expects. It shouldn't expect 80. It should just expect 20 numbers. And we have to change our flatten layer so it doesn't just fully flatten out this entire example. It needs to create a 4 by 4. It needs to create a 4 by 20 instead of a 4 by 80. So let's see how this could be implemented. Basically right now we have an input that is a 4 by 8 by 10 that feeds into the flatten layer, and currently the flatten layer just stretches it out. So if you remember the implementation of flatten, it takes our x and it just views it as whatever the batch dimension is, and then negative 1. So effectively what it does right now is it does e.view of 4, negative 1, and the shape of this, of course, is 4 by 80. So that's what currently happens, and we instead want this to be a 4 by 4 by 20, where these consecutive 10-dimensional vectors get concatenated. So you know how in Python you can take a list of range of 10? So we have numbers from 0 to 9, and we can index like this to get all the even parts, and we can also index like starting at 1 and going in steps of 2 to get all the odd parts. So one way to implement this is to take a list of range of 10, and one way to implement this, it would be as follows. We can take e, and we can index into it for all the batch elements, and then just even elements in this dimension, so at indexes 0, 2, 4, and 8, and then all the parts here from this last dimension, and this gives us the even characters, and then here this gives us all the odd characters. And basically what we want to do is we want to make sure that these get concatenated in PyTorch, and then we want to concatenate these two tensors along the second dimension. So this and the shape of it would be 4 by 4 by 20. This is definitely the result we want. We are explicitly grabbing the even parts and the odd parts, and we're arranging those 4 by 4 by 10 right next to each other and concatenate. So this works, but it turns out that what also works is you can simply use a view again, and just request the right shape. And it just so happens that in this case, those vectors will again end up being arranged exactly the way we want. So in particular, if we take e, and we just view it as a 4 by 4 by 20, which is what we want, we can check that this is exactly equal to, let me call this, this is the explicit concatenation, I suppose. So explicit dot shape is 4 by 4 by 20. If you just view it as 4 by 4 by 20, you can check that, when you compare it to explicit, you get a big, this is element-wise operation, so making sure that all of them are true, values to true. So basically, long story short, we don't need to make an explicit call to concatenate, etc. We can simply take this input tensor to flatten, and we can just view it in whatever way we want. And in particular, we don't want to stretch things out with negative 1. We want to actually create a three-dimensional array, and depending on how many, vectors that are consecutive, we want to fuse, like for example, 2, then we can just simply ask for this dimension to be 20, and use a negative 1 here, and PyTorch will figure out how many groups it needs to pack into this additional batch dimension. So let's now go into flatten and implement this. Okay, so I scrolled up here to flatten, and what we'd like to do is we'd like to change it now. So let me create a constructor, and take the number of elements that are consecutive, that we would like to use, and then we'd like to concatenate now in the last dimension of the output. So here we're just going to remember, self.n equals n. And then I want to be careful here, because PyTorch actually has a torch.flatten, and its keyword arguments are different, and they kind of like function differently. So our flatten is going to start to depart from PyTorch flatten. So let me call it flatten consecutive, or something like that, just to make sure that our APIs are about equal. So this, basically flattens only some n consecutive elements, and puts them into the last dimension. Now here, the shape of x is b by t by c. So let me pop those out into variables. And recall that in our example down below, b was 4, t was 8, and c was 10. Now, instead of doing x.view of b by negative 1, right, this is what we had before. We want this to be b by negative 1 by, and basically here, we want c times n. That's how many consecutive elements we want. And here, instead of negative 1, I don't super love the use of negative 1, because I like to be very explicit, so that you get error messages when things don't go according to your expectation. So what do we expect here? We expect this to become t divide n, using integer division here. So that's what I expect to happen. And then one more thing I want to do here is, remember previously, all the way in the beginning, n was 3, and basically we're concatenating all the three characters that existed there. So we basically concatenated everything. And so sometimes that can create a spurious dimension of 1 here. So if it is the case that x.shapeAt1 is 1, then it's kind of like a spurious dimension. So we don't want to return a 3, so we don't want to return a 3, so we don't want to return a 3-dimensional tensor with a 1 here. We just want to return a 2-dimensional tensor exactly as we did before. So in this case, basically, we will just say x equals x.squeeze, that is a PyTorch function. And squeeze takes a dimension that it either squeezes out all the dimensions of a tensor that are 1, or you can specify the exact dimension that you want to be squeezed. And again, I like to be as explicit as possible, always, so I expect to squeeze out the first dimension only of this tensor, this 3-dimensional tensor. And if this dimension here is 1, then I just want to return b by c times n. And so self.out will be x, and then we return self.out. So that's the candidate implementation. And of course, this should be self.in instead of just n. So let's run, and let's come here now and take it for a spin. So flattened consecutive, and in the beginning, let's just use 8. So this should recover the previous behavior. So flattened consecutive of 8, which is the current block size, we can do this, that should recover the previous behavior. So we should be able to run the model. And here we can inspect, I have a little code snippet here, where I iterate over all the layers, I print the name, of this class, and the shape. And so we see the shapes as we expect them after every single layer in its output. So now let's try to restructure it using our flattened consecutive and do it hierarchically. So in particular, we want to flatten consecutive, not just block size, but just 2. And then we want to process this with linear. Now, the number of inputs to this linear will not be n embed times block size, it will now only be n embed times 2, 20. This goes through the first layer. And now we can, in principle, just copy paste this. Now, the next linear layer should expect n hidden times 2. And the last piece of it should expect n hidden times 2 again. So this is sort of like the naive version of it. So running this, we now have a much, much bigger model. And we should be able to basically just just forward the model. And now we can inspect the numbers in between. So 4x8x20 was flattened consecutively into 4x4x20. This was projected into 4x4x200. And then BatchNorm just worked out of the box. We have to verify that BatchNorm does the correct thing, even though it takes a three-dimensional input instead of two-dimensional input. Then we have 10H, which is element-wise. Then we crushed it again. So we flattened consecutively and ended up with a 4x2x400 now. Then linear brought it back down to 200, BatchNorm 10H. And lastly, we get a 4x400. And we see that the flattened consecutive for the last flatten here, it squeezed out that dimension of 1. So we only ended up with 4x400. And then linear BatchNorm 10H and the last linear layer to get our logits. And so the logits end up in the same shape as they were before. But now we actually have a nice three-layer, neural net. And it basically corresponds to, whoops, sorry. It basically corresponds exactly to this network now, except only this piece here because we only have three layers. Whereas here in this example, there's four layers with a total receptive field size of 16 characters instead of just eight characters. So the block size here is 16. So this piece of it is basically implemented here. Now we just have to kind of figure out some good, channel numbers to use here. Now in particular, I changed the number of hidden units to be 68 in this architecture because when I use 68, the number of parameters comes out to be 22,000. So that's exactly the same that we had before. And we have the same amount of capacity at this neural net in terms of the number of parameters. But the question is whether we are utilizing those parameters in a more efficient architecture. So what I did then is I got rid of a lot of the debugging cells here and I rerun the optimization. And scrolling down to the result, we see that we get the identical performance roughly. So our validation loss now is 2.029 and previously it was 2.027. So controlling for the number of parameters, changing from the flat to hierarchical is not giving us anything yet. That said, there are two things to point out. Number one, we didn't really torture the architecture here very much. This is just my first guess. And there's a bunch of hyperparameter search that we could do in terms of how we allocate our budget of parameters to what layers. Number two, we still may have a bug inside the BatchNorm1D layer. So let's take a look at that because it runs but doesn't do the right thing. So I pulled up the layer inspector sort of that we have here and printed out the shape along the way. And currently it looks like the BatchNorm is receiving an input that is 32 by 4 by 68, right? And here on the right, I have the current implementation of BatchNorm that we have right now. Now, this BatchNorm assumed, in the way we wrote it and at the time, that X is two-dimensional. So it was N by D, where N was the batch size. So that's why we only reduced the mean and the variance over the zeroth dimension. But now X will basically become three-dimensional. So what's happening inside the BatchNorm layer right now? And how come it's working at all and not giving any errors? The reason for that is basically because everything broadcasts properly, but the BatchNorm is not doing what we want it to do. So in particular, let's basically think through what's happening inside the BatchNorm. I'm looking at what's happening here. I have the code here. So we're receiving an input of 32 by 4 by 68. And then we are doing here, X dot mean. Here I have E instead of X. But we're doing the mean over zero. And that's actually giving us 1 by 4 by 68. So we're doing the mean only over the very first dimension. And it's giving us a mean and a variance that still maintain this dimension here. So these means are only taken over 32 numbers in the first dimension. And then when we perform this, everything broadcasts correctly still. But basically what ends up happening is when we also look at the running mean, the shape of it. So I'm looking at the model that layers the three, which is the first BatchNorm layer, and then looking at whatever the running mean became and its shape. The shape of this running mean now is 1 by 4 by 68. Instead of it being just size of dimension, because we have 68 channels, we expect to have 68 means and variances that we're maintaining. But actually we have an array of 4 by 68. And so basically what this is telling us is this BatchNorm is only... This BatchNorm is currently working in parallel over 4 times 68 instead of just 68 channels. So basically we are maintaining this. We are maintaining statistics for every one of these four positions individually and independently. And instead what we want to do is we want to treat this 4 as a Batch dimension, just like the 0th dimension. So as far as the BatchNorm is concerned, it doesn't want to average... We don't want to average over 32 numbers. We want to now average over 32 times 4 numbers for every single one of these 68 channels. And so let me now remove this. It turns out that when you look at the documentation of Torch.mean... So let's go to Torch.mean. In one of its signatures, when we specify the dimension, we see that the dimension here is not just... It can be int or it can also be a tuple of ints. So we can reduce over multiple integers at the same time, over multiple dimensions at the same time. So instead of just reducing over 0, we can pass in a tuple, 0, 1, and here 0, 1 as well. And then what's going to happen is the output, of course, is going to be the same. But now what's going to happen is because we reduce over 0 and 1, if we look at inmean.shape, we see that now we've reduced. We took the mean over both the 0th and the first dimension. So we're just getting 68 numbers and a bunch of spurious dimensions here. So now this becomes 1 by 1 by 68. And the running mean and the running variance, analogously, will become 1 by 1 by 68. So even though there are the spurious dimensions, the correct thing will happen in that we are only maintaining means and variances for 68 channels. And we're now calculating the mean and variance across 32 times 4 dimensions. So that's exactly what we want. And let's change the implementation of BatchNorm1D that we have so that it can take in two-dimensional or three-dimensional inputs and perform accordingly. So at the end of the day, the fix is relatively straightforward. Basically, the dimension we want to reduce over is either 0 or the tuple 0 and 1, depending on the dimensionality of x. So if x.ndim is 2, so it's a two-dimensional tensor, then the dimension we want to reduce over is just the integer 0. And if x.ndim is 3, so it's a three-dimensional tensor, then the dims we're going to assume are 0 and 1 that we want to reduce over. And then here, we just pass in dim. And if the dimensionality of x is anything else, we're going to get an error, which is good. So that should be the fix. Now, I want to point out one more thing. We're actually departing from the API of PyTorch here a little bit, because when you come to BatchNorm1D in PyTorch, you can scroll down and you can see that the input to this layer can either be n by c, where n is the batch size and c is the number of features or channels, or it actually does accept three-dimensional inputs, but it expects it to be n by c by l, where l is, say, like the sequence length or something like that. So this is a problem because you see how c is nested here in the middle. And so when it gets three-dimensional inputs, this BatchNorm layer will reduce over 0 and 2 instead of 0 and 1. So basically, PyTorch BatchNorm1D layer assumes that c will always be the first dimension, whereas we assume here that c is the last dimension, and there are some number of batch dimensions beforehand. And so, it expects n by c or n by c by l. We expect n by c or n by l by c. And so, it's a deviation. I think it's okay. I prefer it this way, honestly, so this is the way that we will keep it for our purposes. So I redefined the layers, reinitialized the neural net, and did a single forward pass with a break just for one step. Looking at the shapes along the way, they're, of course, identical. All the shapes are the same, but the way we see that things are actually working as we want them to, is that we can actually do the same thing. So the way we see that things are actually working as we want them to now is that when we look at the BatchNorm layer, the running mean shape is now 1 by 1 by 68. So we're only maintaining 68 means for every one of our channels, and we're treating both the 0th and the first dimension as a batch dimension, which is exactly what we want. So let me retrain the neural net now. Okay, so I've retrained the neural net with the bug fix. We get a nice curve. And when we look at the validation performance, we do actually see a slight improvement. So it went from 2.029 to 2.022. So basically, the bug inside the BatchNorm was holding us back, like, a little bit, it looks like. And we are getting a tiny improvement now, but it's not clear if this is statistically significant. And the reason we slightly expect an improvement is because we're not maintaining so many different means and variances that are only estimated using 32 numbers, effectively. Now we are estimating them using 32 times 4 numbers. So you just have a lot more numbers that go into any one estimate of the mean and variance. And it allows things to be a bit more stable and less wiggly inside those estimates of the BatchNorm. So pretty nice. With this more general architecture in place, we are now set up to push the performance further by increasing the size of the network. So, for example, I've bumped up the number of embeddings to 24 instead of 10, and also increased the number of hidden units. But using the exact same architecture, we now have 76,000 parameters, and the training takes a lot longer, but we do get a nice curve. And then when you actually evaluate the performance, we are now getting validation performance of 1.993. So we've crossed over 1.993. So we've crossed over 1.993. We've crossed over the 2.0 sort of territory. And we're at about 1.99. But we are starting to have to wait quite a bit longer. But we are starting to have to wait quite a bit longer. And we're a little bit in the dark with respect to the correct setting of the hyperparameters here and the learning rates and so on, because the experiments are starting to take longer to train. And so we are missing sort of like an experimental harness on which we could run a number of experiments on which we could run a number of experiments and really tune this architecture very well. So I'd like to conclude now with a few notes. We basically improved our performance from a starting of 2.1 to 2.9. But I don't want that to be the focus because honestly we're kind of in the dark. We have no experimental harness. We're just guessing and checking. And this whole thing is terrible. We're just looking at the training loss. Normally you want to look at both the training and the validation loss together. The whole thing looks different if you're actually trying to squeeze out numbers. That said, we did implement this architecture from the WaveNet paper. But we did not implement this specific forward pass of it where you have a more complicated structure that is this gated linear layer kind of. And there's residual connections and skip connections and so on. So we did not implement that. We just implemented this structure. I would like to briefly hint or preview how what we've done here relates to convolutional neural networks as used in the WaveNet paper. And basically the use of convolutions is strictly for efficiency. It doesn't actually change the model we've implemented. So here for example, let me look at a specific name to work with an example. So we have a name in our training set and it's D'Andre. And it has seven letters. So that is eight independent examples in our model. So all these rows here are independent examples of D'Andre. Now you can forward of course any one of these rows independently. So I can take my model and call it on any individual index. Notice by the way here I'm being a little bit tricky. The reason for this is that it's a one dimensional array of eight. So you can't actually call the model on it. You're going to get an error because there's no batch dimension. So when you do extra at a list of seven then the shape of this becomes one by eight. So I get an extra batch dimension of one and then we can forward the model. So that forwards a single example and you might imagine that you actually may want to forward all of these eight at the same time. So pre-allocating some memory and then doing a for loop eight times and forwarding all of those eight here will give us all the logits in all these different cases. Now for us with the model as we've implemented it right now this is eight independent calls to our model. But what convolutions allow you to do is it allow you to basically slide this model efficiently over the input sequence. And so this for loop can be done not outside in Python but inside of kernels in CUDA. And so this for loop gets hidden into the convolution. So the convolution basically you can think of it as it's a for loop applying a little linear filter over space of some input sequence. And in our case the space we're interested in is one dimensional and we're interested in sliding these filters over the input data. So this diagram actually is fairly good as well. Basically what we've done is here they are highlighting in black one single sort of like tree of this calculation. So just calculating the single output example here. And so this is basically what we've implemented here. We've implemented a single, this black structure we've implemented that and calculated a single output, like a single example. But what convolutions allow you to do is it allows you to take this black structure and kind of like slide it over the input sequence here and calculate all of these orange outputs at the same time. Or here that corresponds to calculating all of these outputs of at all the positions of deandre at the same time. And the reason that this is much more efficient is because number one, as I mentioned, the for loop is inside the CUDA kernels in the sliding. So that makes it efficient. But number two, notice the variable reuse here. For example if we look at this circle, this node here, this node here is the right child of this node, but it's also the left child of the node here. And so basically this node and its value is used twice. And so right now, in this naive way, we'd have to recalculate it. But here we are allowed to reuse it. So in the convolutional neural network, you think of these linear layers that we have up above as filters. And we take these filters and they're linear filters, and you slide them over input sequence, and we calculate the first layer, and then the second layer, and then the third layer, and then the output layer of the sandwich, and it's all done very efficiently using these convolutions. So we're going to cover that in a future video. The second thing I hope you took away from this video is you've seen me basically implement all of these layer Lego building blocks, or module building blocks. And I'm implementing them over here, and we've implemented a number of layers together, and we're also implementing these containers. And we've overall PyTorchified our code quite a bit more. Now, basically what we're doing here is we're reimplementing Torch.nn, which is the neural network's library on top of Torch.tensor. And it looks very much like this, except it is much better because it's in PyTorch instead of jinkling my Jupyter notebook. So I think going forward I will probably have considered us having unlocked Torch.nn. We understand roughly what's in there, how these modules work, how they're nested, and what they're doing on top of Torch.tensor. So hopefully we'll just switch over and continue and start using Torch.nn directly. The next thing I hope you got a bit of a sense of is what the development process of building deep neural networks looks like. Which I think was relatively representative to some extent. So number one, we are spending a lot of time in the documentation page of PyTorch. And we're reading through all the layers, looking at documentations, what are the shapes of the inputs, what can they be, what does the layer do, and so on. Unfortunately, I have to say the PyTorch documentation is not very good. They spend a ton of time on hardcore engineering of all kinds of distributed primitives, etc. But as far as I can tell, no one is maintaining documentation. It will lie to you, it will be wrong, it will be incomplete, it will be unclear. So unfortunately, it is what it is and you just kind of do your best with what they've given us. Number two, the other thing that I hope you got a sense of is there's a ton of trying to make the shapes work. And there's a lot of gymnastics around these multi-dimensional arrays. And are they two-dimensional, three-dimensional, four-dimensional? Do the layers take what shapes? Is it NCL or NLC? And you're permuting and viewing, and it just gets pretty messy. And so that brings me to number three. I very often prototype these layers and implementations in Jupyter Notebooks and make sure that all the shapes work out. And I'm spending a lot of time basically babysitting the shapes and making sure everything is correct. And then once I'm satisfied with the functionality in a Jupyter Notebook, I will take that code and copy-paste it into my repository of actual code that I'm training with. And so then I'm working with VS Code on the side. So I usually have Jupyter Notebook and VS Code. I develop in Jupyter Notebook, I paste into VS Code, and then I kick off experiments from the repo, of course, from the code repository. So that's roughly some notes on the development process of working with neural nets. Lastly, I think this lecture unlocks a lot of potential further lectures because, number one, we have to convert our neural network to actually use these dilated causal convolutional layers, so implementing the comnet. Number two, I potentially start to get into what this means, where are residual connections and skip connections and why are they useful. Number three, as I mentioned, we don't have any experimental harness. So right now I'm just guessing, checking everything. This is not representative of typical deep learning workflows. You have to set up your evaluation harness. You can kick off experiments. You have lots of arguments that your script can take. You're kicking off a lot of experimentation. You're looking at a lot of plots of training and validation losses, and you're looking at what is working and what is not working. And you're working on this like population level, and you're doing all these hyperparameter searches. And so we've done none of that so far. So how to set that up and how to make it good, I think is a whole another topic. And number three, we should probably cover recurring neural networks. RNNs, LSTMs, Grooves, and of course Transformers. So many places to go, and we'll cover that in the future. For now, bye. Sorry, I forgot to say that if you are interested, I think it is kind of interesting to try to beat this number 1.993, because I really haven't tried a lot of experimentation here, and there's quite a bit of longing for it potentially, to still push this further. So I haven't tried any other ways of allocating these channels in this neural net. Maybe the number of dimensions for the embedding is all wrong. Maybe it's possible to actually take the original network with just one hidden layer and make it big enough and actually beat my fancy hierarchical network. It's not obvious. That would be kind of embarrassing if this did not do better, even once you torture it a little bit. Maybe you can read the WaveNet paper and try to figure out how some of these layers work and implement them yourselves using what we have. And of course you can always tune some of the initialization or some of the optimization and see if you can improve it that way. So I'd be curious if people can come up with some ways to beat this. And yeah, that's it for now. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.4, "text": " Hi everyone. Today we are continuing our implementation of MakeMore, our favorite", "tokens": [50365, 2421, 1518, 13, 2692, 321, 366, 9289, 527, 11420, 295, 4387, 33986, 11, 527, 2954, 50585], "temperature": 0.0, "avg_logprob": -0.0532645456718676, "compression_ratio": 1.7682539682539682, "no_speech_prob": 0.016635971143841743}, {"id": 1, "seek": 0, "start": 4.4, "end": 9.0, "text": " character-level language model. Now, you'll notice that the background behind me is different. That's", "tokens": [50585, 2517, 12, 12418, 2856, 2316, 13, 823, 11, 291, 603, 3449, 300, 264, 3678, 2261, 385, 307, 819, 13, 663, 311, 50815], "temperature": 0.0, "avg_logprob": -0.0532645456718676, "compression_ratio": 1.7682539682539682, "no_speech_prob": 0.016635971143841743}, {"id": 2, "seek": 0, "start": 9.0, "end": 14.38, "text": " because I am in Kyoto, and it is awesome. So I'm in a hotel room here. Now, over the last few", "tokens": [50815, 570, 286, 669, 294, 48470, 11, 293, 309, 307, 3476, 13, 407, 286, 478, 294, 257, 7622, 1808, 510, 13, 823, 11, 670, 264, 1036, 1326, 51084], "temperature": 0.0, "avg_logprob": -0.0532645456718676, "compression_ratio": 1.7682539682539682, "no_speech_prob": 0.016635971143841743}, {"id": 3, "seek": 0, "start": 14.38, "end": 19.62, "text": " lectures, we've built up to this architecture that is a multi-layer perceptron character-level", "tokens": [51084, 16564, 11, 321, 600, 3094, 493, 281, 341, 9482, 300, 307, 257, 4825, 12, 8376, 260, 43276, 2044, 2517, 12, 12418, 51346], "temperature": 0.0, "avg_logprob": -0.0532645456718676, "compression_ratio": 1.7682539682539682, "no_speech_prob": 0.016635971143841743}, {"id": 4, "seek": 0, "start": 19.62, "end": 23.7, "text": " language model. So we see that it receives three previous characters and tries to predict the", "tokens": [51346, 2856, 2316, 13, 407, 321, 536, 300, 309, 20717, 1045, 3894, 4342, 293, 9898, 281, 6069, 264, 51550], "temperature": 0.0, "avg_logprob": -0.0532645456718676, "compression_ratio": 1.7682539682539682, "no_speech_prob": 0.016635971143841743}, {"id": 5, "seek": 0, "start": 23.7, "end": 28.36, "text": " fourth character in a sequence using a very simple multi-layer perceptron using one hidden", "tokens": [51550, 6409, 2517, 294, 257, 8310, 1228, 257, 588, 2199, 4825, 12, 8376, 260, 43276, 2044, 1228, 472, 7633, 51783], "temperature": 0.0, "avg_logprob": -0.0532645456718676, "compression_ratio": 1.7682539682539682, "no_speech_prob": 0.016635971143841743}, {"id": 6, "seek": 2836, "start": 28.36, "end": 33.46, "text": " layer of neurons with tenational neuralities. So what we'd like to do now in this lecture is I'd", "tokens": [50365, 4583, 295, 22027, 365, 2064, 1478, 18161, 1088, 13, 407, 437, 321, 1116, 411, 281, 360, 586, 294, 341, 7991, 307, 286, 1116, 50620], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 7, "seek": 2836, "start": 33.46, "end": 37.4, "text": " like to complexify this architecture. In particular, we would like to take more characters", "tokens": [50620, 411, 281, 3997, 2505, 341, 9482, 13, 682, 1729, 11, 321, 576, 411, 281, 747, 544, 4342, 50817], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 8, "seek": 2836, "start": 37.4, "end": 42.66, "text": " in a sequence as an input, not just three. And in addition to that, we don't just want to feed", "tokens": [50817, 294, 257, 8310, 382, 364, 4846, 11, 406, 445, 1045, 13, 400, 294, 4500, 281, 300, 11, 321, 500, 380, 445, 528, 281, 3154, 51080], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 9, "seek": 2836, "start": 42.66, "end": 47.019999999999996, "text": " them all into a single hidden layer because that squashes too much information too quickly.", "tokens": [51080, 552, 439, 666, 257, 2167, 7633, 4583, 570, 300, 2339, 12808, 886, 709, 1589, 886, 2661, 13, 51298], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 10, "seek": 2836, "start": 47.68, "end": 52.760000000000005, "text": " Instead, we would like to make a deeper model that progressively fuses this information to make", "tokens": [51331, 7156, 11, 321, 576, 411, 281, 652, 257, 7731, 2316, 300, 46667, 283, 8355, 341, 1589, 281, 652, 51585], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 11, "seek": 2836, "start": 52.760000000000005, "end": 57.96, "text": " its guess about the next character in a sequence. And so we'll see that as we make this architecture", "tokens": [51585, 1080, 2041, 466, 264, 958, 2517, 294, 257, 8310, 13, 400, 370, 321, 603, 536, 300, 382, 321, 652, 341, 9482, 51845], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 12, "seek": 2836, "start": 57.96, "end": 58.34, "text": " more complex, we'll be able to make a more complex model that progressively fuses this", "tokens": [51845, 544, 3997, 11, 321, 603, 312, 1075, 281, 652, 257, 544, 3997, 2316, 300, 46667, 283, 8355, 341, 51864], "temperature": 0.0, "avg_logprob": -0.11968174228420506, "compression_ratio": 2.01840490797546, "no_speech_prob": 4.676347816712223e-05}, {"id": 13, "seek": 5834, "start": 58.34, "end": 61.440000000000005, "text": " information to make it more complex. We're actually going to arrive at something that looks", "tokens": [50365, 1589, 281, 652, 309, 544, 3997, 13, 492, 434, 767, 516, 281, 8881, 412, 746, 300, 1542, 50520], "temperature": 0.0, "avg_logprob": -0.11768468221028645, "compression_ratio": 1.7417218543046358, "no_speech_prob": 0.000790628488175571}, {"id": 14, "seek": 5834, "start": 61.440000000000005, "end": 66.52000000000001, "text": " very much like a WaveNet. So WaveNet is this paper published by DeepMind in 2016.", "tokens": [50520, 588, 709, 411, 257, 28530, 31890, 13, 407, 28530, 31890, 307, 341, 3035, 6572, 538, 14895, 44, 471, 294, 6549, 13, 50774], "temperature": 0.0, "avg_logprob": -0.11768468221028645, "compression_ratio": 1.7417218543046358, "no_speech_prob": 0.000790628488175571}, {"id": 15, "seek": 5834, "start": 67.48, "end": 73.5, "text": " And it is also a language model, basically, but it tries to predict audio sequences instead of", "tokens": [50822, 400, 309, 307, 611, 257, 2856, 2316, 11, 1936, 11, 457, 309, 9898, 281, 6069, 6278, 22978, 2602, 295, 51123], "temperature": 0.0, "avg_logprob": -0.11768468221028645, "compression_ratio": 1.7417218543046358, "no_speech_prob": 0.000790628488175571}, {"id": 16, "seek": 5834, "start": 73.5, "end": 79.28, "text": " character-level sequences or word-level sequences. But fundamentally, the modeling setup is", "tokens": [51123, 2517, 12, 12418, 22978, 420, 1349, 12, 12418, 22978, 13, 583, 17879, 11, 264, 15983, 8657, 307, 51412], "temperature": 0.0, "avg_logprob": -0.11768468221028645, "compression_ratio": 1.7417218543046358, "no_speech_prob": 0.000790628488175571}, {"id": 17, "seek": 5834, "start": 79.28, "end": 84.28, "text": " identical. It is an autoregressive model, and it tries to predict the next character in a sequence.", "tokens": [51412, 14800, 13, 467, 307, 364, 1476, 418, 3091, 488, 2316, 11, 293, 309, 9898, 281, 6069, 264, 958, 2517, 294, 257, 8310, 13, 51662], "temperature": 0.0, "avg_logprob": -0.11768468221028645, "compression_ratio": 1.7417218543046358, "no_speech_prob": 0.000790628488175571}, {"id": 18, "seek": 5834, "start": 84.28, "end": 87.94, "text": " And the architecture actually takes this interesting hierarchical", "tokens": [51662, 400, 264, 9482, 767, 2516, 341, 1880, 35250, 804, 51845], "temperature": 0.0, "avg_logprob": -0.11768468221028645, "compression_ratio": 1.7417218543046358, "no_speech_prob": 0.000790628488175571}, {"id": 19, "seek": 8794, "start": 87.94, "end": 93.92, "text": " sort of approach to predicting the next character in a sequence with this tree-like structure.", "tokens": [50365, 1333, 295, 3109, 281, 32884, 264, 958, 2517, 294, 257, 8310, 365, 341, 4230, 12, 4092, 3877, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10602920705621893, "compression_ratio": 1.7435064935064934, "no_speech_prob": 0.0010884996736422181}, {"id": 20, "seek": 8794, "start": 94.75999999999999, "end": 98.72, "text": " And this is the architecture, and we're going to implement it in the course of this video.", "tokens": [50706, 400, 341, 307, 264, 9482, 11, 293, 321, 434, 516, 281, 4445, 309, 294, 264, 1164, 295, 341, 960, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10602920705621893, "compression_ratio": 1.7435064935064934, "no_speech_prob": 0.0010884996736422181}, {"id": 21, "seek": 8794, "start": 99.03999999999999, "end": 104.25999999999999, "text": " So let's get started. So the starter code for part five is very similar to where we ended up in", "tokens": [50920, 407, 718, 311, 483, 1409, 13, 407, 264, 22465, 3089, 337, 644, 1732, 307, 588, 2531, 281, 689, 321, 4590, 493, 294, 51181], "temperature": 0.0, "avg_logprob": -0.10602920705621893, "compression_ratio": 1.7435064935064934, "no_speech_prob": 0.0010884996736422181}, {"id": 22, "seek": 8794, "start": 104.25999999999999, "end": 109.18, "text": " in part three. Recall that part four was the manual dot propagation exercise. That is kind", "tokens": [51181, 294, 644, 1045, 13, 9647, 336, 300, 644, 1451, 390, 264, 9688, 5893, 38377, 5380, 13, 663, 307, 733, 51427], "temperature": 0.0, "avg_logprob": -0.10602920705621893, "compression_ratio": 1.7435064935064934, "no_speech_prob": 0.0010884996736422181}, {"id": 23, "seek": 8794, "start": 109.18, "end": 114.12, "text": " of an aside. So we are coming back to part three, copy-pasting chunks out of it. And that is our", "tokens": [51427, 295, 364, 7359, 13, 407, 321, 366, 1348, 646, 281, 644, 1045, 11, 5055, 12, 79, 30587, 24004, 484, 295, 309, 13, 400, 300, 307, 527, 51674], "temperature": 0.0, "avg_logprob": -0.10602920705621893, "compression_ratio": 1.7435064935064934, "no_speech_prob": 0.0010884996736422181}, {"id": 24, "seek": 8794, "start": 114.12, "end": 116.92, "text": " starter code for part five. I've changed very few things otherwise.", "tokens": [51674, 22465, 3089, 337, 644, 1732, 13, 286, 600, 3105, 588, 1326, 721, 5911, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10602920705621893, "compression_ratio": 1.7435064935064934, "no_speech_prob": 0.0010884996736422181}, {"id": 25, "seek": 11794, "start": 117.94, "end": 122.39999999999999, "text": " However, both should look familiar to if you've gone through part three. So in particular, very", "tokens": [50365, 2908, 11, 1293, 820, 574, 4963, 281, 498, 291, 600, 2780, 807, 644, 1045, 13, 407, 294, 1729, 11, 588, 50588], "temperature": 1.0, "avg_logprob": -1.8819697769233463, "compression_ratio": 1.8512720156555773, "no_speech_prob": 0.0012290746672078967}, {"id": 26, "seek": 11794, "start": 122.39999999999999, "end": 128.88, "text": " briefly, we are doing imports. We are reading our data set of words. And we are processing the", "tokens": [50588, 10515, 11, 321, 366, 884, 41596, 13, 492, 366, 3760, 527, 1412, 992, 295, 2283, 13, 400, 321, 366, 9007, 264, 50912], "temperature": 1.0, "avg_logprob": -1.8819697769233463, "compression_ratio": 1.8512720156555773, "no_speech_prob": 0.0012290746672078967}, {"id": 27, "seek": 11794, "start": 128.88, "end": 134.04, "text": " dataset of words into individual examples. And none of this data generation code has changed.", "tokens": [50912, 28872, 295, 2283, 666, 2609, 5110, 13, 400, 6022, 295, 341, 1412, 5125, 3089, 575, 3105, 13, 51170], "temperature": 1.0, "avg_logprob": -1.8819697769233463, "compression_ratio": 1.8512720156555773, "no_speech_prob": 0.0012290746672078967}, {"id": 28, "seek": 11794, "start": 134.04, "end": 140.06, "text": " And basically we have lots and lots of examples. In particular, we have 182,000 examples", "tokens": [51170, 400, 1936, 321, 362, 3195, 293, 3195, 295, 5110, 13, 682, 1729, 11, 321, 362, 2443, 17, 11, 1360, 5110, 51471], "temperature": 1.0, "avg_logprob": -1.8819697769233463, "compression_ratio": 1.8512720156555773, "no_speech_prob": 0.0012290746672078967}, {"id": 29, "seek": 11794, "start": 140.06, "end": 146.2, "text": " of three characters try to predict the fourth one. And we've broken up every one of these words into", "tokens": [51471, 295, 1045, 4342, 853, 281, 6069, 264, 6409, 472, 13, 400, 321, 600, 5463, 493, 633, 472, 295, 613, 2283, 666, 51778], "temperature": 1.0, "avg_logprob": -1.8819697769233463, "compression_ratio": 1.8512720156555773, "no_speech_prob": 0.0012290746672078967}, {"id": 30, "seek": 11794, "start": 146.2, "end": 147.06, "text": " little problems.", "tokens": [51778, 707, 2740, 13, 51821], "temperature": 1.0, "avg_logprob": -1.8819697769233463, "compression_ratio": 1.8512720156555773, "no_speech_prob": 0.0012290746672078967}, {"id": 31, "seek": 14706, "start": 147.06, "end": 150.66, "text": " given three characters predict the fourth one so this is our data set and this is what we're", "tokens": [50365, 2212, 1045, 4342, 6069, 264, 6409, 472, 370, 341, 307, 527, 1412, 992, 293, 341, 307, 437, 321, 434, 50545], "temperature": 0.0, "avg_logprob": -0.05454708028722693, "compression_ratio": 1.8932806324110671, "no_speech_prob": 0.08488231152296066}, {"id": 32, "seek": 14706, "start": 150.66, "end": 157.22, "text": " trying to get the neural net to do now in part three we started to develop our code around these", "tokens": [50545, 1382, 281, 483, 264, 18161, 2533, 281, 360, 586, 294, 644, 1045, 321, 1409, 281, 1499, 527, 3089, 926, 613, 50873], "temperature": 0.0, "avg_logprob": -0.05454708028722693, "compression_ratio": 1.8932806324110671, "no_speech_prob": 0.08488231152296066}, {"id": 33, "seek": 14706, "start": 157.22, "end": 163.22, "text": " layer modules that are for example a class linear and we're doing this because we want to think of", "tokens": [50873, 4583, 16679, 300, 366, 337, 1365, 257, 1508, 8213, 293, 321, 434, 884, 341, 570, 321, 528, 281, 519, 295, 51173], "temperature": 0.0, "avg_logprob": -0.05454708028722693, "compression_ratio": 1.8932806324110671, "no_speech_prob": 0.08488231152296066}, {"id": 34, "seek": 14706, "start": 163.22, "end": 168.98000000000002, "text": " these modules as building blocks and like a lego building block bricks that we can sort of like", "tokens": [51173, 613, 16679, 382, 2390, 8474, 293, 411, 257, 476, 1571, 2390, 3461, 25497, 300, 321, 393, 1333, 295, 411, 51461], "temperature": 0.0, "avg_logprob": -0.05454708028722693, "compression_ratio": 1.8932806324110671, "no_speech_prob": 0.08488231152296066}, {"id": 35, "seek": 14706, "start": 168.98000000000002, "end": 174.26, "text": " stack up into neural networks and we can feed data between these layers and stack them up into", "tokens": [51461, 8630, 493, 666, 18161, 9590, 293, 321, 393, 3154, 1412, 1296, 613, 7914, 293, 8630, 552, 493, 666, 51725], "temperature": 0.0, "avg_logprob": -0.05454708028722693, "compression_ratio": 1.8932806324110671, "no_speech_prob": 0.08488231152296066}, {"id": 36, "seek": 17426, "start": 174.26, "end": 181.7, "text": " sort of graphs now we also developed these layers to have apis and signatures very similar to those", "tokens": [50365, 1333, 295, 24877, 586, 321, 611, 4743, 613, 7914, 281, 362, 1882, 271, 293, 32322, 588, 2531, 281, 729, 50737], "temperature": 0.0, "avg_logprob": -0.036924854914347334, "compression_ratio": 1.9612403100775193, "no_speech_prob": 7.404189818771556e-05}, {"id": 37, "seek": 17426, "start": 181.7, "end": 186.73999999999998, "text": " that are found in pytorch so we have torch.nn and it's got all these layer building blocks that you", "tokens": [50737, 300, 366, 1352, 294, 25878, 284, 339, 370, 321, 362, 27822, 13, 26384, 293, 309, 311, 658, 439, 613, 4583, 2390, 8474, 300, 291, 50989], "temperature": 0.0, "avg_logprob": -0.036924854914347334, "compression_ratio": 1.9612403100775193, "no_speech_prob": 7.404189818771556e-05}, {"id": 38, "seek": 17426, "start": 186.73999999999998, "end": 191.94, "text": " would use in practice and we were developing all these to mimic the apis of these so for example", "tokens": [50989, 576, 764, 294, 3124, 293, 321, 645, 6416, 439, 613, 281, 31075, 264, 1882, 271, 295, 613, 370, 337, 1365, 51249], "temperature": 0.0, "avg_logprob": -0.036924854914347334, "compression_ratio": 1.9612403100775193, "no_speech_prob": 7.404189818771556e-05}, {"id": 39, "seek": 17426, "start": 191.94, "end": 197.78, "text": " we have linear so there will also be a torch.nn.linear and its signature will be very", "tokens": [51249, 321, 362, 8213, 370, 456, 486, 611, 312, 257, 27822, 13, 26384, 13, 28263, 293, 1080, 13397, 486, 312, 588, 51541], "temperature": 0.0, "avg_logprob": -0.036924854914347334, "compression_ratio": 1.9612403100775193, "no_speech_prob": 7.404189818771556e-05}, {"id": 40, "seek": 17426, "start": 197.78, "end": 202.57999999999998, "text": " similar to our signature and the functionality will be also quite identical as far as i'm aware", "tokens": [51541, 2531, 281, 527, 13397, 293, 264, 14980, 486, 312, 611, 1596, 14800, 382, 1400, 382, 741, 478, 3650, 51781], "temperature": 0.0, "avg_logprob": -0.036924854914347334, "compression_ratio": 1.9612403100775193, "no_speech_prob": 7.404189818771556e-05}, {"id": 41, "seek": 17426, "start": 202.57999999999998, "end": 203.62, "text": " so we have the linear layer", "tokens": [51781, 370, 321, 362, 264, 8213, 4583, 51833], "temperature": 0.0, "avg_logprob": -0.036924854914347334, "compression_ratio": 1.9612403100775193, "no_speech_prob": 7.404189818771556e-05}, {"id": 42, "seek": 20426, "start": 204.26, "end": 210.73999999999998, "text": " with the batchnorm1d layer and the 10h layer that we developed previously and linear just does a", "tokens": [50365, 365, 264, 15245, 13403, 16, 67, 4583, 293, 264, 1266, 71, 4583, 300, 321, 4743, 8046, 293, 8213, 445, 775, 257, 50689], "temperature": 0.0, "avg_logprob": -0.12109156882408822, "compression_ratio": 1.86328125, "no_speech_prob": 0.00018939068831969053}, {"id": 43, "seek": 20426, "start": 210.73999999999998, "end": 216.5, "text": " matrix multiply in the forward pass of this module batchnorm of course is this crazy layer that we", "tokens": [50689, 8141, 12972, 294, 264, 2128, 1320, 295, 341, 10088, 15245, 13403, 295, 1164, 307, 341, 3219, 4583, 300, 321, 50977], "temperature": 0.0, "avg_logprob": -0.12109156882408822, "compression_ratio": 1.86328125, "no_speech_prob": 0.00018939068831969053}, {"id": 44, "seek": 20426, "start": 216.5, "end": 221.29999999999998, "text": " developed in the previous lecture and what's crazy about it is well there's many things", "tokens": [50977, 4743, 294, 264, 3894, 7991, 293, 437, 311, 3219, 466, 309, 307, 731, 456, 311, 867, 721, 51217], "temperature": 0.0, "avg_logprob": -0.12109156882408822, "compression_ratio": 1.86328125, "no_speech_prob": 0.00018939068831969053}, {"id": 45, "seek": 20426, "start": 222.01999999999998, "end": 227.22, "text": " number one it has these running mean and variances that are trained outside of back propagation they", "tokens": [51253, 1230, 472, 309, 575, 613, 2614, 914, 293, 1374, 21518, 300, 366, 8895, 2380, 295, 646, 38377, 436, 51513], "temperature": 0.0, "avg_logprob": -0.12109156882408822, "compression_ratio": 1.86328125, "no_speech_prob": 0.00018939068831969053}, {"id": 46, "seek": 20426, "start": 227.22, "end": 233.54, "text": " are trained using exponential moving average inside this layer when we call the forward pass", "tokens": [51513, 366, 8895, 1228, 21510, 2684, 4274, 1854, 341, 4583, 562, 321, 818, 264, 2128, 1320, 51829], "temperature": 0.0, "avg_logprob": -0.12109156882408822, "compression_ratio": 1.86328125, "no_speech_prob": 0.00018939068831969053}, {"id": 47, "seek": 23426, "start": 234.42, "end": 239.45999999999998, "text": " uh in addition to that there's this training flag because the behavior of bastion is different", "tokens": [50373, 2232, 294, 4500, 281, 300, 456, 311, 341, 3097, 7166, 570, 264, 5223, 295, 8414, 313, 307, 819, 50625], "temperature": 0.0, "avg_logprob": -0.11305313267983681, "compression_ratio": 1.972318339100346, "no_speech_prob": 0.0004525042022578418}, {"id": 48, "seek": 23426, "start": 239.45999999999998, "end": 243.38, "text": " during train time and evaluation time and so suddenly we have to be very careful that", "tokens": [50625, 1830, 3847, 565, 293, 13344, 565, 293, 370, 5800, 321, 362, 281, 312, 588, 5026, 300, 50821], "temperature": 0.0, "avg_logprob": -0.11305313267983681, "compression_ratio": 1.972318339100346, "no_speech_prob": 0.0004525042022578418}, {"id": 49, "seek": 23426, "start": 243.38, "end": 247.85999999999999, "text": " bastion is in its correct state that it's in the evaluation state or training state so that's", "tokens": [50821, 8414, 313, 307, 294, 1080, 3006, 1785, 300, 309, 311, 294, 264, 13344, 1785, 420, 3097, 1785, 370, 300, 311, 51045], "temperature": 0.0, "avg_logprob": -0.11305313267983681, "compression_ratio": 1.972318339100346, "no_speech_prob": 0.0004525042022578418}, {"id": 50, "seek": 23426, "start": 247.85999999999999, "end": 252.73999999999998, "text": " something to now keep track of something that sometimes introduces bugs because you forget to", "tokens": [51045, 746, 281, 586, 1066, 2837, 295, 746, 300, 2171, 31472, 15120, 570, 291, 2870, 281, 51289], "temperature": 0.0, "avg_logprob": -0.11305313267983681, "compression_ratio": 1.972318339100346, "no_speech_prob": 0.0004525042022578418}, {"id": 51, "seek": 23426, "start": 252.73999999999998, "end": 258.02, "text": " put it into the right mode and finally we saw that bastion couples the statistics or the the", "tokens": [51289, 829, 309, 666, 264, 558, 4391, 293, 2721, 321, 1866, 300, 8414, 313, 20368, 264, 12523, 420, 264, 264, 51553], "temperature": 0.0, "avg_logprob": -0.11305313267983681, "compression_ratio": 1.972318339100346, "no_speech_prob": 0.0004525042022578418}, {"id": 52, "seek": 23426, "start": 258.02, "end": 264.02, "text": " activations across the examples in the batch so normally we thought of the batch as just an efficiency thing", "tokens": [51553, 2430, 763, 2108, 264, 5110, 294, 264, 15245, 370, 5646, 321, 1194, 295, 264, 15245, 382, 445, 364, 10493, 551, 51853], "temperature": 0.0, "avg_logprob": -0.11305313267983681, "compression_ratio": 1.972318339100346, "no_speech_prob": 0.0004525042022578418}, {"id": 53, "seek": 26426, "start": 264.82, "end": 270.9, "text": " but now we are coupling the computation across batch elements and it's done for the purposes of", "tokens": [50393, 457, 586, 321, 366, 37447, 264, 24903, 2108, 15245, 4959, 293, 309, 311, 1096, 337, 264, 9932, 295, 50697], "temperature": 0.0, "avg_logprob": -0.05794272912996952, "compression_ratio": 1.83203125, "no_speech_prob": 0.0003698360233101994}, {"id": 54, "seek": 26426, "start": 270.9, "end": 276.34, "text": " controlling the activation statistics as we saw in the previous video so it's a very weird layer", "tokens": [50697, 14905, 264, 24433, 12523, 382, 321, 1866, 294, 264, 3894, 960, 370, 309, 311, 257, 588, 3657, 4583, 50969], "temperature": 0.0, "avg_logprob": -0.05794272912996952, "compression_ratio": 1.83203125, "no_speech_prob": 0.0003698360233101994}, {"id": 55, "seek": 26426, "start": 276.34, "end": 281.3, "text": " at least a lot of bugs um partly for example because you have to modulate the training and", "tokens": [50969, 412, 1935, 257, 688, 295, 15120, 1105, 17031, 337, 1365, 570, 291, 362, 281, 1072, 5256, 264, 3097, 293, 51217], "temperature": 0.0, "avg_logprob": -0.05794272912996952, "compression_ratio": 1.83203125, "no_speech_prob": 0.0003698360233101994}, {"id": 56, "seek": 26426, "start": 281.3, "end": 288.74, "text": " eval phase and so on um in addition for example you have to wait for uh the mean and the variance", "tokens": [51217, 1073, 304, 5574, 293, 370, 322, 1105, 294, 4500, 337, 1365, 291, 362, 281, 1699, 337, 2232, 264, 914, 293, 264, 21977, 51589], "temperature": 0.0, "avg_logprob": -0.05794272912996952, "compression_ratio": 1.83203125, "no_speech_prob": 0.0003698360233101994}, {"id": 57, "seek": 26426, "start": 288.74, "end": 293.78, "text": " to settle and to actually reach a steady state and so um you have to make sure that you", "tokens": [51589, 281, 11852, 293, 281, 767, 2524, 257, 13211, 1785, 293, 370, 1105, 291, 362, 281, 652, 988, 300, 291, 51841], "temperature": 0.0, "avg_logprob": -0.05794272912996952, "compression_ratio": 1.83203125, "no_speech_prob": 0.0003698360233101994}, {"id": 58, "seek": 29426, "start": 294.26, "end": 301.53999999999996, "text": " if you write the problem invalid you can more simply to you know drag and drop um", "tokens": [50365, 498, 291, 2464, 264, 1154, 34702, 291, 393, 544, 2935, 281, 291, 458, 5286, 293, 3270, 1105, 50729], "temperature": 1.0, "avg_logprob": -3.5066165924072266, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.00028736895183101296}, {"id": 59, "seek": 29426, "start": 302.26, "end": 322.9, "text": " to get whatever range you want uh and i want to show you the behavior of bastion", "tokens": [50765, 281, 483, 2035, 3613, 291, 528, 2232, 293, 741, 528, 281, 855, 291, 264, 5223, 295, 8414, 313, 51797], "temperature": 1.0, "avg_logprob": -3.5066165924072266, "compression_ratio": 1.4336283185840708, "no_speech_prob": 0.00028736895183101296}, {"id": 60, "seek": 32426, "start": 324.26, "end": 325.94, "text": " And then we have a list of layers.", "tokens": [50365, 400, 550, 321, 362, 257, 1329, 295, 7914, 13, 50449], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 61, "seek": 32426, "start": 325.94, "end": 329.84, "text": " And it's a linear, feeds to BatchNorm, feeds to 10H,", "tokens": [50449, 400, 309, 311, 257, 8213, 11, 23712, 281, 363, 852, 45, 687, 11, 23712, 281, 1266, 39, 11, 50644], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 62, "seek": 32426, "start": 329.84, "end": 331.78, "text": " and then a linear output layer.", "tokens": [50644, 293, 550, 257, 8213, 5598, 4583, 13, 50741], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 63, "seek": 32426, "start": 331.78, "end": 333.2, "text": " And its weights are scaled down,", "tokens": [50741, 400, 1080, 17443, 366, 36039, 760, 11, 50812], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 64, "seek": 32426, "start": 333.2, "end": 336.71999999999997, "text": " so we are not confidently wrong at initialization.", "tokens": [50812, 370, 321, 366, 406, 41956, 2085, 412, 5883, 2144, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 65, "seek": 32426, "start": 336.71999999999997, "end": 339.26, "text": " We see that this is about 12,000 parameters.", "tokens": [50988, 492, 536, 300, 341, 307, 466, 2272, 11, 1360, 9834, 13, 51115], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 66, "seek": 32426, "start": 339.26, "end": 342.86, "text": " We're telling PyTorch that the parameters require gradients.", "tokens": [51115, 492, 434, 3585, 9953, 51, 284, 339, 300, 264, 9834, 3651, 2771, 2448, 13, 51295], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 67, "seek": 32426, "start": 342.86, "end": 345.7, "text": " The optimization is, as far as I'm aware, identical,", "tokens": [51295, 440, 19618, 307, 11, 382, 1400, 382, 286, 478, 3650, 11, 14800, 11, 51437], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 68, "seek": 32426, "start": 345.7, "end": 347.94, "text": " and should look very, very familiar.", "tokens": [51437, 293, 820, 574, 588, 11, 588, 4963, 13, 51549], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 69, "seek": 32426, "start": 347.94, "end": 348.98, "text": " Nothing changed here.", "tokens": [51549, 6693, 3105, 510, 13, 51601], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 70, "seek": 32426, "start": 350.0, "end": 352.58, "text": " Loss function looks very crazy.", "tokens": [51652, 441, 772, 2445, 1542, 588, 3219, 13, 51781], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 71, "seek": 32426, "start": 352.58, "end": 354.02, "text": " We should probably fix this.", "tokens": [51781, 492, 820, 1391, 3191, 341, 13, 51853], "temperature": 0.0, "avg_logprob": -0.14527244567871095, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.005985003896057606}, {"id": 72, "seek": 35402, "start": 354.02, "end": 357.28, "text": " And that's because 32 batch elements are too few.", "tokens": [50365, 400, 300, 311, 570, 8858, 15245, 4959, 366, 886, 1326, 13, 50528], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 73, "seek": 35402, "start": 357.28, "end": 359.88, "text": " And so you can get very lucky or unlucky", "tokens": [50528, 400, 370, 291, 393, 483, 588, 6356, 420, 38838, 50658], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 74, "seek": 35402, "start": 359.88, "end": 361.2, "text": " in any one of these batches,", "tokens": [50658, 294, 604, 472, 295, 613, 15245, 279, 11, 50724], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 75, "seek": 35402, "start": 361.2, "end": 364.24, "text": " and it creates a very thick loss function.", "tokens": [50724, 293, 309, 7829, 257, 588, 5060, 4470, 2445, 13, 50876], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 76, "seek": 35402, "start": 364.24, "end": 366.44, "text": " So we're gonna fix that soon.", "tokens": [50876, 407, 321, 434, 799, 3191, 300, 2321, 13, 50986], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 77, "seek": 35402, "start": 366.44, "end": 369.24, "text": " Now, once we want to evaluate the trained neural network,", "tokens": [50986, 823, 11, 1564, 321, 528, 281, 13059, 264, 8895, 18161, 3209, 11, 51126], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 78, "seek": 35402, "start": 369.24, "end": 371.4, "text": " we need to remember, because of the BatchNorm layers,", "tokens": [51126, 321, 643, 281, 1604, 11, 570, 295, 264, 363, 852, 45, 687, 7914, 11, 51234], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 79, "seek": 35402, "start": 371.4, "end": 374.35999999999996, "text": " to set all the layers to be training equals false.", "tokens": [51234, 281, 992, 439, 264, 7914, 281, 312, 3097, 6915, 7908, 13, 51382], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 80, "seek": 35402, "start": 374.35999999999996, "end": 377.24, "text": " This only matters for the BatchNorm layer so far.", "tokens": [51382, 639, 787, 7001, 337, 264, 363, 852, 45, 687, 4583, 370, 1400, 13, 51526], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 81, "seek": 35402, "start": 377.24, "end": 378.85999999999996, "text": " And then we evaluate.", "tokens": [51526, 400, 550, 321, 13059, 13, 51607], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 82, "seek": 35402, "start": 380.34, "end": 383.74, "text": " We see that currently we have validation loss of 2.10,", "tokens": [51681, 492, 536, 300, 4362, 321, 362, 24071, 4470, 295, 568, 13, 3279, 11, 51851], "temperature": 0.0, "avg_logprob": -0.09658604642770587, "compression_ratio": 1.6678200692041523, "no_speech_prob": 1.7802816728362814e-05}, {"id": 83, "seek": 38374, "start": 383.74, "end": 387.2, "text": " which is fairly good, but there's still a ways to go.", "tokens": [50365, 597, 307, 6457, 665, 11, 457, 456, 311, 920, 257, 2098, 281, 352, 13, 50538], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 84, "seek": 38374, "start": 387.2, "end": 390.5, "text": " But even at 2.10, we see that when we sample from the model,", "tokens": [50538, 583, 754, 412, 568, 13, 3279, 11, 321, 536, 300, 562, 321, 6889, 490, 264, 2316, 11, 50703], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 85, "seek": 38374, "start": 390.5, "end": 393.38, "text": " we actually get relatively name-like results", "tokens": [50703, 321, 767, 483, 7226, 1315, 12, 4092, 3542, 50847], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 86, "seek": 38374, "start": 393.38, "end": 395.1, "text": " that do not exist in a training set.", "tokens": [50847, 300, 360, 406, 2514, 294, 257, 3097, 992, 13, 50933], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 87, "seek": 38374, "start": 395.1, "end": 400.1, "text": " So for example, Yvonne, Kilo, Pros, Alaya, et cetera.", "tokens": [50933, 407, 337, 1365, 11, 398, 85, 22419, 11, 591, 10720, 11, 26024, 11, 967, 4427, 11, 1030, 11458, 13, 51183], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 88, "seek": 38374, "start": 401.6, "end": 406.2, "text": " So certainly not reasonable, not unreasonable, I would say,", "tokens": [51258, 407, 3297, 406, 10585, 11, 406, 41730, 11, 286, 576, 584, 11, 51488], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 89, "seek": 38374, "start": 406.2, "end": 407.32, "text": " but not amazing.", "tokens": [51488, 457, 406, 2243, 13, 51544], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 90, "seek": 38374, "start": 407.32, "end": 409.76, "text": " And we can still push this validation loss even lower", "tokens": [51544, 400, 321, 393, 920, 2944, 341, 24071, 4470, 754, 3126, 51666], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 91, "seek": 38374, "start": 409.76, "end": 412.86, "text": " and get much better samples that are even more name-like.", "tokens": [51666, 293, 483, 709, 1101, 10938, 300, 366, 754, 544, 1315, 12, 4092, 13, 51821], "temperature": 0.0, "avg_logprob": -0.12918972778320312, "compression_ratio": 1.602189781021898, "no_speech_prob": 0.0003741051477845758}, {"id": 92, "seek": 41374, "start": 413.74, "end": 416.64, "text": " So let's improve this model now.", "tokens": [50365, 407, 718, 311, 3470, 341, 2316, 586, 13, 50510], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 93, "seek": 41374, "start": 416.64, "end": 418.2, "text": " Okay, first, let's fix this graph,", "tokens": [50510, 1033, 11, 700, 11, 718, 311, 3191, 341, 4295, 11, 50588], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 94, "seek": 41374, "start": 418.2, "end": 419.7, "text": " because it is daggers in my eyes,", "tokens": [50588, 570, 309, 307, 15460, 9458, 294, 452, 2575, 11, 50663], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 95, "seek": 41374, "start": 419.7, "end": 422.0, "text": " and I just can't take it anymore.", "tokens": [50663, 293, 286, 445, 393, 380, 747, 309, 3602, 13, 50778], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 96, "seek": 41374, "start": 422.0, "end": 426.90000000000003, "text": " So lossI, if you recall, is a Python list of floats.", "tokens": [50778, 407, 4470, 40, 11, 498, 291, 9901, 11, 307, 257, 15329, 1329, 295, 37878, 13, 51023], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 97, "seek": 41374, "start": 426.90000000000003, "end": 430.02, "text": " So for example, the first 10 elements look like this.", "tokens": [51023, 407, 337, 1365, 11, 264, 700, 1266, 4959, 574, 411, 341, 13, 51179], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 98, "seek": 41374, "start": 431.06, "end": 432.34000000000003, "text": " Now, what we'd like to do, basically,", "tokens": [51231, 823, 11, 437, 321, 1116, 411, 281, 360, 11, 1936, 11, 51295], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 99, "seek": 41374, "start": 432.34000000000003, "end": 435.18, "text": " is we need to average up some of these values", "tokens": [51295, 307, 321, 643, 281, 4274, 493, 512, 295, 613, 4190, 51437], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 100, "seek": 41374, "start": 435.18, "end": 439.78000000000003, "text": " to get a more sort of representative value along the way.", "tokens": [51437, 281, 483, 257, 544, 1333, 295, 12424, 2158, 2051, 264, 636, 13, 51667], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 101, "seek": 41374, "start": 439.78000000000003, "end": 441.86, "text": " So one way to do this is the following.", "tokens": [51667, 407, 472, 636, 281, 360, 341, 307, 264, 3480, 13, 51771], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 102, "seek": 41374, "start": 441.86, "end": 443.04, "text": " In PyTorch, if I create, for example, this,", "tokens": [51771, 682, 9953, 51, 284, 339, 11, 498, 286, 1884, 11, 337, 1365, 11, 341, 11, 51830], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 103, "seek": 41374, "start": 443.04, "end": 443.68, "text": " I'm gonna do this.", "tokens": [51830, 286, 478, 799, 360, 341, 13, 51862], "temperature": 0.0, "avg_logprob": -0.24114511527267157, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.00017087826563511044}, {"id": 104, "seek": 44368, "start": 443.68, "end": 447.48, "text": " If I create, for example, a tensor of the first 10 numbers,", "tokens": [50365, 759, 286, 1884, 11, 337, 1365, 11, 257, 40863, 295, 264, 700, 1266, 3547, 11, 50555], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 105, "seek": 44368, "start": 447.48, "end": 449.92, "text": " then this is currently a one-dimensional array.", "tokens": [50555, 550, 341, 307, 4362, 257, 472, 12, 18759, 10225, 13, 50677], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 106, "seek": 44368, "start": 449.92, "end": 452.84000000000003, "text": " But recall that I can view this array as two-dimensional.", "tokens": [50677, 583, 9901, 300, 286, 393, 1910, 341, 10225, 382, 732, 12, 18759, 13, 50823], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 107, "seek": 44368, "start": 452.84000000000003, "end": 455.8, "text": " So for example, I can view it as a two-by-five array,", "tokens": [50823, 407, 337, 1365, 11, 286, 393, 1910, 309, 382, 257, 732, 12, 2322, 12, 18621, 10225, 11, 50971], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 108, "seek": 44368, "start": 455.8, "end": 459.14, "text": " and this is a 2D tensor now, two-by-five.", "tokens": [50971, 293, 341, 307, 257, 568, 35, 40863, 586, 11, 732, 12, 2322, 12, 18621, 13, 51138], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 109, "seek": 44368, "start": 459.14, "end": 461.46000000000004, "text": " And you see what PyTorch has done is that the first row", "tokens": [51138, 400, 291, 536, 437, 9953, 51, 284, 339, 575, 1096, 307, 300, 264, 700, 5386, 51254], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 110, "seek": 44368, "start": 461.46000000000004, "end": 463.8, "text": " of this tensor is the first five elements,", "tokens": [51254, 295, 341, 40863, 307, 264, 700, 1732, 4959, 11, 51371], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 111, "seek": 44368, "start": 463.8, "end": 466.84000000000003, "text": " and the second row is the second five elements.", "tokens": [51371, 293, 264, 1150, 5386, 307, 264, 1150, 1732, 4959, 13, 51523], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 112, "seek": 44368, "start": 466.84000000000003, "end": 470.18, "text": " I can also view it as a five-by-two as an example.", "tokens": [51523, 286, 393, 611, 1910, 309, 382, 257, 1732, 12, 2322, 12, 20534, 382, 364, 1365, 13, 51690], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 113, "seek": 44368, "start": 470.18, "end": 473.68, "text": " And then recall that I can also use negative one", "tokens": [51690, 400, 550, 9901, 300, 286, 393, 611, 764, 3671, 472, 51865], "temperature": 0.0, "avg_logprob": -0.1395157441398166, "compression_ratio": 1.9766536964980546, "no_speech_prob": 0.001014150446280837}, {"id": 114, "seek": 47368, "start": 473.68, "end": 475.92, "text": " in place of one of these numbers.", "tokens": [50365, 294, 1081, 295, 472, 295, 613, 3547, 13, 50477], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 115, "seek": 47368, "start": 475.92, "end": 478.64, "text": " And PyTorch will calculate what that number must be", "tokens": [50477, 400, 9953, 51, 284, 339, 486, 8873, 437, 300, 1230, 1633, 312, 50613], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 116, "seek": 47368, "start": 478.64, "end": 481.08, "text": " in order to make the number of elements work out.", "tokens": [50613, 294, 1668, 281, 652, 264, 1230, 295, 4959, 589, 484, 13, 50735], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 117, "seek": 47368, "start": 481.08, "end": 484.72, "text": " So this can be this, or like that.", "tokens": [50735, 407, 341, 393, 312, 341, 11, 420, 411, 300, 13, 50917], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 118, "seek": 47368, "start": 484.72, "end": 485.52, "text": " Both will work.", "tokens": [50917, 6767, 486, 589, 13, 50957], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 119, "seek": 47368, "start": 485.52, "end": 486.72, "text": " Of course, this would not work.", "tokens": [50957, 2720, 1164, 11, 341, 576, 406, 589, 13, 51017], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 120, "seek": 47368, "start": 489.22, "end": 492.68, "text": " OK, so this allows it to spread out some of the consecutive values", "tokens": [51142, 2264, 11, 370, 341, 4045, 309, 281, 3974, 484, 512, 295, 264, 30497, 4190, 51315], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 121, "seek": 47368, "start": 492.68, "end": 493.72, "text": " into rows.", "tokens": [51315, 666, 13241, 13, 51367], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 122, "seek": 47368, "start": 493.72, "end": 495.72, "text": " So that's very helpful, because what we can do now", "tokens": [51367, 407, 300, 311, 588, 4961, 11, 570, 437, 321, 393, 360, 586, 51467], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 123, "seek": 47368, "start": 495.72, "end": 499.6, "text": " is, first of all, we're going to create a Torch.tensor out", "tokens": [51467, 307, 11, 700, 295, 439, 11, 321, 434, 516, 281, 1884, 257, 7160, 339, 13, 83, 23153, 484, 51661], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 124, "seek": 47368, "start": 499.6, "end": 502.48, "text": " of the list of floats.", "tokens": [51661, 295, 264, 1329, 295, 37878, 13, 51805], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 125, "seek": 47368, "start": 502.48, "end": 503.52, "text": " And then we're going to view it.", "tokens": [51805, 400, 550, 321, 434, 516, 281, 1910, 309, 13, 51857], "temperature": 0.0, "avg_logprob": -0.21585582165007897, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0003839844430331141}, {"id": 126, "seek": 50352, "start": 503.52, "end": 506.52, "text": " As whatever it is, but we're going", "tokens": [50365, 1018, 2035, 309, 307, 11, 457, 321, 434, 516, 50515], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 127, "seek": 50352, "start": 506.52, "end": 510.64, "text": " to stretch it out into rows of 1,000 consecutive elements.", "tokens": [50515, 281, 5985, 309, 484, 666, 13241, 295, 502, 11, 1360, 30497, 4959, 13, 50721], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 128, "seek": 50352, "start": 510.64, "end": 514.56, "text": " So the shape of this now becomes 200 by 1,000.", "tokens": [50721, 407, 264, 3909, 295, 341, 586, 3643, 2331, 538, 502, 11, 1360, 13, 50917], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 129, "seek": 50352, "start": 514.56, "end": 519.52, "text": " And each row is 1,000 consecutive elements in this list.", "tokens": [50917, 400, 1184, 5386, 307, 502, 11, 1360, 30497, 4959, 294, 341, 1329, 13, 51165], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 130, "seek": 50352, "start": 519.52, "end": 521.02, "text": " So that's very helpful, because now we", "tokens": [51165, 407, 300, 311, 588, 4961, 11, 570, 586, 321, 51240], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 131, "seek": 50352, "start": 521.02, "end": 524.04, "text": " can do a mean along the rows.", "tokens": [51240, 393, 360, 257, 914, 2051, 264, 13241, 13, 51391], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 132, "seek": 50352, "start": 524.04, "end": 527.24, "text": " And the shape of this will just be 200.", "tokens": [51391, 400, 264, 3909, 295, 341, 486, 445, 312, 2331, 13, 51551], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 133, "seek": 50352, "start": 527.24, "end": 529.84, "text": " And so we've taken basically the mean on every row.", "tokens": [51551, 400, 370, 321, 600, 2726, 1936, 264, 914, 322, 633, 5386, 13, 51681], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 134, "seek": 50352, "start": 529.84, "end": 533.06, "text": " So plt.plot of that should be something nicer.", "tokens": [51681, 407, 499, 83, 13, 564, 310, 295, 300, 820, 312, 746, 22842, 13, 51842], "temperature": 0.0, "avg_logprob": -0.18536181449890138, "compression_ratio": 1.7234042553191489, "no_speech_prob": 3.0527615308528766e-05}, {"id": 135, "seek": 53306, "start": 533.06, "end": 535.18, "text": " Much better.", "tokens": [50365, 12313, 1101, 13, 50471], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 136, "seek": 53306, "start": 535.18, "end": 537.7399999999999, "text": " So we see that we've basically made a lot of progress.", "tokens": [50471, 407, 321, 536, 300, 321, 600, 1936, 1027, 257, 688, 295, 4205, 13, 50599], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 137, "seek": 53306, "start": 537.7399999999999, "end": 540.9, "text": " And then here, this is the learning rate decay.", "tokens": [50599, 400, 550, 510, 11, 341, 307, 264, 2539, 3314, 21039, 13, 50757], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 138, "seek": 53306, "start": 540.9, "end": 543.18, "text": " So here we see that the learning rate decay subtracted", "tokens": [50757, 407, 510, 321, 536, 300, 264, 2539, 3314, 21039, 16390, 292, 50871], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 139, "seek": 53306, "start": 543.18, "end": 544.9, "text": " a ton of energy out of the system,", "tokens": [50871, 257, 2952, 295, 2281, 484, 295, 264, 1185, 11, 50957], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 140, "seek": 53306, "start": 544.9, "end": 547.8199999999999, "text": " and allowed us to settle into the local minimum", "tokens": [50957, 293, 4350, 505, 281, 11852, 666, 264, 2654, 7285, 51103], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 141, "seek": 53306, "start": 547.8199999999999, "end": 549.5, "text": " in this optimization.", "tokens": [51103, 294, 341, 19618, 13, 51187], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 142, "seek": 53306, "start": 549.5, "end": 551.42, "text": " So this is a much nicer plot.", "tokens": [51187, 407, 341, 307, 257, 709, 22842, 7542, 13, 51283], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 143, "seek": 53306, "start": 551.42, "end": 554.68, "text": " Let me come up and delete the monster.", "tokens": [51283, 961, 385, 808, 493, 293, 12097, 264, 10090, 13, 51446], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 144, "seek": 53306, "start": 554.68, "end": 556.6999999999999, "text": " And we're going to be using this going forward.", "tokens": [51446, 400, 321, 434, 516, 281, 312, 1228, 341, 516, 2128, 13, 51547], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 145, "seek": 53306, "start": 556.6999999999999, "end": 558.2399999999999, "text": " Now, next up, what I'm bothered by", "tokens": [51547, 823, 11, 958, 493, 11, 437, 286, 478, 22996, 538, 51624], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 146, "seek": 53306, "start": 558.2399999999999, "end": 561.18, "text": " is that you see our forward pass is a little bit gnarly, and takes a lot of time.", "tokens": [51624, 307, 300, 291, 536, 527, 2128, 1320, 307, 257, 707, 857, 290, 20062, 356, 11, 293, 2516, 257, 688, 295, 565, 13, 51771], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 147, "seek": 53306, "start": 561.18, "end": 561.9399999999999, "text": " So we're going to go ahead and do that.", "tokens": [51771, 407, 321, 434, 516, 281, 352, 2286, 293, 360, 300, 13, 51809], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 148, "seek": 53306, "start": 561.9399999999999, "end": 562.06, "text": " And we're going to go ahead and do that.", "tokens": [51809, 400, 321, 434, 516, 281, 352, 2286, 293, 360, 300, 13, 51815], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 149, "seek": 53306, "start": 562.06, "end": 563.06, "text": " And we're going to go ahead and do that.", "tokens": [51815, 400, 321, 434, 516, 281, 352, 2286, 293, 360, 300, 13, 51865], "temperature": 0.0, "avg_logprob": -0.3183212487593941, "compression_ratio": 2.0289389067524115, "no_speech_prob": 0.0004487347323447466}, {"id": 150, "seek": 56306, "start": 563.06, "end": 564.7399999999999, "text": " And we're going to explain too many lines of code.", "tokens": [50365, 400, 321, 434, 516, 281, 2903, 886, 867, 3876, 295, 3089, 13, 50449], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 151, "seek": 56306, "start": 564.7399999999999, "end": 567.38, "text": " So in particular, we see that we've organized some of the layers", "tokens": [50449, 407, 294, 1729, 11, 321, 536, 300, 321, 600, 9983, 512, 295, 264, 7914, 50581], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 152, "seek": 56306, "start": 567.38, "end": 571.9, "text": " inside the layers list, but not all of them for no reason.", "tokens": [50581, 1854, 264, 7914, 1329, 11, 457, 406, 439, 295, 552, 337, 572, 1778, 13, 50807], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 153, "seek": 56306, "start": 571.9, "end": 574.78, "text": " So in particular, we see that we still have the embedding table special", "tokens": [50807, 407, 294, 1729, 11, 321, 536, 300, 321, 920, 362, 264, 12240, 3584, 3199, 2121, 50951], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 154, "seek": 56306, "start": 574.78, "end": 577.02, "text": " cased outside of the layers.", "tokens": [50951, 269, 1937, 2380, 295, 264, 7914, 13, 51063], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 155, "seek": 56306, "start": 577.02, "end": 579.78, "text": " And in addition to that, the viewing operation here", "tokens": [51063, 400, 294, 4500, 281, 300, 11, 264, 17480, 6916, 510, 51201], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 156, "seek": 56306, "start": 579.78, "end": 581.68, "text": " is also outside of our layers.", "tokens": [51201, 307, 611, 2380, 295, 527, 7914, 13, 51296], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 157, "seek": 56306, "start": 581.68, "end": 583.64, "text": " Let's create layers for these, and then we", "tokens": [51296, 961, 311, 1884, 7914, 337, 613, 11, 293, 550, 321, 51394], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 158, "seek": 56306, "start": 583.64, "end": 586.5999999999999, "text": " can add those layers to just our list.", "tokens": [51394, 393, 909, 729, 7914, 281, 445, 527, 1329, 13, 51542], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 159, "seek": 56306, "start": 586.5999999999999, "end": 589.54, "text": " So in particular, the two things that we need is here,", "tokens": [51542, 407, 294, 1729, 11, 264, 732, 721, 300, 321, 643, 307, 510, 11, 51689], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 160, "seek": 56306, "start": 589.54, "end": 592.5799999999999, "text": " we have this embedding table, and we are indexing", "tokens": [51689, 321, 362, 341, 12240, 3584, 3199, 11, 293, 321, 366, 8186, 278, 51841], "temperature": 0.0, "avg_logprob": -0.15880434204931973, "compression_ratio": 1.981818181818182, "no_speech_prob": 0.0008399095386266708}, {"id": 161, "seek": 59258, "start": 592.58, "end": 598.7, "text": " at the integers inside the batch xb, inside the tensor xb.", "tokens": [50365, 412, 264, 41674, 1854, 264, 15245, 2031, 65, 11, 1854, 264, 40863, 2031, 65, 13, 50671], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 162, "seek": 59258, "start": 598.7, "end": 602.58, "text": " So that's an embedding table lookup just done with indexing.", "tokens": [50671, 407, 300, 311, 364, 12240, 3584, 3199, 574, 1010, 445, 1096, 365, 8186, 278, 13, 50865], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 163, "seek": 59258, "start": 602.58, "end": 604.7, "text": " And then here, we see that we have this view operation,", "tokens": [50865, 400, 550, 510, 11, 321, 536, 300, 321, 362, 341, 1910, 6916, 11, 50971], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 164, "seek": 59258, "start": 604.7, "end": 606.62, "text": " which if you recall from the previous video,", "tokens": [50971, 597, 498, 291, 9901, 490, 264, 3894, 960, 11, 51067], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 165, "seek": 59258, "start": 606.62, "end": 611.0, "text": " simply rearranges the character embeddings", "tokens": [51067, 2935, 29875, 10350, 264, 2517, 12240, 29432, 51286], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 166, "seek": 59258, "start": 611.0, "end": 613.08, "text": " and stretches them out into a row.", "tokens": [51286, 293, 29058, 552, 484, 666, 257, 5386, 13, 51390], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 167, "seek": 59258, "start": 613.08, "end": 616.4200000000001, "text": " And effectively, what that does is the concatenation operation,", "tokens": [51390, 400, 8659, 11, 437, 300, 775, 307, 264, 1588, 7186, 399, 6916, 11, 51557], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 168, "seek": 59258, "start": 616.4200000000001, "end": 619.1, "text": " basically, except it's free because viewing", "tokens": [51557, 1936, 11, 3993, 309, 311, 1737, 570, 17480, 51691], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 169, "seek": 59258, "start": 619.1, "end": 621.1800000000001, "text": " is very cheap in PyTorch.", "tokens": [51691, 307, 588, 7084, 294, 9953, 51, 284, 339, 13, 51795], "temperature": 0.0, "avg_logprob": -0.15016417218069744, "compression_ratio": 1.667953667953668, "no_speech_prob": 0.0010041320929303765}, {"id": 170, "seek": 62118, "start": 621.18, "end": 623.42, "text": " And no memory is being copied.", "tokens": [50365, 400, 572, 4675, 307, 885, 25365, 13, 50477], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 171, "seek": 62118, "start": 623.42, "end": 626.04, "text": " We're just re-representing how we view that tensor.", "tokens": [50477, 492, 434, 445, 319, 12, 19919, 11662, 278, 577, 321, 1910, 300, 40863, 13, 50608], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 172, "seek": 62118, "start": 626.04, "end": 630.78, "text": " So let's create modules for both of these operations,", "tokens": [50608, 407, 718, 311, 1884, 16679, 337, 1293, 295, 613, 7705, 11, 50845], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 173, "seek": 62118, "start": 630.78, "end": 634.06, "text": " the embedding operation and the flattening operation.", "tokens": [50845, 264, 12240, 3584, 6916, 293, 264, 24183, 278, 6916, 13, 51009], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 174, "seek": 62118, "start": 634.06, "end": 638.8599999999999, "text": " So I actually wrote the code just to save some time.", "tokens": [51009, 407, 286, 767, 4114, 264, 3089, 445, 281, 3155, 512, 565, 13, 51249], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 175, "seek": 62118, "start": 638.8599999999999, "end": 641.8599999999999, "text": " So we have a module embedding and a module flatten.", "tokens": [51249, 407, 321, 362, 257, 10088, 12240, 3584, 293, 257, 10088, 24183, 13, 51399], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 176, "seek": 62118, "start": 641.8599999999999, "end": 644.88, "text": " And both of them simply do the indexing operation", "tokens": [51399, 400, 1293, 295, 552, 2935, 360, 264, 8186, 278, 6916, 51550], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 177, "seek": 62118, "start": 644.88, "end": 649.62, "text": " in a forward pass and the flattening operation here.", "tokens": [51550, 294, 257, 2128, 1320, 293, 264, 24183, 278, 6916, 510, 13, 51787], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 178, "seek": 62118, "start": 649.62, "end": 651.06, "text": " And this.", "tokens": [51787, 400, 341, 13, 51859], "temperature": 0.0, "avg_logprob": -0.1757852046861561, "compression_ratio": 1.8630136986301369, "no_speech_prob": 9.086811041925102e-05}, {"id": 179, "seek": 65118, "start": 651.18, "end": 656.88, "text": " C now will just become a self.weight inside an embedding module.", "tokens": [50365, 383, 586, 486, 445, 1813, 257, 2698, 13, 12329, 1854, 364, 12240, 3584, 10088, 13, 50650], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 180, "seek": 65118, "start": 656.88, "end": 659.76, "text": " And I'm calling these layers specifically embedding and flatten", "tokens": [50650, 400, 286, 478, 5141, 613, 7914, 4682, 12240, 3584, 293, 24183, 50794], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 181, "seek": 65118, "start": 659.76, "end": 663.0799999999999, "text": " because it turns out that both of them actually exist in PyTorch.", "tokens": [50794, 570, 309, 4523, 484, 300, 1293, 295, 552, 767, 2514, 294, 9953, 51, 284, 339, 13, 50960], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 182, "seek": 65118, "start": 663.0799999999999, "end": 665.8199999999999, "text": " So in PyTorch, we have n and dot embedding.", "tokens": [50960, 407, 294, 9953, 51, 284, 339, 11, 321, 362, 297, 293, 5893, 12240, 3584, 13, 51097], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 183, "seek": 65118, "start": 665.8199999999999, "end": 667.26, "text": " And it also takes the number of embeddings", "tokens": [51097, 400, 309, 611, 2516, 264, 1230, 295, 12240, 29432, 51169], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 184, "seek": 65118, "start": 667.26, "end": 670.9599999999999, "text": " and the dimensionality of the embedding, just like we have here.", "tokens": [51169, 293, 264, 10139, 1860, 295, 264, 12240, 3584, 11, 445, 411, 321, 362, 510, 13, 51354], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 185, "seek": 65118, "start": 670.9599999999999, "end": 673.3399999999999, "text": " But in addition, PyTorch takes in a lot of other keyword arguments", "tokens": [51354, 583, 294, 4500, 11, 9953, 51, 284, 339, 2516, 294, 257, 688, 295, 661, 20428, 12869, 51473], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 186, "seek": 65118, "start": 673.3399999999999, "end": 678.0799999999999, "text": " that we are not using for our purposes yet.", "tokens": [51473, 300, 321, 366, 406, 1228, 337, 527, 9932, 1939, 13, 51710], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 187, "seek": 65118, "start": 678.0799999999999, "end": 680.7199999999999, "text": " And for flatten, that also exists in PyTorch.", "tokens": [51710, 400, 337, 24183, 11, 300, 611, 8198, 294, 9953, 51, 284, 339, 13, 51842], "temperature": 0.2, "avg_logprob": -0.15293510981968472, "compression_ratio": 1.8093525179856116, "no_speech_prob": 0.0007246207096613944}, {"id": 188, "seek": 68072, "start": 680.72, "end": 683.84, "text": " But it also takes additional keyword arguments that we are not using.", "tokens": [50365, 583, 309, 611, 2516, 4497, 20428, 12869, 300, 321, 366, 406, 1228, 13, 50521], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 189, "seek": 68072, "start": 683.84, "end": 686.86, "text": " So we have a very simple flatten.", "tokens": [50521, 407, 321, 362, 257, 588, 2199, 24183, 13, 50672], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 190, "seek": 68072, "start": 686.86, "end": 687.86, "text": " But both of them exist in PyTorch.", "tokens": [50672, 583, 1293, 295, 552, 2514, 294, 9953, 51, 284, 339, 13, 50722], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 191, "seek": 68072, "start": 687.86, "end": 690.1800000000001, "text": " They're just a bit more simpler.", "tokens": [50722, 814, 434, 445, 257, 857, 544, 18587, 13, 50838], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 192, "seek": 68072, "start": 690.1800000000001, "end": 698.5, "text": " And now that we have these, we can simply take out some of these special cased things.", "tokens": [50838, 400, 586, 300, 321, 362, 613, 11, 321, 393, 2935, 747, 484, 512, 295, 613, 2121, 269, 1937, 721, 13, 51254], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 193, "seek": 68072, "start": 698.5, "end": 705.82, "text": " So instead of C, we're just going to have an embedding and vocab size and n embed.", "tokens": [51254, 407, 2602, 295, 383, 11, 321, 434, 445, 516, 281, 362, 364, 12240, 3584, 293, 2329, 455, 2744, 293, 297, 12240, 13, 51620], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 194, "seek": 68072, "start": 705.82, "end": 709.12, "text": " And then after the embedding, we are going to flatten.", "tokens": [51620, 400, 550, 934, 264, 12240, 3584, 11, 321, 366, 516, 281, 24183, 13, 51785], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 195, "seek": 68072, "start": 709.12, "end": 710.62, "text": " So let's construct those modules.", "tokens": [51785, 407, 718, 311, 7690, 729, 16679, 13, 51860], "temperature": 0.0, "avg_logprob": -0.12669886260473429, "compression_ratio": 1.6862745098039216, "no_speech_prob": 5.86288224440068e-05}, {"id": 196, "seek": 71072, "start": 710.72, "end": 713.3000000000001, "text": " And now I can take out this C.", "tokens": [50365, 400, 586, 286, 393, 747, 484, 341, 383, 13, 50494], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 197, "seek": 71072, "start": 713.3000000000001, "end": 715.6, "text": " And here, I don't have to special case it anymore.", "tokens": [50494, 400, 510, 11, 286, 500, 380, 362, 281, 2121, 1389, 309, 3602, 13, 50609], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 198, "seek": 71072, "start": 715.6, "end": 719.1800000000001, "text": " Because now, C is the embedding's weight.", "tokens": [50609, 1436, 586, 11, 383, 307, 264, 12240, 3584, 311, 3364, 13, 50788], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 199, "seek": 71072, "start": 719.1800000000001, "end": 722.0600000000001, "text": " And it's inside layers.", "tokens": [50788, 400, 309, 311, 1854, 7914, 13, 50932], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 200, "seek": 71072, "start": 722.0600000000001, "end": 724.34, "text": " So this should just work.", "tokens": [50932, 407, 341, 820, 445, 589, 13, 51046], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 201, "seek": 71072, "start": 724.34, "end": 727.96, "text": " And then here, our forward pass simplifies substantially.", "tokens": [51046, 400, 550, 510, 11, 527, 2128, 1320, 6883, 11221, 30797, 13, 51227], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 202, "seek": 71072, "start": 727.96, "end": 733.46, "text": " Because we don't need to do these now outside of these layers, outside and explicitly.", "tokens": [51227, 1436, 321, 500, 380, 643, 281, 360, 613, 586, 2380, 295, 613, 7914, 11, 2380, 293, 20803, 13, 51502], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 203, "seek": 71072, "start": 733.46, "end": 735.36, "text": " They're now inside layers.", "tokens": [51502, 814, 434, 586, 1854, 7914, 13, 51597], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 204, "seek": 71072, "start": 735.36, "end": 737.32, "text": " So we can delete those.", "tokens": [51597, 407, 321, 393, 12097, 729, 13, 51695], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 205, "seek": 71072, "start": 737.32, "end": 740.6, "text": " But now to kick things off, we want this little x,", "tokens": [51695, 583, 586, 281, 4437, 721, 766, 11, 321, 528, 341, 707, 2031, 11, 51859], "temperature": 0.0, "avg_logprob": -0.15030080621892755, "compression_ratio": 1.6733067729083666, "no_speech_prob": 0.0002786644035950303}, {"id": 206, "seek": 74060, "start": 740.6, "end": 743.1, "text": " which in the beginning is just xb,", "tokens": [50365, 597, 294, 264, 2863, 307, 445, 2031, 65, 11, 50490], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 207, "seek": 74060, "start": 743.1, "end": 747.9200000000001, "text": " the tensor of integers specifying the identities of these characters at the input.", "tokens": [50490, 264, 40863, 295, 41674, 1608, 5489, 264, 24239, 295, 613, 4342, 412, 264, 4846, 13, 50731], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 208, "seek": 74060, "start": 747.9200000000001, "end": 751.1, "text": " And so these characters can now directly feed into the first layer.", "tokens": [50731, 400, 370, 613, 4342, 393, 586, 3838, 3154, 666, 264, 700, 4583, 13, 50890], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 209, "seek": 74060, "start": 751.1, "end": 752.74, "text": " And this should just work.", "tokens": [50890, 400, 341, 820, 445, 589, 13, 50972], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 210, "seek": 74060, "start": 752.74, "end": 755.48, "text": " So let me come here and insert a break.", "tokens": [50972, 407, 718, 385, 808, 510, 293, 8969, 257, 1821, 13, 51109], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 211, "seek": 74060, "start": 755.48, "end": 757.98, "text": " Because I just want to make sure that the first iteration of this runs,", "tokens": [51109, 1436, 286, 445, 528, 281, 652, 988, 300, 264, 700, 24784, 295, 341, 6676, 11, 51234], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 212, "seek": 74060, "start": 757.98, "end": 759.84, "text": " and that there's no mistake.", "tokens": [51234, 293, 300, 456, 311, 572, 6146, 13, 51327], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 213, "seek": 74060, "start": 759.84, "end": 761.72, "text": " So that ran properly.", "tokens": [51327, 407, 300, 5872, 6108, 13, 51421], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 214, "seek": 74060, "start": 761.72, "end": 764.84, "text": " And basically, we've substantially simplified the forward pass here.", "tokens": [51421, 400, 1936, 11, 321, 600, 30797, 26335, 264, 2128, 1320, 510, 13, 51577], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 215, "seek": 74060, "start": 764.84, "end": 766.74, "text": " Okay, I'm sorry, I changed my microphone.", "tokens": [51577, 1033, 11, 286, 478, 2597, 11, 286, 3105, 452, 10952, 13, 51672], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 216, "seek": 74060, "start": 766.74, "end": 769.72, "text": " So hopefully, the audio is a little bit better.", "tokens": [51672, 407, 4696, 11, 264, 6278, 307, 257, 707, 857, 1101, 13, 51821], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 217, "seek": 74060, "start": 769.72, "end": 770.48, "text": " Now, one last thing.", "tokens": [51821, 823, 11, 472, 1036, 551, 13, 51859], "temperature": 0.2, "avg_logprob": -0.19360786594756663, "compression_ratio": 1.7076923076923076, "no_speech_prob": 0.0010892428690567613}, {"id": 218, "seek": 77048, "start": 770.48, "end": 774.1, "text": " One more thing that I would like to do in order to PyTorchify our code even further,", "tokens": [50365, 1485, 544, 551, 300, 286, 576, 411, 281, 360, 294, 1668, 281, 9953, 51, 284, 339, 2505, 527, 3089, 754, 3052, 11, 50546], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 219, "seek": 77048, "start": 774.1, "end": 778.44, "text": " is that right now we are maintaining all of our modules in a naked list of layers.", "tokens": [50546, 307, 300, 558, 586, 321, 366, 14916, 439, 295, 527, 16679, 294, 257, 15791, 1329, 295, 7914, 13, 50763], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 220, "seek": 77048, "start": 778.44, "end": 784.1800000000001, "text": " And we can also simplify this, because we can introduce the concept of PyTorch containers.", "tokens": [50763, 400, 321, 393, 611, 20460, 341, 11, 570, 321, 393, 5366, 264, 3410, 295, 9953, 51, 284, 339, 17089, 13, 51050], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 221, "seek": 77048, "start": 784.1800000000001, "end": 787.48, "text": " So in torch.nn, which we are basically rebuilding from scratch here,", "tokens": [51050, 407, 294, 27822, 13, 26384, 11, 597, 321, 366, 1936, 36717, 490, 8459, 510, 11, 51215], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 222, "seek": 77048, "start": 787.48, "end": 789.34, "text": " there's a concept of containers.", "tokens": [51215, 456, 311, 257, 3410, 295, 17089, 13, 51308], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 223, "seek": 77048, "start": 789.34, "end": 794.86, "text": " And these containers are basically a way of organizing layers into lists or dicts and", "tokens": [51308, 400, 613, 17089, 366, 1936, 257, 636, 295, 17608, 7914, 666, 14511, 420, 12569, 82, 293, 51584], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 224, "seek": 77048, "start": 794.86, "end": 795.86, "text": " so on.", "tokens": [51584, 370, 322, 13, 51634], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 225, "seek": 77048, "start": 795.86, "end": 800.48, "text": " So in particular, there's a sequential, which maintains a list of layers, and there's a", "tokens": [51634, 407, 294, 1729, 11, 456, 311, 257, 42881, 11, 597, 33385, 257, 1329, 295, 7914, 11, 293, 456, 311, 257, 51865], "temperature": 0.0, "avg_logprob": -0.12182456859643909, "compression_ratio": 1.8719723183391004, "no_speech_prob": 0.001059922855347395}, {"id": 226, "seek": 80048, "start": 800.48, "end": 802.76, "text": " module class in PyTorch.", "tokens": [50365, 10088, 1508, 294, 9953, 51, 284, 339, 13, 50479], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 227, "seek": 80048, "start": 802.76, "end": 806.94, "text": " And it basically just passes a given input through all the layers sequentially, exactly", "tokens": [50479, 400, 309, 1936, 445, 11335, 257, 2212, 4846, 807, 439, 264, 7914, 5123, 3137, 11, 2293, 50688], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 228, "seek": 80048, "start": 806.94, "end": 808.98, "text": " as we are doing here.", "tokens": [50688, 382, 321, 366, 884, 510, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 229, "seek": 80048, "start": 808.98, "end": 811.2, "text": " So let's write our own sequential.", "tokens": [50790, 407, 718, 311, 2464, 527, 1065, 42881, 13, 50901], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 230, "seek": 80048, "start": 811.2, "end": 812.84, "text": " I've written a code here.", "tokens": [50901, 286, 600, 3720, 257, 3089, 510, 13, 50983], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 231, "seek": 80048, "start": 812.84, "end": 816.0, "text": " And basically, the code for sequential is quite straightforward.", "tokens": [50983, 400, 1936, 11, 264, 3089, 337, 42881, 307, 1596, 15325, 13, 51141], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 232, "seek": 80048, "start": 816.0, "end": 819.14, "text": " We pass in a list of layers, which we keep here.", "tokens": [51141, 492, 1320, 294, 257, 1329, 295, 7914, 11, 597, 321, 1066, 510, 13, 51298], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 233, "seek": 80048, "start": 819.14, "end": 823.1800000000001, "text": " And then given any input in a forward pass, we just call all the layers sequentially and", "tokens": [51298, 400, 550, 2212, 604, 4846, 294, 257, 2128, 1320, 11, 321, 445, 818, 439, 264, 7914, 5123, 3137, 293, 51500], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 234, "seek": 80048, "start": 823.1800000000001, "end": 824.1800000000001, "text": " return the result.", "tokens": [51500, 2736, 264, 1874, 13, 51550], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 235, "seek": 80048, "start": 824.1800000000001, "end": 828.4, "text": " And in terms of the parameters, it's just all the parameters of the child modules.", "tokens": [51550, 400, 294, 2115, 295, 264, 9834, 11, 309, 311, 445, 439, 264, 9834, 295, 264, 1440, 16679, 13, 51761], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 236, "seek": 80048, "start": 828.4, "end": 829.6, "text": " So we can run this.", "tokens": [51761, 407, 321, 393, 1190, 341, 13, 51821], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 237, "seek": 80048, "start": 829.6, "end": 830.1, "text": " Okay.", "tokens": [51821, 1033, 13, 51846], "temperature": 0.0, "avg_logprob": -0.12553923280089052, "compression_ratio": 1.8785714285714286, "no_speech_prob": 0.0010220694821327925}, {"id": 238, "seek": 83048, "start": 830.48, "end": 834.52, "text": " And again, simplify this substantially, because we don't maintain this naked list of layers.", "tokens": [50365, 400, 797, 11, 20460, 341, 30797, 11, 570, 321, 500, 380, 6909, 341, 15791, 1329, 295, 7914, 13, 50567], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 239, "seek": 83048, "start": 834.52, "end": 841.0, "text": " We now have a notion of a model, which is a module, and in particular, is a sequential", "tokens": [50567, 492, 586, 362, 257, 10710, 295, 257, 2316, 11, 597, 307, 257, 10088, 11, 293, 294, 1729, 11, 307, 257, 42881, 50891], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 240, "seek": 83048, "start": 841.0, "end": 845.04, "text": " of all these layers.", "tokens": [50891, 295, 439, 613, 7914, 13, 51093], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 241, "seek": 83048, "start": 845.04, "end": 849.7, "text": " And now parameters are simply just model.parameters.", "tokens": [51093, 400, 586, 9834, 366, 2935, 445, 2316, 13, 2181, 335, 6202, 13, 51326], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 242, "seek": 83048, "start": 849.7, "end": 854.1, "text": " And so that list comprehension now lives here.", "tokens": [51326, 400, 370, 300, 1329, 44991, 586, 2909, 510, 13, 51546], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 243, "seek": 83048, "start": 854.1, "end": 858.1, "text": " And then here we are doing all the things we used to do.", "tokens": [51546, 400, 550, 510, 321, 366, 884, 439, 264, 721, 321, 1143, 281, 360, 13, 51746], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 244, "seek": 83048, "start": 858.1, "end": 859.6, "text": " Now here, the code again simplifies substantially.", "tokens": [51746, 823, 510, 11, 264, 3089, 797, 6883, 11221, 30797, 13, 51821], "temperature": 0.0, "avg_logprob": -0.12660194578624906, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.00034044156200252473}, {"id": 245, "seek": 85960, "start": 859.6, "end": 865.32, "text": " Because we don't have to do this forwarding here, instead we just call the model on the", "tokens": [50365, 1436, 321, 500, 380, 362, 281, 360, 341, 2128, 278, 510, 11, 2602, 321, 445, 818, 264, 2316, 322, 264, 50651], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 246, "seek": 85960, "start": 865.32, "end": 866.32, "text": " input data.", "tokens": [50651, 4846, 1412, 13, 50701], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 247, "seek": 85960, "start": 866.32, "end": 869.6, "text": " And the input data here are the integers inside xb.", "tokens": [50701, 400, 264, 4846, 1412, 510, 366, 264, 41674, 1854, 2031, 65, 13, 50865], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 248, "seek": 85960, "start": 869.6, "end": 873.9, "text": " So we can simply do logits, which are the outputs of our model, are simply the model", "tokens": [50865, 407, 321, 393, 2935, 360, 3565, 1208, 11, 597, 366, 264, 23930, 295, 527, 2316, 11, 366, 2935, 264, 2316, 51080], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 249, "seek": 85960, "start": 873.9, "end": 877.0400000000001, "text": " called on xb.", "tokens": [51080, 1219, 322, 2031, 65, 13, 51237], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 250, "seek": 85960, "start": 877.0400000000001, "end": 881.52, "text": " And then the cross entropy here takes the logits and the targets.", "tokens": [51237, 400, 550, 264, 3278, 30867, 510, 2516, 264, 3565, 1208, 293, 264, 12911, 13, 51461], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 251, "seek": 85960, "start": 881.52, "end": 884.12, "text": " So this simplifies substantially.", "tokens": [51461, 407, 341, 6883, 11221, 30797, 13, 51591], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 252, "seek": 85960, "start": 884.12, "end": 885.84, "text": " And then this looks good.", "tokens": [51591, 400, 550, 341, 1542, 665, 13, 51677], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 253, "seek": 85960, "start": 885.84, "end": 887.44, "text": " So let's just make sure this runs.", "tokens": [51677, 407, 718, 311, 445, 652, 988, 341, 6676, 13, 51757], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 254, "seek": 85960, "start": 887.44, "end": 888.44, "text": " That looks good.", "tokens": [51757, 663, 1542, 665, 13, 51807], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 255, "seek": 85960, "start": 888.44, "end": 889.44, "text": " Okay.", "tokens": [51807, 1033, 13, 51857], "temperature": 0.0, "avg_logprob": -0.1452941131591797, "compression_ratio": 1.786008230452675, "no_speech_prob": 0.0045181214809417725}, {"id": 256, "seek": 88944, "start": 889.44, "end": 893.7600000000001, "text": " Now here, we actually have some work to do still here, but I'm going to come back later.", "tokens": [50365, 823, 510, 11, 321, 767, 362, 512, 589, 281, 360, 920, 510, 11, 457, 286, 478, 516, 281, 808, 646, 1780, 13, 50581], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 257, "seek": 88944, "start": 893.7600000000001, "end": 895.1600000000001, "text": " For now, there's no more layers.", "tokens": [50581, 1171, 586, 11, 456, 311, 572, 544, 7914, 13, 50651], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 258, "seek": 88944, "start": 895.1600000000001, "end": 897.0400000000001, "text": " There's a model that layers.", "tokens": [50651, 821, 311, 257, 2316, 300, 7914, 13, 50745], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 259, "seek": 88944, "start": 897.0400000000001, "end": 900.9000000000001, "text": " But it's naughty to access attributes of these classes directly.", "tokens": [50745, 583, 309, 311, 32154, 281, 2105, 17212, 295, 613, 5359, 3838, 13, 50938], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 260, "seek": 88944, "start": 900.9000000000001, "end": 903.32, "text": " So we'll come back and fix this later.", "tokens": [50938, 407, 321, 603, 808, 646, 293, 3191, 341, 1780, 13, 51059], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 261, "seek": 88944, "start": 903.32, "end": 907.62, "text": " And then here, of course, this simplifies substantially as well, because logits are", "tokens": [51059, 400, 550, 510, 11, 295, 1164, 11, 341, 6883, 11221, 30797, 382, 731, 11, 570, 3565, 1208, 366, 51274], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 262, "seek": 88944, "start": 907.62, "end": 910.84, "text": " the model called on x.", "tokens": [51274, 264, 2316, 1219, 322, 2031, 13, 51435], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 263, "seek": 88944, "start": 910.84, "end": 914.2600000000001, "text": " And then these logits come here.", "tokens": [51435, 400, 550, 613, 3565, 1208, 808, 510, 13, 51606], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 264, "seek": 88944, "start": 914.2600000000001, "end": 918.12, "text": " So we can evaluate the train and validation loss, which currently is terrible, because", "tokens": [51606, 407, 321, 393, 13059, 264, 3847, 293, 24071, 4470, 11, 597, 4362, 307, 6237, 11, 570, 51799], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 265, "seek": 88944, "start": 918.12, "end": 919.2800000000001, "text": " we just initialized it in neural net.", "tokens": [51799, 321, 445, 5883, 1602, 309, 294, 18161, 2533, 13, 51857], "temperature": 0.0, "avg_logprob": -0.12885612378017508, "compression_ratio": 1.7242524916943522, "no_speech_prob": 0.0002296261372976005}, {"id": 266, "seek": 91928, "start": 919.28, "end": 921.8, "text": " And then we can also sample from the model.", "tokens": [50365, 400, 550, 321, 393, 611, 6889, 490, 264, 2316, 13, 50491], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 267, "seek": 91928, "start": 921.8, "end": 927.1, "text": " And this simplifies dramatically as well, because we just want to call the model onto", "tokens": [50491, 400, 341, 6883, 11221, 17548, 382, 731, 11, 570, 321, 445, 528, 281, 818, 264, 2316, 3911, 50756], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 268, "seek": 91928, "start": 927.1, "end": 930.52, "text": " the context and outcome logits.", "tokens": [50756, 264, 4319, 293, 9700, 3565, 1208, 13, 50927], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 269, "seek": 91928, "start": 930.52, "end": 934.6, "text": " And then these logits go into softmax and get the probabilities, et cetera.", "tokens": [50927, 400, 550, 613, 3565, 1208, 352, 666, 2787, 41167, 293, 483, 264, 33783, 11, 1030, 11458, 13, 51131], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 270, "seek": 91928, "start": 934.6, "end": 938.4, "text": " So we can sample from this model.", "tokens": [51131, 407, 321, 393, 6889, 490, 341, 2316, 13, 51321], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 271, "seek": 91928, "start": 938.4, "end": 939.4, "text": " What did I screw up?", "tokens": [51321, 708, 630, 286, 5630, 493, 30, 51371], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 272, "seek": 91928, "start": 939.4, "end": 940.4, "text": " Okay.", "tokens": [51371, 1033, 13, 51421], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 273, "seek": 91928, "start": 940.4, "end": 947.16, "text": " So I fixed the issue and we now get the result that we expect, which is gibberish, because", "tokens": [51421, 407, 286, 6806, 264, 2734, 293, 321, 586, 483, 264, 1874, 300, 321, 2066, 11, 597, 307, 4553, 43189, 11, 570, 51759], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 274, "seek": 91928, "start": 947.16, "end": 948.4399999999999, "text": " the model is not trained.", "tokens": [51759, 264, 2316, 307, 406, 8895, 13, 51823], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 275, "seek": 91928, "start": 948.4399999999999, "end": 949.16, "text": " Okay.", "tokens": [51823, 1033, 13, 51859], "temperature": 0.0, "avg_logprob": -0.18208376027769962, "compression_ratio": 1.7183673469387755, "no_speech_prob": 0.0010600571986287832}, {"id": 276, "seek": 94916, "start": 949.16, "end": 950.8, "text": " So we initialize it from scratch.", "tokens": [50365, 407, 321, 5883, 1125, 309, 490, 8459, 13, 50447], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 277, "seek": 94916, "start": 950.8, "end": 955.0799999999999, "text": " The problem was that when I fixed this cell to be modeled out layers instead of just layers,", "tokens": [50447, 440, 1154, 390, 300, 562, 286, 6806, 341, 2815, 281, 312, 37140, 484, 7914, 2602, 295, 445, 7914, 11, 50661], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 278, "seek": 94916, "start": 955.0799999999999, "end": 957.42, "text": " I did not actually run the cell.", "tokens": [50661, 286, 630, 406, 767, 1190, 264, 2815, 13, 50778], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 279, "seek": 94916, "start": 957.42, "end": 960.4599999999999, "text": " And so our neural net was in a training mode.", "tokens": [50778, 400, 370, 527, 18161, 2533, 390, 294, 257, 3097, 4391, 13, 50930], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 280, "seek": 94916, "start": 960.4599999999999, "end": 964.0, "text": " And what caused the issue here is the batch norm layer, as batch norm layer often likes", "tokens": [50930, 400, 437, 7008, 264, 2734, 510, 307, 264, 15245, 2026, 4583, 11, 382, 15245, 2026, 4583, 2049, 5902, 51107], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 281, "seek": 94916, "start": 964.0, "end": 967.28, "text": " to do, because batch norm was in the training mode.", "tokens": [51107, 281, 360, 11, 570, 15245, 2026, 390, 294, 264, 3097, 4391, 13, 51271], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 282, "seek": 94916, "start": 967.28, "end": 971.52, "text": " And here we are passing in an input, which is a batch of just a single example made up", "tokens": [51271, 400, 510, 321, 366, 8437, 294, 364, 4846, 11, 597, 307, 257, 15245, 295, 445, 257, 2167, 1365, 1027, 493, 51483], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 283, "seek": 94916, "start": 971.52, "end": 973.22, "text": " of the context.", "tokens": [51483, 295, 264, 4319, 13, 51568], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 284, "seek": 94916, "start": 973.22, "end": 976.8399999999999, "text": " And so if you are trying to pass in a single example into a batch norm that is in the training", "tokens": [51568, 400, 370, 498, 291, 366, 1382, 281, 1320, 294, 257, 2167, 1365, 666, 257, 15245, 2026, 300, 307, 294, 264, 3097, 51749], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 285, "seek": 94916, "start": 976.8399999999999, "end": 977.8399999999999, "text": " mode.", "tokens": [51749, 4391, 13, 51799], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 286, "seek": 94916, "start": 977.8399999999999, "end": 978.9599999999999, "text": " You're going to end up estimating the variance.", "tokens": [51799, 509, 434, 516, 281, 917, 493, 8017, 990, 264, 21977, 13, 51855], "temperature": 0.0, "avg_logprob": -0.15793166583097434, "compression_ratio": 1.9383116883116882, "no_speech_prob": 0.002148012863472104}, {"id": 287, "seek": 97896, "start": 978.96, "end": 984.6, "text": " Using the input and the variance of a single number is not a number because it is a measure", "tokens": [50365, 11142, 264, 4846, 293, 264, 21977, 295, 257, 2167, 1230, 307, 406, 257, 1230, 570, 309, 307, 257, 3481, 50647], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 288, "seek": 97896, "start": 984.6, "end": 985.9000000000001, "text": " of a spread.", "tokens": [50647, 295, 257, 3974, 13, 50712], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 289, "seek": 97896, "start": 985.9000000000001, "end": 990.7800000000001, "text": " So for example, the variance of just a single number five, you can see is not a number.", "tokens": [50712, 407, 337, 1365, 11, 264, 21977, 295, 445, 257, 2167, 1230, 1732, 11, 291, 393, 536, 307, 406, 257, 1230, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 290, "seek": 97896, "start": 990.7800000000001, "end": 992.98, "text": " And so that's what happened.", "tokens": [50956, 400, 370, 300, 311, 437, 2011, 13, 51066], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 291, "seek": 97896, "start": 992.98, "end": 995.02, "text": " And batch norm basically caused an issue.", "tokens": [51066, 400, 15245, 2026, 1936, 7008, 364, 2734, 13, 51168], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 292, "seek": 97896, "start": 995.02, "end": 998.1600000000001, "text": " And then that polluted all of the further processing.", "tokens": [51168, 400, 550, 300, 6418, 4866, 439, 295, 264, 3052, 9007, 13, 51325], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 293, "seek": 97896, "start": 998.1600000000001, "end": 1001.36, "text": " So all that we had to do was make sure that this runs.", "tokens": [51325, 407, 439, 300, 321, 632, 281, 360, 390, 652, 988, 300, 341, 6676, 13, 51485], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 294, "seek": 97896, "start": 1001.36, "end": 1006.6600000000001, "text": " And we basically made the issue of, again, we didn't actually see the issue with the", "tokens": [51485, 400, 321, 1936, 1027, 264, 2734, 295, 11, 797, 11, 321, 994, 380, 767, 536, 264, 2734, 365, 264, 51750], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 295, "seek": 97896, "start": 1006.6600000000001, "end": 1007.6600000000001, "text": " loss.", "tokens": [51750, 4470, 13, 51800], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 296, "seek": 97896, "start": 1007.6600000000001, "end": 1008.6600000000001, "text": " We could have evaluated the loss.", "tokens": [51800, 492, 727, 362, 25509, 264, 4470, 13, 51850], "temperature": 0.0, "avg_logprob": -0.14393465142501027, "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00035104263224639}, {"id": 297, "seek": 100896, "start": 1008.96, "end": 1013.46, "text": " We got the wrong result because batch norm was in the training mode.", "tokens": [50365, 492, 658, 264, 2085, 1874, 570, 15245, 2026, 390, 294, 264, 3097, 4391, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 298, "seek": 100896, "start": 1013.46, "end": 1014.46, "text": " And so we still get a result.", "tokens": [50590, 400, 370, 321, 920, 483, 257, 1874, 13, 50640], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 299, "seek": 100896, "start": 1014.46, "end": 1019.5400000000001, "text": " It's just the wrong result because it's using the sample statistics of the batch.", "tokens": [50640, 467, 311, 445, 264, 2085, 1874, 570, 309, 311, 1228, 264, 6889, 12523, 295, 264, 15245, 13, 50894], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 300, "seek": 100896, "start": 1019.5400000000001, "end": 1023.1800000000001, "text": " Whereas we want to use the running mean and running variance inside the batch norm.", "tokens": [50894, 13813, 321, 528, 281, 764, 264, 2614, 914, 293, 2614, 21977, 1854, 264, 15245, 2026, 13, 51076], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 301, "seek": 100896, "start": 1023.1800000000001, "end": 1029.6200000000001, "text": " And so again, an example of introducing a bug in line because we did not properly maintain", "tokens": [51076, 400, 370, 797, 11, 364, 1365, 295, 15424, 257, 7426, 294, 1622, 570, 321, 630, 406, 6108, 6909, 51398], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 302, "seek": 100896, "start": 1029.6200000000001, "end": 1031.4, "text": " the state of what is training or not.", "tokens": [51398, 264, 1785, 295, 437, 307, 3097, 420, 406, 13, 51487], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 303, "seek": 100896, "start": 1031.4, "end": 1032.4, "text": " Okay.", "tokens": [51487, 1033, 13, 51537], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 304, "seek": 100896, "start": 1032.4, "end": 1033.48, "text": " So I rerun everything.", "tokens": [51537, 407, 286, 43819, 409, 1203, 13, 51591], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 305, "seek": 100896, "start": 1033.48, "end": 1034.52, "text": " And here's where we are.", "tokens": [51591, 400, 510, 311, 689, 321, 366, 13, 51643], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 306, "seek": 100896, "start": 1034.52, "end": 1037.8, "text": " As a reminder, we have the training loss of 2.05 and validation of 2.10.", "tokens": [51643, 1018, 257, 13548, 11, 321, 362, 264, 3097, 4470, 295, 568, 13, 13328, 293, 24071, 295, 568, 13, 3279, 13, 51807], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 307, "seek": 100896, "start": 1037.8, "end": 1038.8, "text": " Now, let's go back.", "tokens": [51807, 823, 11, 718, 311, 352, 646, 13, 51857], "temperature": 0.0, "avg_logprob": -0.1866182378820471, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0003820702841039747}, {"id": 308, "seek": 103880, "start": 1038.8, "end": 1042.7, "text": " Now, because these losses are very similar to each other, we have a sense that we are", "tokens": [50365, 823, 11, 570, 613, 15352, 366, 588, 2531, 281, 1184, 661, 11, 321, 362, 257, 2020, 300, 321, 366, 50560], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 309, "seek": 103880, "start": 1042.7, "end": 1045.24, "text": " not overfitting too much on this task.", "tokens": [50560, 406, 670, 69, 2414, 886, 709, 322, 341, 5633, 13, 50687], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 310, "seek": 103880, "start": 1045.24, "end": 1048.8799999999999, "text": " And we can make additional progress in our performance by scaling up the size of the", "tokens": [50687, 400, 321, 393, 652, 4497, 4205, 294, 527, 3389, 538, 21589, 493, 264, 2744, 295, 264, 50869], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 311, "seek": 103880, "start": 1048.8799999999999, "end": 1051.98, "text": " neural network and making everything bigger and deeper.", "tokens": [50869, 18161, 3209, 293, 1455, 1203, 3801, 293, 7731, 13, 51024], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 312, "seek": 103880, "start": 1051.98, "end": 1056.6, "text": " Now, currently, we are using this architecture here, where we are taking in some number of", "tokens": [51024, 823, 11, 4362, 11, 321, 366, 1228, 341, 9482, 510, 11, 689, 321, 366, 1940, 294, 512, 1230, 295, 51255], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 313, "seek": 103880, "start": 1056.6, "end": 1060.04, "text": " characters, going into a single hidden layer, and then going to the prediction of the next", "tokens": [51255, 4342, 11, 516, 666, 257, 2167, 7633, 4583, 11, 293, 550, 516, 281, 264, 17630, 295, 264, 958, 51427], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 314, "seek": 103880, "start": 1060.04, "end": 1061.4199999999998, "text": " character.", "tokens": [51427, 2517, 13, 51496], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 315, "seek": 103880, "start": 1061.4199999999998, "end": 1066.3999999999999, "text": " The problem here is we don't have a naive way of making this bigger in a productive", "tokens": [51496, 440, 1154, 510, 307, 321, 500, 380, 362, 257, 29052, 636, 295, 1455, 341, 3801, 294, 257, 13304, 51745], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 316, "seek": 103880, "start": 1066.3999999999999, "end": 1067.3999999999999, "text": " way.", "tokens": [51745, 636, 13, 51795], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 317, "seek": 103880, "start": 1067.3999999999999, "end": 1068.6, "text": " We could, of course, use our neural network.", "tokens": [51795, 492, 727, 11, 295, 1164, 11, 764, 527, 18161, 3209, 13, 51855], "temperature": 0.0, "avg_logprob": -0.18374760510170296, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0006132963462732732}, {"id": 318, "seek": 106860, "start": 1068.6, "end": 1072.78, "text": " We could use our layers, sort of building blocks and materials to introduce additional", "tokens": [50365, 492, 727, 764, 527, 7914, 11, 1333, 295, 2390, 8474, 293, 5319, 281, 5366, 4497, 50574], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 319, "seek": 106860, "start": 1072.78, "end": 1075.28, "text": " layers here and make the network deeper.", "tokens": [50574, 7914, 510, 293, 652, 264, 3209, 7731, 13, 50699], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 320, "seek": 106860, "start": 1075.28, "end": 1079.4199999999998, "text": " But it is still the case that we are crushing all of the characters into a single layer", "tokens": [50699, 583, 309, 307, 920, 264, 1389, 300, 321, 366, 31317, 439, 295, 264, 4342, 666, 257, 2167, 4583, 50906], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 321, "seek": 106860, "start": 1079.4199999999998, "end": 1081.26, "text": " all the way at the beginning.", "tokens": [50906, 439, 264, 636, 412, 264, 2863, 13, 50998], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 322, "seek": 106860, "start": 1081.26, "end": 1084.98, "text": " And even if we make this a bigger layer and add neurons, it's still kind of like silly", "tokens": [50998, 400, 754, 498, 321, 652, 341, 257, 3801, 4583, 293, 909, 22027, 11, 309, 311, 920, 733, 295, 411, 11774, 51184], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 323, "seek": 106860, "start": 1084.98, "end": 1090.04, "text": " to squash all that information so fast in a single step.", "tokens": [51184, 281, 30725, 439, 300, 1589, 370, 2370, 294, 257, 2167, 1823, 13, 51437], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 324, "seek": 106860, "start": 1090.04, "end": 1093.58, "text": " So what we'd like to do instead is we'd like our network to look a lot more like this in", "tokens": [51437, 407, 437, 321, 1116, 411, 281, 360, 2602, 307, 321, 1116, 411, 527, 3209, 281, 574, 257, 688, 544, 411, 341, 294, 51614], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 325, "seek": 106860, "start": 1093.58, "end": 1094.9599999999998, "text": " the WaveNet case.", "tokens": [51614, 264, 28530, 31890, 1389, 13, 51683], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 326, "seek": 106860, "start": 1094.9599999999998, "end": 1098.2199999999998, "text": " So you see in the WaveNet, when we are trying to make the prediction for the next character", "tokens": [51683, 407, 291, 536, 294, 264, 28530, 31890, 11, 562, 321, 366, 1382, 281, 652, 264, 17630, 337, 264, 958, 2517, 51846], "temperature": 0.0, "avg_logprob": -0.09786748561729379, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0007040555356070399}, {"id": 327, "seek": 109822, "start": 1098.22, "end": 1104.94, "text": " in a sequence, it is a function of the previous characters that feed in, but not all of these", "tokens": [50365, 294, 257, 8310, 11, 309, 307, 257, 2445, 295, 264, 3894, 4342, 300, 3154, 294, 11, 457, 406, 439, 295, 613, 50701], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 328, "seek": 109822, "start": 1104.94, "end": 1110.0, "text": " different characters are not just crushed to a single layer and then you have a sandwich.", "tokens": [50701, 819, 4342, 366, 406, 445, 19889, 281, 257, 2167, 4583, 293, 550, 291, 362, 257, 11141, 13, 50954], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 329, "seek": 109822, "start": 1110.0, "end": 1112.28, "text": " They are crushed slowly.", "tokens": [50954, 814, 366, 19889, 5692, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 330, "seek": 109822, "start": 1112.28, "end": 1117.8, "text": " So in particular, we take two characters and we fuse them into sort of like a bigram representation.", "tokens": [51068, 407, 294, 1729, 11, 321, 747, 732, 4342, 293, 321, 31328, 552, 666, 1333, 295, 411, 257, 955, 2356, 10290, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 331, "seek": 109822, "start": 1117.8, "end": 1120.48, "text": " And we do that for all these characters consecutively.", "tokens": [51344, 400, 321, 360, 300, 337, 439, 613, 4342, 27154, 3413, 13, 51478], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 332, "seek": 109822, "start": 1120.48, "end": 1127.0, "text": " And then we take the bigrams and we fuse those into four character level chunks.", "tokens": [51478, 400, 550, 321, 747, 264, 955, 2356, 82, 293, 321, 31328, 729, 666, 1451, 2517, 1496, 24004, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 333, "seek": 109822, "start": 1127.0, "end": 1128.1000000000001, "text": " And then we fuse that again.", "tokens": [51804, 400, 550, 321, 31328, 300, 797, 13, 51859], "temperature": 0.0, "avg_logprob": -0.1338978145433509, "compression_ratio": 1.8515625, "no_speech_prob": 0.0025954104494303465}, {"id": 334, "seek": 112822, "start": 1128.22, "end": 1132.06, "text": " And so we do that in this tree-like hierarchical manner.", "tokens": [50365, 400, 370, 321, 360, 300, 294, 341, 4230, 12, 4092, 35250, 804, 9060, 13, 50557], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 335, "seek": 112822, "start": 1132.06, "end": 1137.22, "text": " So we fuse the information from the previous context slowly into the network as it gets", "tokens": [50557, 407, 321, 31328, 264, 1589, 490, 264, 3894, 4319, 5692, 666, 264, 3209, 382, 309, 2170, 50815], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 336, "seek": 112822, "start": 1137.22, "end": 1138.22, "text": " deeper.", "tokens": [50815, 7731, 13, 50865], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 337, "seek": 112822, "start": 1138.22, "end": 1141.0, "text": " And so this is the kind of architecture that we want to implement.", "tokens": [50865, 400, 370, 341, 307, 264, 733, 295, 9482, 300, 321, 528, 281, 4445, 13, 51004], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 338, "seek": 112822, "start": 1141.0, "end": 1146.74, "text": " Now in the WaveNet's case, this is a visualization of a stack of dilated causal convolution layers.", "tokens": [51004, 823, 294, 264, 28530, 31890, 311, 1389, 11, 341, 307, 257, 25801, 295, 257, 8630, 295, 11504, 770, 38755, 45216, 7914, 13, 51291], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 339, "seek": 112822, "start": 1146.74, "end": 1150.18, "text": " And this makes it sound very scary, but actually the idea is very simple.", "tokens": [51291, 400, 341, 1669, 309, 1626, 588, 6958, 11, 457, 767, 264, 1558, 307, 588, 2199, 13, 51463], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 340, "seek": 112822, "start": 1150.18, "end": 1154.34, "text": " And the fact that it's a dilated causal convolution layer is really just an implementation detail", "tokens": [51463, 400, 264, 1186, 300, 309, 311, 257, 11504, 770, 38755, 45216, 4583, 307, 534, 445, 364, 11420, 2607, 51671], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 341, "seek": 112822, "start": 1154.34, "end": 1155.66, "text": " to make everything fast.", "tokens": [51671, 281, 652, 1203, 2370, 13, 51737], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 342, "seek": 112822, "start": 1155.66, "end": 1157.22, "text": " We're going to see that later.", "tokens": [51737, 492, 434, 516, 281, 536, 300, 1780, 13, 51815], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 343, "seek": 112822, "start": 1157.22, "end": 1158.22, "text": " But for now, let's just keep going.", "tokens": [51815, 583, 337, 586, 11, 718, 311, 445, 1066, 516, 13, 51865], "temperature": 0.0, "avg_logprob": -0.10827611243888123, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.00015819424879737198}, {"id": 344, "seek": 115822, "start": 1158.22, "end": 1161.78, "text": " We're going to keep the basic idea of it, which is this progressive fusion.", "tokens": [50365, 492, 434, 516, 281, 1066, 264, 3875, 1558, 295, 309, 11, 597, 307, 341, 16131, 23100, 13, 50543], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 345, "seek": 115822, "start": 1161.78, "end": 1166.22, "text": " So we want to make the network deeper, and at each level, we want to fuse only two consecutive", "tokens": [50543, 407, 321, 528, 281, 652, 264, 3209, 7731, 11, 293, 412, 1184, 1496, 11, 321, 528, 281, 31328, 787, 732, 30497, 50765], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 346, "seek": 115822, "start": 1166.22, "end": 1167.26, "text": " elements.", "tokens": [50765, 4959, 13, 50817], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 347, "seek": 115822, "start": 1167.26, "end": 1171.82, "text": " Two characters, then two bigrams, then two fourgrams, and so on.", "tokens": [50817, 4453, 4342, 11, 550, 732, 955, 2356, 82, 11, 550, 732, 1451, 1342, 82, 11, 293, 370, 322, 13, 51045], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 348, "seek": 115822, "start": 1171.82, "end": 1172.82, "text": " So let's implement this.", "tokens": [51045, 407, 718, 311, 4445, 341, 13, 51095], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 349, "seek": 115822, "start": 1172.82, "end": 1176.1000000000001, "text": " Okay, so first up, let me scroll to where we built the dataset, and let's change the", "tokens": [51095, 1033, 11, 370, 700, 493, 11, 718, 385, 11369, 281, 689, 321, 3094, 264, 28872, 11, 293, 718, 311, 1319, 264, 51259], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 350, "seek": 115822, "start": 1176.1000000000001, "end": 1178.52, "text": " block size from three to eight.", "tokens": [51259, 3461, 2744, 490, 1045, 281, 3180, 13, 51380], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 351, "seek": 115822, "start": 1178.52, "end": 1183.3600000000001, "text": " So we're going to be taking eight characters of context to predict the ninth character.", "tokens": [51380, 407, 321, 434, 516, 281, 312, 1940, 3180, 4342, 295, 4319, 281, 6069, 264, 28207, 2517, 13, 51622], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 352, "seek": 115822, "start": 1183.3600000000001, "end": 1185.26, "text": " So the dataset now looks like this.", "tokens": [51622, 407, 264, 28872, 586, 1542, 411, 341, 13, 51717], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 353, "seek": 115822, "start": 1185.26, "end": 1188.22, "text": " We have a lot more context feeding in to predict any next character.", "tokens": [51717, 492, 362, 257, 688, 544, 4319, 12919, 294, 281, 6069, 604, 958, 2517, 13, 51865], "temperature": 0.0, "avg_logprob": -0.13317316342023464, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.004657728597521782}, {"id": 354, "seek": 118822, "start": 1188.22, "end": 1189.22, "text": " So we're going to have a sequence.", "tokens": [50365, 407, 321, 434, 516, 281, 362, 257, 8310, 13, 50415], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 355, "seek": 118822, "start": 1189.22, "end": 1193.66, "text": " And these eight characters are going to be processed in this tree-like structure.", "tokens": [50415, 400, 613, 3180, 4342, 366, 516, 281, 312, 18846, 294, 341, 4230, 12, 4092, 3877, 13, 50637], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 356, "seek": 118822, "start": 1193.66, "end": 1197.66, "text": " Now if we scroll here, everything here should just be able to work.", "tokens": [50637, 823, 498, 321, 11369, 510, 11, 1203, 510, 820, 445, 312, 1075, 281, 589, 13, 50837], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 357, "seek": 118822, "start": 1197.66, "end": 1199.96, "text": " So we should be able to redefine the network.", "tokens": [50837, 407, 321, 820, 312, 1075, 281, 38818, 533, 264, 3209, 13, 50952], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 358, "seek": 118822, "start": 1199.96, "end": 1204.46, "text": " You see that the number of parameters has increased by 10,000, and that's because the", "tokens": [50952, 509, 536, 300, 264, 1230, 295, 9834, 575, 6505, 538, 1266, 11, 1360, 11, 293, 300, 311, 570, 264, 51177], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 359, "seek": 118822, "start": 1204.46, "end": 1205.72, "text": " block size has grown.", "tokens": [51177, 3461, 2744, 575, 7709, 13, 51240], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 360, "seek": 118822, "start": 1205.72, "end": 1208.14, "text": " So this first linear layer is much, much bigger.", "tokens": [51240, 407, 341, 700, 8213, 4583, 307, 709, 11, 709, 3801, 13, 51361], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 361, "seek": 118822, "start": 1208.14, "end": 1212.74, "text": " Our linear layer now takes eight characters into this middle layer.", "tokens": [51361, 2621, 8213, 4583, 586, 2516, 3180, 4342, 666, 341, 2808, 4583, 13, 51591], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 362, "seek": 118822, "start": 1212.74, "end": 1214.5, "text": " So there's a lot more parameters there.", "tokens": [51591, 407, 456, 311, 257, 688, 544, 9834, 456, 13, 51679], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 363, "seek": 118822, "start": 1214.5, "end": 1216.26, "text": " But this should just run.", "tokens": [51679, 583, 341, 820, 445, 1190, 13, 51767], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 364, "seek": 118822, "start": 1216.26, "end": 1218.14, "text": " Let me just break right after this.", "tokens": [51767, 961, 385, 445, 1821, 558, 934, 341, 13, 51861], "temperature": 0.0, "avg_logprob": -0.17153212448646282, "compression_ratio": 1.7967741935483872, "no_speech_prob": 0.003372554201632738}, {"id": 365, "seek": 121814, "start": 1218.14, "end": 1219.14, "text": " This is the very first iteration.", "tokens": [50365, 639, 307, 264, 588, 700, 24784, 13, 50415], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 366, "seek": 121814, "start": 1219.14, "end": 1221.5, "text": " So you see that this runs just fine.", "tokens": [50415, 407, 291, 536, 300, 341, 6676, 445, 2489, 13, 50533], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 367, "seek": 121814, "start": 1221.5, "end": 1223.5800000000002, "text": " It's just that this network doesn't make too much sense.", "tokens": [50533, 467, 311, 445, 300, 341, 3209, 1177, 380, 652, 886, 709, 2020, 13, 50637], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 368, "seek": 121814, "start": 1223.5800000000002, "end": 1227.14, "text": " We're crushing way too much information way too fast.", "tokens": [50637, 492, 434, 31317, 636, 886, 709, 1589, 636, 886, 2370, 13, 50815], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 369, "seek": 121814, "start": 1227.14, "end": 1232.1200000000001, "text": " So let's now come in and see how we could try to implement the hierarchical scheme.", "tokens": [50815, 407, 718, 311, 586, 808, 294, 293, 536, 577, 321, 727, 853, 281, 4445, 264, 35250, 804, 12232, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 370, "seek": 121814, "start": 1232.1200000000001, "end": 1235.8000000000002, "text": " Now before we dive into the detail of the re-implementation here, I was just curious", "tokens": [51064, 823, 949, 321, 9192, 666, 264, 2607, 295, 264, 319, 12, 332, 781, 19631, 510, 11, 286, 390, 445, 6369, 51248], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 371, "seek": 121814, "start": 1235.8000000000002, "end": 1239.64, "text": " to actually run it and see where we are in terms of the baseline performance of just", "tokens": [51248, 281, 767, 1190, 309, 293, 536, 689, 321, 366, 294, 2115, 295, 264, 20518, 3389, 295, 445, 51440], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 372, "seek": 121814, "start": 1239.64, "end": 1242.5, "text": " lazily scaling up the context length.", "tokens": [51440, 19320, 953, 21589, 493, 264, 4319, 4641, 13, 51583], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 373, "seek": 121814, "start": 1242.5, "end": 1243.5, "text": " So I let it run.", "tokens": [51583, 407, 286, 718, 309, 1190, 13, 51633], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 374, "seek": 121814, "start": 1243.5, "end": 1245.0, "text": " We get a nice loss curve.", "tokens": [51633, 492, 483, 257, 1481, 4470, 7605, 13, 51708], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 375, "seek": 121814, "start": 1245.0, "end": 1248.0400000000002, "text": " And then evaluating the loss, we actually see quite a bit of improvement.", "tokens": [51708, 400, 550, 27479, 264, 4470, 11, 321, 767, 536, 1596, 257, 857, 295, 10444, 13, 51860], "temperature": 0.0, "avg_logprob": -0.13151397705078124, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0002745213860180229}, {"id": 376, "seek": 124814, "start": 1248.14, "end": 1251.0400000000002, "text": " This is from increasing the context length.", "tokens": [50365, 639, 307, 490, 5662, 264, 4319, 4641, 13, 50510], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 377, "seek": 124814, "start": 1251.0400000000002, "end": 1253.24, "text": " So I started a little bit of a performance log here.", "tokens": [50510, 407, 286, 1409, 257, 707, 857, 295, 257, 3389, 3565, 510, 13, 50620], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 378, "seek": 124814, "start": 1253.24, "end": 1259.18, "text": " And previously where we were is we were getting a performance of 2.10 on the validation loss.", "tokens": [50620, 400, 8046, 689, 321, 645, 307, 321, 645, 1242, 257, 3389, 295, 568, 13, 3279, 322, 264, 24071, 4470, 13, 50917], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 379, "seek": 124814, "start": 1259.18, "end": 1265.0800000000002, "text": " And now simply scaling up the context length from 3 to 8 gives us a performance of 2.02.", "tokens": [50917, 400, 586, 2935, 21589, 493, 264, 4319, 4641, 490, 805, 281, 1649, 2709, 505, 257, 3389, 295, 568, 13, 12756, 13, 51212], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 380, "seek": 124814, "start": 1265.0800000000002, "end": 1267.0600000000002, "text": " So quite a bit of an improvement here.", "tokens": [51212, 407, 1596, 257, 857, 295, 364, 10444, 510, 13, 51311], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 381, "seek": 124814, "start": 1267.0600000000002, "end": 1270.9, "text": " And also when you sample from the model, you see that the names are definitely improving", "tokens": [51311, 400, 611, 562, 291, 6889, 490, 264, 2316, 11, 291, 536, 300, 264, 5288, 366, 2138, 11470, 51503], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 382, "seek": 124814, "start": 1270.9, "end": 1273.26, "text": " qualitatively as well.", "tokens": [51503, 31312, 356, 382, 731, 13, 51621], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 383, "seek": 124814, "start": 1273.26, "end": 1278.14, "text": " So we could, of course, spend a lot of time here tuning things and making it even bigger", "tokens": [51621, 407, 321, 727, 11, 295, 1164, 11, 3496, 257, 688, 295, 565, 510, 15164, 721, 293, 1455, 309, 754, 3801, 51865], "temperature": 0.0, "avg_logprob": -0.10536890251691951, "compression_ratio": 1.7835051546391754, "no_speech_prob": 0.0005539553239941597}, {"id": 384, "seek": 127814, "start": 1278.14, "end": 1283.88, "text": " and scaling up the network further, even with the simple sort of setup here.", "tokens": [50365, 293, 21589, 493, 264, 3209, 3052, 11, 754, 365, 264, 2199, 1333, 295, 8657, 510, 13, 50652], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 385, "seek": 127814, "start": 1283.88, "end": 1287.5800000000002, "text": " But let's continue and let's implement the hierarchical model and treat this as just", "tokens": [50652, 583, 718, 311, 2354, 293, 718, 311, 4445, 264, 35250, 804, 2316, 293, 2387, 341, 382, 445, 50837], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 386, "seek": 127814, "start": 1287.5800000000002, "end": 1289.96, "text": " a rough baseline performance.", "tokens": [50837, 257, 5903, 20518, 3389, 13, 50956], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 387, "seek": 127814, "start": 1289.96, "end": 1294.3600000000001, "text": " But there's a lot of optimization left on the table in terms of some of the hyperparameters", "tokens": [50956, 583, 456, 311, 257, 688, 295, 19618, 1411, 322, 264, 3199, 294, 2115, 295, 512, 295, 264, 9848, 2181, 335, 6202, 51176], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 388, "seek": 127814, "start": 1294.3600000000001, "end": 1296.26, "text": " that you're hopefully getting a sense of now.", "tokens": [51176, 300, 291, 434, 4696, 1242, 257, 2020, 295, 586, 13, 51271], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 389, "seek": 127814, "start": 1296.26, "end": 1300.4, "text": " Okay, so let's scroll up now and come back up.", "tokens": [51271, 1033, 11, 370, 718, 311, 11369, 493, 586, 293, 808, 646, 493, 13, 51478], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 390, "seek": 127814, "start": 1300.4, "end": 1304.3600000000001, "text": " And what I've done here is I've created a bit of a scratch space for us to just look", "tokens": [51478, 400, 437, 286, 600, 1096, 510, 307, 286, 600, 2942, 257, 857, 295, 257, 8459, 1901, 337, 505, 281, 445, 574, 51676], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 391, "seek": 127814, "start": 1304.3600000000001, "end": 1307.0400000000002, "text": " at the forward pass of the neural net and inspect the shape of the network.", "tokens": [51676, 412, 264, 2128, 1320, 295, 264, 18161, 2533, 293, 15018, 264, 3909, 295, 264, 3209, 13, 51810], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 392, "seek": 127814, "start": 1307.0400000000002, "end": 1308.0400000000002, "text": " So let's go ahead and do that.", "tokens": [51810, 407, 718, 311, 352, 2286, 293, 360, 300, 13, 51860], "temperature": 0.0, "avg_logprob": -0.18149696814047322, "compression_ratio": 1.7317073170731707, "no_speech_prob": 0.0002909170580096543}, {"id": 393, "seek": 130804, "start": 1308.04, "end": 1310.0, "text": " So let's go ahead and look at the shape of the tensors along the way as the neural net", "tokens": [50365, 407, 718, 311, 352, 2286, 293, 574, 412, 264, 3909, 295, 264, 10688, 830, 2051, 264, 636, 382, 264, 18161, 2533, 50463], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 394, "seek": 130804, "start": 1310.0, "end": 1312.36, "text": " forwards.", "tokens": [50463, 30126, 13, 50581], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 395, "seek": 130804, "start": 1312.36, "end": 1317.58, "text": " So here I'm just temporarily for debugging, creating a batch of just, say, four examples.", "tokens": [50581, 407, 510, 286, 478, 445, 23750, 337, 45592, 11, 4084, 257, 15245, 295, 445, 11, 584, 11, 1451, 5110, 13, 50842], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 396, "seek": 130804, "start": 1317.58, "end": 1319.32, "text": " So four random integers.", "tokens": [50842, 407, 1451, 4974, 41674, 13, 50929], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 397, "seek": 130804, "start": 1319.32, "end": 1322.46, "text": " Then I'm plucking out those rows from our training set.", "tokens": [50929, 1396, 286, 478, 499, 33260, 484, 729, 13241, 490, 527, 3097, 992, 13, 51086], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 398, "seek": 130804, "start": 1322.46, "end": 1326.72, "text": " And then I'm passing into the model the input XB.", "tokens": [51086, 400, 550, 286, 478, 8437, 666, 264, 2316, 264, 4846, 1783, 33, 13, 51299], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 399, "seek": 130804, "start": 1326.72, "end": 1331.04, "text": " Now the shape of XB here, because we have only four examples, is four by eight.", "tokens": [51299, 823, 264, 3909, 295, 1783, 33, 510, 11, 570, 321, 362, 787, 1451, 5110, 11, 307, 1451, 538, 3180, 13, 51515], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 400, "seek": 130804, "start": 1331.04, "end": 1334.48, "text": " And this eight is now the current block size.", "tokens": [51515, 400, 341, 3180, 307, 586, 264, 2190, 3461, 2744, 13, 51687], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 401, "seek": 130804, "start": 1334.48, "end": 1337.98, "text": " So inspecting XB, we just see that we have four examples.", "tokens": [51687, 407, 15018, 278, 1783, 33, 11, 321, 445, 536, 300, 321, 362, 1451, 5110, 13, 51862], "temperature": 0.0, "avg_logprob": -0.18377700641000871, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.001057353219948709}, {"id": 402, "seek": 133798, "start": 1337.98, "end": 1341.72, "text": " Each one of them is a row of XB.", "tokens": [50365, 6947, 472, 295, 552, 307, 257, 5386, 295, 1783, 33, 13, 50552], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 403, "seek": 133798, "start": 1341.72, "end": 1344.6200000000001, "text": " And we have eight characters here.", "tokens": [50552, 400, 321, 362, 3180, 4342, 510, 13, 50697], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 404, "seek": 133798, "start": 1344.6200000000001, "end": 1349.74, "text": " And this integer tensor just contains the identities of those characters.", "tokens": [50697, 400, 341, 24922, 40863, 445, 8306, 264, 24239, 295, 729, 4342, 13, 50953], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 405, "seek": 133798, "start": 1349.74, "end": 1352.38, "text": " So the first layer of our neural net is the embedding layer.", "tokens": [50953, 407, 264, 700, 4583, 295, 527, 18161, 2533, 307, 264, 12240, 3584, 4583, 13, 51085], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 406, "seek": 133798, "start": 1352.38, "end": 1357.02, "text": " So passing XB, this integer tensor, through the embedding layer creates an output that", "tokens": [51085, 407, 8437, 1783, 33, 11, 341, 24922, 40863, 11, 807, 264, 12240, 3584, 4583, 7829, 364, 5598, 300, 51317], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 407, "seek": 133798, "start": 1357.02, "end": 1359.3600000000001, "text": " is four by eight by 10.", "tokens": [51317, 307, 1451, 538, 3180, 538, 1266, 13, 51434], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 408, "seek": 133798, "start": 1359.3600000000001, "end": 1365.02, "text": " So our embedding table has, for each character, a 10-dimensional vector that we are trying", "tokens": [51434, 407, 527, 12240, 3584, 3199, 575, 11, 337, 1184, 2517, 11, 257, 1266, 12, 18759, 8062, 300, 321, 366, 1382, 51717], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 409, "seek": 133798, "start": 1365.02, "end": 1366.02, "text": " to learn.", "tokens": [51717, 281, 1466, 13, 51767], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 410, "seek": 133798, "start": 1366.02, "end": 1367.3, "text": " And so what the embedding layer does here...", "tokens": [51767, 400, 370, 437, 264, 12240, 3584, 4583, 775, 510, 485, 51831], "temperature": 0.0, "avg_logprob": -0.10813704475027616, "compression_ratio": 1.836, "no_speech_prob": 0.0007445208611898124}, {"id": 411, "seek": 136730, "start": 1367.3, "end": 1372.36, "text": " What the layer does here is it blocks out the embedding vector for each one of these", "tokens": [50365, 708, 264, 4583, 775, 510, 307, 309, 8474, 484, 264, 12240, 3584, 8062, 337, 1184, 472, 295, 613, 50618], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 412, "seek": 136730, "start": 1372.36, "end": 1378.76, "text": " integers and organizes it all in a four by eight by 10 tensor now.", "tokens": [50618, 41674, 293, 4645, 279, 309, 439, 294, 257, 1451, 538, 3180, 538, 1266, 40863, 586, 13, 50938], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 413, "seek": 136730, "start": 1378.76, "end": 1383.1599999999999, "text": " So all of these integers are translated into 10-dimensional vectors inside this three-dimensional", "tokens": [50938, 407, 439, 295, 613, 41674, 366, 16805, 666, 1266, 12, 18759, 18875, 1854, 341, 1045, 12, 18759, 51158], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 414, "seek": 136730, "start": 1383.1599999999999, "end": 1384.98, "text": " tensor now.", "tokens": [51158, 40863, 586, 13, 51249], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 415, "seek": 136730, "start": 1384.98, "end": 1389.3799999999999, "text": " Now passing that through the flatten layer, as you recall, what this does is it views", "tokens": [51249, 823, 8437, 300, 807, 264, 24183, 4583, 11, 382, 291, 9901, 11, 437, 341, 775, 307, 309, 6809, 51469], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 416, "seek": 136730, "start": 1389.3799999999999, "end": 1392.52, "text": " this tensor as just a four by 80 tensor.", "tokens": [51469, 341, 40863, 382, 445, 257, 1451, 538, 4688, 40863, 13, 51626], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 417, "seek": 136730, "start": 1392.52, "end": 1396.8, "text": " And what that effectively does is that all these 10-dimensional embeddings for all these", "tokens": [51626, 400, 437, 300, 8659, 775, 307, 300, 439, 613, 1266, 12, 18759, 12240, 29432, 337, 439, 613, 51840], "temperature": 0.0, "avg_logprob": -0.10458336705746858, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.0003787110617849976}, {"id": 418, "seek": 139680, "start": 1396.8, "end": 1401.8, "text": " eight characters just end up being stretched out into a long row.", "tokens": [50365, 3180, 4342, 445, 917, 493, 885, 23563, 484, 666, 257, 938, 5386, 13, 50615], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 419, "seek": 139680, "start": 1401.8, "end": 1404.82, "text": " And that looks kind of like a concatenation operation, basically.", "tokens": [50615, 400, 300, 1542, 733, 295, 411, 257, 1588, 7186, 399, 6916, 11, 1936, 13, 50766], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 420, "seek": 139680, "start": 1404.82, "end": 1409.1, "text": " So by viewing the tensor differently, we now have a four by 80.", "tokens": [50766, 407, 538, 17480, 264, 40863, 7614, 11, 321, 586, 362, 257, 1451, 538, 4688, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 421, "seek": 139680, "start": 1409.1, "end": 1416.3799999999999, "text": " And inside this 80, it's all the 10-dimensional vectors just concatenated next to each other.", "tokens": [50980, 400, 1854, 341, 4688, 11, 309, 311, 439, 264, 1266, 12, 18759, 18875, 445, 1588, 7186, 770, 958, 281, 1184, 661, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 422, "seek": 139680, "start": 1416.3799999999999, "end": 1423.48, "text": " And the linear layer, of course, takes 80 and creates 200 channels just via matrix multiplication.", "tokens": [51344, 400, 264, 8213, 4583, 11, 295, 1164, 11, 2516, 4688, 293, 7829, 2331, 9235, 445, 5766, 8141, 27290, 13, 51699], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 423, "seek": 139680, "start": 1423.48, "end": 1424.48, "text": " So so far, so good.", "tokens": [51699, 407, 370, 1400, 11, 370, 665, 13, 51749], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 424, "seek": 139680, "start": 1424.48, "end": 1425.72, "text": " Now I'd like to show you something surprising.", "tokens": [51749, 823, 286, 1116, 411, 281, 855, 291, 746, 8830, 13, 51811], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 425, "seek": 139680, "start": 1425.72, "end": 1426.72, "text": " Let's see.", "tokens": [51811, 961, 311, 536, 13, 51861], "temperature": 0.0, "avg_logprob": -0.1373538025154555, "compression_ratio": 1.569023569023569, "no_speech_prob": 0.0003066443314310163}, {"id": 426, "seek": 142680, "start": 1426.8, "end": 1432.74, "text": " Let's look at the insides of the linear layer and remind ourselves how it works.", "tokens": [50365, 961, 311, 574, 412, 264, 1028, 1875, 295, 264, 8213, 4583, 293, 4160, 4175, 577, 309, 1985, 13, 50662], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 427, "seek": 142680, "start": 1432.74, "end": 1437.74, "text": " The linear layer here in a forward pass takes the input x, multiplies it with a weight,", "tokens": [50662, 440, 8213, 4583, 510, 294, 257, 2128, 1320, 2516, 264, 4846, 2031, 11, 12788, 530, 309, 365, 257, 3364, 11, 50912], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 428, "seek": 142680, "start": 1437.74, "end": 1439.76, "text": " and then optionally adds bias.", "tokens": [50912, 293, 550, 3614, 379, 10860, 12577, 13, 51013], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 429, "seek": 142680, "start": 1439.76, "end": 1443.06, "text": " And the weight here is two-dimensional, as defined here, and the bias is one-dimensional", "tokens": [51013, 400, 264, 3364, 510, 307, 732, 12, 18759, 11, 382, 7642, 510, 11, 293, 264, 12577, 307, 472, 12, 18759, 51178], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 430, "seek": 142680, "start": 1443.06, "end": 1444.62, "text": " here.", "tokens": [51178, 510, 13, 51256], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 431, "seek": 142680, "start": 1444.62, "end": 1448.68, "text": " So effectively, in terms of the shapes involved, what's happening inside this linear layer", "tokens": [51256, 407, 8659, 11, 294, 2115, 295, 264, 10854, 3288, 11, 437, 311, 2737, 1854, 341, 8213, 4583, 51459], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 432, "seek": 142680, "start": 1448.68, "end": 1451.24, "text": " looks like this right now.", "tokens": [51459, 1542, 411, 341, 558, 586, 13, 51587], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 433, "seek": 142680, "start": 1451.24, "end": 1455.8, "text": " And I'm using random numbers here, but I'm just illustrating the shapes and what happens.", "tokens": [51587, 400, 286, 478, 1228, 4974, 3547, 510, 11, 457, 286, 478, 445, 8490, 8754, 264, 10854, 293, 437, 2314, 13, 51815], "temperature": 0.0, "avg_logprob": -0.09825801849365234, "compression_ratio": 1.818840579710145, "no_speech_prob": 0.0005200176383368671}, {"id": 434, "seek": 145580, "start": 1455.8, "end": 1460.62, "text": " Basically, a four by 80 input comes into the linear layer, gets multiplied by this", "tokens": [50365, 8537, 11, 257, 1451, 538, 4688, 4846, 1487, 666, 264, 8213, 4583, 11, 2170, 17207, 538, 341, 50606], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 435, "seek": 145580, "start": 1460.62, "end": 1464.84, "text": " 80 by 200 weight matrix inside, and then there's a plus 200 bias.", "tokens": [50606, 4688, 538, 2331, 3364, 8141, 1854, 11, 293, 550, 456, 311, 257, 1804, 2331, 12577, 13, 50817], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 436, "seek": 145580, "start": 1464.84, "end": 1468.78, "text": " And the shape of the whole thing that comes out of the linear layer is four by 200, as", "tokens": [50817, 400, 264, 3909, 295, 264, 1379, 551, 300, 1487, 484, 295, 264, 8213, 4583, 307, 1451, 538, 2331, 11, 382, 51014], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 437, "seek": 145580, "start": 1468.78, "end": 1470.78, "text": " we see here.", "tokens": [51014, 321, 536, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 438, "seek": 145580, "start": 1470.78, "end": 1476.18, "text": " Now notice here, by the way, that this here will create a four by 200 tensor, and then", "tokens": [51114, 823, 3449, 510, 11, 538, 264, 636, 11, 300, 341, 510, 486, 1884, 257, 1451, 538, 2331, 40863, 11, 293, 550, 51384], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 439, "seek": 145580, "start": 1476.18, "end": 1482.36, "text": " plus 200, there's a broadcasting happening here, but four by 200 broadcasts with 200,", "tokens": [51384, 1804, 2331, 11, 456, 311, 257, 30024, 2737, 510, 11, 457, 1451, 538, 2331, 9975, 82, 365, 2331, 11, 51693], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 440, "seek": 145580, "start": 1482.36, "end": 1483.68, "text": " so everything works here.", "tokens": [51693, 370, 1203, 1985, 510, 13, 51759], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 441, "seek": 145580, "start": 1483.68, "end": 1484.6399999999999, "text": " So now the surprising thing that I want to show you is this.", "tokens": [51759, 407, 586, 264, 8830, 551, 300, 286, 528, 281, 855, 291, 307, 341, 13, 51807], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 442, "seek": 145580, "start": 1484.6399999999999, "end": 1485.6399999999999, "text": " I'm going to show you how this works.", "tokens": [51807, 286, 478, 516, 281, 855, 291, 577, 341, 1985, 13, 51857], "temperature": 0.0, "avg_logprob": -0.24933595526708316, "compression_ratio": 1.8762886597938144, "no_speech_prob": 0.0003050469676963985}, {"id": 443, "seek": 148564, "start": 1485.64, "end": 1489.46, "text": " One thing that I'd like to show you that you may not expect is that this input here", "tokens": [50365, 1485, 551, 300, 286, 1116, 411, 281, 855, 291, 300, 291, 815, 406, 2066, 307, 300, 341, 4846, 510, 50556], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 444, "seek": 148564, "start": 1489.46, "end": 1493.64, "text": " that is being multiplied doesn't actually have to be two-dimensional.", "tokens": [50556, 300, 307, 885, 17207, 1177, 380, 767, 362, 281, 312, 732, 12, 18759, 13, 50765], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 445, "seek": 148564, "start": 1493.64, "end": 1498.18, "text": " This matrix multiply operator in PyTorch is quite powerful, and in fact, you can actually", "tokens": [50765, 639, 8141, 12972, 12973, 294, 9953, 51, 284, 339, 307, 1596, 4005, 11, 293, 294, 1186, 11, 291, 393, 767, 50992], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 446, "seek": 148564, "start": 1498.18, "end": 1502.0200000000002, "text": " pass in higher dimensional arrays or tensors, and everything works fine.", "tokens": [50992, 1320, 294, 2946, 18795, 41011, 420, 10688, 830, 11, 293, 1203, 1985, 2489, 13, 51184], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 447, "seek": 148564, "start": 1502.0200000000002, "end": 1505.96, "text": " So for example, this could be four by five by 80, and the result in that case will become", "tokens": [51184, 407, 337, 1365, 11, 341, 727, 312, 1451, 538, 1732, 538, 4688, 11, 293, 264, 1874, 294, 300, 1389, 486, 1813, 51381], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 448, "seek": 148564, "start": 1505.96, "end": 1508.3600000000001, "text": " four by five by 200.", "tokens": [51381, 1451, 538, 1732, 538, 2331, 13, 51501], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 449, "seek": 148564, "start": 1508.3600000000001, "end": 1511.72, "text": " You can add as many dimensions as you like on the left here.", "tokens": [51501, 509, 393, 909, 382, 867, 12819, 382, 291, 411, 322, 264, 1411, 510, 13, 51669], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 450, "seek": 148564, "start": 1511.72, "end": 1515.64, "text": " And so effectively, what's happening is that the matrix multiplication only works on a", "tokens": [51669, 400, 370, 8659, 11, 437, 311, 2737, 307, 300, 264, 8141, 27290, 787, 1985, 322, 257, 51865], "temperature": 0.0, "avg_logprob": -0.11456998189290364, "compression_ratio": 1.7584097859327217, "no_speech_prob": 0.004509540740400553}, {"id": 451, "seek": 151564, "start": 1515.64, "end": 1519.18, "text": " matrix multiplication on the last dimension, and the dimensions before it in the input", "tokens": [50365, 8141, 27290, 322, 264, 1036, 10139, 11, 293, 264, 12819, 949, 309, 294, 264, 4846, 50542], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 452, "seek": 151564, "start": 1519.18, "end": 1524.7800000000002, "text": " tensor are left unchanged.", "tokens": [50542, 40863, 366, 1411, 44553, 13, 50822], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 453, "seek": 151564, "start": 1524.7800000000002, "end": 1531.74, "text": " So basically, these dimensions on the left are all treated as just a batch dimension.", "tokens": [50822, 407, 1936, 11, 613, 12819, 322, 264, 1411, 366, 439, 8668, 382, 445, 257, 15245, 10139, 13, 51170], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 454, "seek": 151564, "start": 1531.74, "end": 1536.5800000000002, "text": " So we can have multiple batch dimensions, and then in parallel over all those dimensions,", "tokens": [51170, 407, 321, 393, 362, 3866, 15245, 12819, 11, 293, 550, 294, 8952, 670, 439, 729, 12819, 11, 51412], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 455, "seek": 151564, "start": 1536.5800000000002, "end": 1539.6200000000001, "text": " we are doing the matrix multiplication on the last dimension.", "tokens": [51412, 321, 366, 884, 264, 8141, 27290, 322, 264, 1036, 10139, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 456, "seek": 151564, "start": 1539.6200000000001, "end": 1544.5200000000002, "text": " So this is quite convenient because we can use that in our network now.", "tokens": [51564, 407, 341, 307, 1596, 10851, 570, 321, 393, 764, 300, 294, 527, 3209, 586, 13, 51809], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 457, "seek": 151564, "start": 1544.5200000000002, "end": 1545.5200000000002, "text": " Because remember that.", "tokens": [51809, 1436, 1604, 300, 13, 51859], "temperature": 0.0, "avg_logprob": -0.16614059448242188, "compression_ratio": 1.9647577092511013, "no_speech_prob": 0.0005804559914395213}, {"id": 458, "seek": 154552, "start": 1545.52, "end": 1549.3799999999999, "text": " We have these eight characters coming in.", "tokens": [50365, 492, 362, 613, 3180, 4342, 1348, 294, 13, 50558], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 459, "seek": 154552, "start": 1549.3799999999999, "end": 1555.12, "text": " And we don't want to now flatten all of it out into a large eight-dimensional vector", "tokens": [50558, 400, 321, 500, 380, 528, 281, 586, 24183, 439, 295, 309, 484, 666, 257, 2416, 3180, 12, 18759, 8062, 50845], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 460, "seek": 154552, "start": 1555.12, "end": 1561.84, "text": " because we don't want to matrix multiply 80 into a weight matrix multiply immediately.", "tokens": [50845, 570, 321, 500, 380, 528, 281, 8141, 12972, 4688, 666, 257, 3364, 8141, 12972, 4258, 13, 51181], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 461, "seek": 154552, "start": 1561.84, "end": 1567.1399999999999, "text": " Instead, we want to group these like this.", "tokens": [51181, 7156, 11, 321, 528, 281, 1594, 613, 411, 341, 13, 51446], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 462, "seek": 154552, "start": 1567.1399999999999, "end": 1571.3799999999999, "text": " So every consecutive two elements, one and two and three and four and five and six and", "tokens": [51446, 407, 633, 30497, 732, 4959, 11, 472, 293, 732, 293, 1045, 293, 1451, 293, 1732, 293, 2309, 293, 51658], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 463, "seek": 154552, "start": 1571.3799999999999, "end": 1574.9, "text": " seven and eight, all of these should be now basically flattened.", "tokens": [51658, 3407, 293, 3180, 11, 439, 295, 613, 820, 312, 586, 1936, 24183, 292, 13, 51834], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 464, "seek": 154552, "start": 1574.9, "end": 1575.52, "text": " Okay.", "tokens": [51834, 1033, 13, 51865], "temperature": 0.0, "avg_logprob": -0.1777329351387772, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00047755014384165406}, {"id": 465, "seek": 157552, "start": 1575.52, "end": 1576.6, "text": " So we can.", "tokens": [50365, 407, 321, 393, 13, 50419], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 466, "seek": 157552, "start": 1576.6, "end": 1578.3799999999999, "text": " End out and multiply by weight matrix.", "tokens": [50419, 6967, 484, 293, 12972, 538, 3364, 8141, 13, 50508], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 467, "seek": 157552, "start": 1578.3799999999999, "end": 1582.24, "text": " But all of these four groups here, we'd like to process in parallel.", "tokens": [50508, 583, 439, 295, 613, 1451, 3935, 510, 11, 321, 1116, 411, 281, 1399, 294, 8952, 13, 50701], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 468, "seek": 157552, "start": 1582.24, "end": 1586.08, "text": " So it's kind of like a batch dimension that we can introduce.", "tokens": [50701, 407, 309, 311, 733, 295, 411, 257, 15245, 10139, 300, 321, 393, 5366, 13, 50893], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 469, "seek": 157552, "start": 1586.08, "end": 1593.82, "text": " And then we can, in parallel, basically process all of these bigram groups in the four batch", "tokens": [50893, 400, 550, 321, 393, 11, 294, 8952, 11, 1936, 1399, 439, 295, 613, 955, 2356, 3935, 294, 264, 1451, 15245, 51280], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 470, "seek": 157552, "start": 1593.82, "end": 1600.02, "text": " dimensions of an individual example, and also over the actual batch dimension of the four", "tokens": [51280, 12819, 295, 364, 2609, 1365, 11, 293, 611, 670, 264, 3539, 15245, 10139, 295, 264, 1451, 51590], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 471, "seek": 157552, "start": 1600.02, "end": 1601.84, "text": " examples in our example here.", "tokens": [51590, 5110, 294, 527, 1365, 510, 13, 51681], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 472, "seek": 157552, "start": 1601.84, "end": 1603.3799999999999, "text": " So let's see how that works.", "tokens": [51681, 407, 718, 311, 536, 577, 300, 1985, 13, 51758], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 473, "seek": 157552, "start": 1603.3799999999999, "end": 1604.8799999999999, "text": " Effectively, what we want is.", "tokens": [51758, 17764, 3413, 11, 437, 321, 528, 307, 13, 51833], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 474, "seek": 157552, "start": 1604.8799999999999, "end": 1605.36, "text": " Right now.", "tokens": [51833, 1779, 586, 13, 51857], "temperature": 0.6000000000000001, "avg_logprob": -0.3375470275878906, "compression_ratio": 1.815686274509804, "no_speech_prob": 0.0015109102241694927}, {"id": 475, "seek": 160536, "start": 1605.36, "end": 1611.76, "text": " Now we take a 4 by 80 and multiply it by 80 by 200 in the linear layer.", "tokens": [50365, 823, 321, 747, 257, 1017, 538, 4688, 293, 12972, 309, 538, 4688, 538, 2331, 294, 264, 8213, 4583, 13, 50685], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 476, "seek": 160536, "start": 1611.84, "end": 1612.5, "text": " This is what happens.", "tokens": [50689, 639, 307, 437, 2314, 13, 50722], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 477, "seek": 160536, "start": 1613.56, "end": 1618.6599999999999, "text": " But instead what we want is we don't want 80 characters or 80 numbers to come in.", "tokens": [50775, 583, 2602, 437, 321, 528, 307, 321, 500, 380, 528, 4688, 4342, 420, 4688, 3547, 281, 808, 294, 13, 51030], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 478, "seek": 160536, "start": 1619.02, "end": 1621.84, "text": " We only want two characters to come in on the very first layer,", "tokens": [51048, 492, 787, 528, 732, 4342, 281, 808, 294, 322, 264, 588, 700, 4583, 11, 51189], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 479, "seek": 160536, "start": 1622.02, "end": 1623.6999999999998, "text": " and those two characters should be fused.", "tokens": [51198, 293, 729, 732, 4342, 820, 312, 283, 4717, 13, 51282], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 480, "seek": 160536, "start": 1624.82, "end": 1628.32, "text": " So in other words, we just want 20 to come in, right?", "tokens": [51338, 407, 294, 661, 2283, 11, 321, 445, 528, 945, 281, 808, 294, 11, 558, 30, 51513], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 481, "seek": 160536, "start": 1629.02, "end": 1630.28, "text": " 20 numbers would come in.", "tokens": [51548, 945, 3547, 576, 808, 294, 13, 51611], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 482, "seek": 160536, "start": 1630.8999999999999, "end": 1634.1399999999999, "text": " And here we don't want a 4 by 80 to feed into the linear layer.", "tokens": [51642, 400, 510, 321, 500, 380, 528, 257, 1017, 538, 4688, 281, 3154, 666, 264, 8213, 4583, 13, 51804], "temperature": 0.0, "avg_logprob": -0.16283967065029456, "compression_ratio": 1.7932489451476794, "no_speech_prob": 0.2297559529542923}, {"id": 483, "seek": 163414, "start": 1634.14, "end": 1637.22, "text": " We actually want these groups of 2 to feed in.", "tokens": [50365, 492, 767, 528, 613, 3935, 295, 568, 281, 3154, 294, 13, 50519], "temperature": 0.0, "avg_logprob": -0.11593201004456137, "compression_ratio": 1.6726457399103138, "no_speech_prob": 1.3374464288062882e-05}, {"id": 484, "seek": 163414, "start": 1637.6200000000001, "end": 1641.7, "text": " So instead of 4 by 80, we want this to be a 4 by 4 by 20.", "tokens": [50539, 407, 2602, 295, 1017, 538, 4688, 11, 321, 528, 341, 281, 312, 257, 1017, 538, 1017, 538, 945, 13, 50743], "temperature": 0.0, "avg_logprob": -0.11593201004456137, "compression_ratio": 1.6726457399103138, "no_speech_prob": 1.3374464288062882e-05}, {"id": 485, "seek": 163414, "start": 1643.26, "end": 1648.7, "text": " So these are the four groups of 2, and each one of them is a 10-dimensional vector.", "tokens": [50821, 407, 613, 366, 264, 1451, 3935, 295, 568, 11, 293, 1184, 472, 295, 552, 307, 257, 1266, 12, 18759, 8062, 13, 51093], "temperature": 0.0, "avg_logprob": -0.11593201004456137, "compression_ratio": 1.6726457399103138, "no_speech_prob": 1.3374464288062882e-05}, {"id": 486, "seek": 163414, "start": 1649.42, "end": 1652.38, "text": " So what we want now is we need to change the flatten layer", "tokens": [51129, 407, 437, 321, 528, 586, 307, 321, 643, 281, 1319, 264, 24183, 4583, 51277], "temperature": 0.0, "avg_logprob": -0.11593201004456137, "compression_ratio": 1.6726457399103138, "no_speech_prob": 1.3374464288062882e-05}, {"id": 487, "seek": 163414, "start": 1652.38, "end": 1656.4, "text": " so it doesn't output a 4 by 80, but it outputs a 4 by 4 by 20,", "tokens": [51277, 370, 309, 1177, 380, 5598, 257, 1017, 538, 4688, 11, 457, 309, 23930, 257, 1017, 538, 1017, 538, 945, 11, 51478], "temperature": 0.0, "avg_logprob": -0.11593201004456137, "compression_ratio": 1.6726457399103138, "no_speech_prob": 1.3374464288062882e-05}, {"id": 488, "seek": 163414, "start": 1657.0, "end": 1664.1200000000001, "text": " where basically every two consecutive characters are packed in", "tokens": [51508, 689, 1936, 633, 732, 30497, 4342, 366, 13265, 294, 51864], "temperature": 0.0, "avg_logprob": -0.11593201004456137, "compression_ratio": 1.6726457399103138, "no_speech_prob": 1.3374464288062882e-05}, {"id": 489, "seek": 166412, "start": 1664.12, "end": 1665.3999999999999, "text": " on the very last dimension.", "tokens": [50365, 322, 264, 588, 1036, 10139, 13, 50429], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 490, "seek": 166412, "start": 1665.9799999999998, "end": 1668.3999999999999, "text": " And then these four is the first batch dimension,", "tokens": [50458, 400, 550, 613, 1451, 307, 264, 700, 15245, 10139, 11, 50579], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 491, "seek": 166412, "start": 1668.8799999999999, "end": 1671.02, "text": " and this four is the second batch dimension,", "tokens": [50603, 293, 341, 1451, 307, 264, 1150, 15245, 10139, 11, 50710], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 492, "seek": 166412, "start": 1671.3999999999999, "end": 1674.3799999999999, "text": " referring to the four groups inside every one of these examples.", "tokens": [50729, 13761, 281, 264, 1451, 3935, 1854, 633, 472, 295, 613, 5110, 13, 50878], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 493, "seek": 166412, "start": 1675.3799999999999, "end": 1677.58, "text": " And then this will just multiply like this.", "tokens": [50928, 400, 550, 341, 486, 445, 12972, 411, 341, 13, 51038], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 494, "seek": 166412, "start": 1677.6999999999998, "end": 1679.3, "text": " So this is what we want to get to.", "tokens": [51044, 407, 341, 307, 437, 321, 528, 281, 483, 281, 13, 51124], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 495, "seek": 166412, "start": 1679.8799999999999, "end": 1681.36, "text": " So we're going to have to change the linear layer", "tokens": [51153, 407, 321, 434, 516, 281, 362, 281, 1319, 264, 8213, 4583, 51227], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 496, "seek": 166412, "start": 1681.36, "end": 1683.28, "text": " in terms of how many inputs it expects.", "tokens": [51227, 294, 2115, 295, 577, 867, 15743, 309, 33280, 13, 51323], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 497, "seek": 166412, "start": 1683.4199999999998, "end": 1685.4399999999998, "text": " It shouldn't expect 80.", "tokens": [51330, 467, 4659, 380, 2066, 4688, 13, 51431], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 498, "seek": 166412, "start": 1685.52, "end": 1686.7199999999998, "text": " It should just expect 20 numbers.", "tokens": [51435, 467, 820, 445, 2066, 945, 3547, 13, 51495], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 499, "seek": 166412, "start": 1687.04, "end": 1688.6999999999998, "text": " And we have to change our flatten layer", "tokens": [51511, 400, 321, 362, 281, 1319, 527, 24183, 4583, 51594], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 500, "seek": 166412, "start": 1688.6999999999998, "end": 1691.8799999999999, "text": " so it doesn't just fully flatten out this entire example.", "tokens": [51594, 370, 309, 1177, 380, 445, 4498, 24183, 484, 341, 2302, 1365, 13, 51753], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 501, "seek": 166412, "start": 1692.28, "end": 1694.1, "text": " It needs to create a 4 by 4.", "tokens": [51773, 467, 2203, 281, 1884, 257, 1017, 538, 1017, 13, 51864], "temperature": 0.0, "avg_logprob": -0.15088585631488122, "compression_ratio": 1.8850174216027875, "no_speech_prob": 0.00300210271961987}, {"id": 502, "seek": 169410, "start": 1694.1, "end": 1696.58, "text": " It needs to create a 4 by 20 instead of a 4 by 80.", "tokens": [50365, 467, 2203, 281, 1884, 257, 1017, 538, 945, 2602, 295, 257, 1017, 538, 4688, 13, 50489], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 503, "seek": 169410, "start": 1697.06, "end": 1698.5, "text": " So let's see how this could be implemented.", "tokens": [50513, 407, 718, 311, 536, 577, 341, 727, 312, 12270, 13, 50585], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 504, "seek": 169410, "start": 1699.24, "end": 1702.74, "text": " Basically right now we have an input that is a 4 by 8 by 10", "tokens": [50622, 8537, 558, 586, 321, 362, 364, 4846, 300, 307, 257, 1017, 538, 1649, 538, 1266, 50797], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 505, "seek": 169410, "start": 1702.74, "end": 1704.6799999999998, "text": " that feeds into the flatten layer,", "tokens": [50797, 300, 23712, 666, 264, 24183, 4583, 11, 50894], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 506, "seek": 169410, "start": 1705.1, "end": 1708.1399999999999, "text": " and currently the flatten layer just stretches it out.", "tokens": [50915, 293, 4362, 264, 24183, 4583, 445, 29058, 309, 484, 13, 51067], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 507, "seek": 169410, "start": 1708.5, "end": 1710.4199999999998, "text": " So if you remember the implementation of flatten,", "tokens": [51085, 407, 498, 291, 1604, 264, 11420, 295, 24183, 11, 51181], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 508, "seek": 169410, "start": 1711.26, "end": 1715.2199999999998, "text": " it takes our x and it just views it as whatever the batch dimension is,", "tokens": [51223, 309, 2516, 527, 2031, 293, 309, 445, 6809, 309, 382, 2035, 264, 15245, 10139, 307, 11, 51421], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 509, "seek": 169410, "start": 1715.32, "end": 1716.1, "text": " and then negative 1.", "tokens": [51426, 293, 550, 3671, 502, 13, 51465], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 510, "seek": 169410, "start": 1716.9399999999998, "end": 1722.0, "text": " So effectively what it does right now is it does e.view of 4, negative 1,", "tokens": [51507, 407, 8659, 437, 309, 775, 558, 586, 307, 309, 775, 308, 13, 1759, 295, 1017, 11, 3671, 502, 11, 51760], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 511, "seek": 169410, "start": 1722.0, "end": 1724.08, "text": " and the shape of this, of course, is 4 by 80.", "tokens": [51760, 293, 264, 3909, 295, 341, 11, 295, 1164, 11, 307, 1017, 538, 4688, 13, 51864], "temperature": 0.0, "avg_logprob": -0.1524577445172249, "compression_ratio": 1.7665505226480835, "no_speech_prob": 0.00021504338656086475}, {"id": 512, "seek": 172410, "start": 1724.1, "end": 1727.5, "text": " So that's what currently happens,", "tokens": [50365, 407, 300, 311, 437, 4362, 2314, 11, 50535], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 513, "seek": 172410, "start": 1727.6599999999999, "end": 1730.26, "text": " and we instead want this to be a 4 by 4 by 20,", "tokens": [50543, 293, 321, 2602, 528, 341, 281, 312, 257, 1017, 538, 1017, 538, 945, 11, 50673], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 514, "seek": 172410, "start": 1730.5, "end": 1733.28, "text": " where these consecutive 10-dimensional vectors get concatenated.", "tokens": [50685, 689, 613, 30497, 1266, 12, 18759, 18875, 483, 1588, 7186, 770, 13, 50824], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 515, "seek": 172410, "start": 1734.1799999999998, "end": 1738.5, "text": " So you know how in Python you can take a list of range of 10?", "tokens": [50869, 407, 291, 458, 577, 294, 15329, 291, 393, 747, 257, 1329, 295, 3613, 295, 1266, 30, 51085], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 516, "seek": 172410, "start": 1739.78, "end": 1742.28, "text": " So we have numbers from 0 to 9,", "tokens": [51149, 407, 321, 362, 3547, 490, 1958, 281, 1722, 11, 51274], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 517, "seek": 172410, "start": 1742.6399999999999, "end": 1745.74, "text": " and we can index like this to get all the even parts,", "tokens": [51292, 293, 321, 393, 8186, 411, 341, 281, 483, 439, 264, 754, 3166, 11, 51447], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 518, "seek": 172410, "start": 1746.28, "end": 1748.74, "text": " and we can also index like starting at 1", "tokens": [51474, 293, 321, 393, 611, 8186, 411, 2891, 412, 502, 51597], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 519, "seek": 172410, "start": 1748.74, "end": 1751.7199999999998, "text": " and going in steps of 2 to get all the odd parts.", "tokens": [51597, 293, 516, 294, 4439, 295, 568, 281, 483, 439, 264, 7401, 3166, 13, 51746], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 520, "seek": 172410, "start": 1753.06, "end": 1754.08, "text": " So one way to implement this is to take a list of range of 10,", "tokens": [51813, 407, 472, 636, 281, 4445, 341, 307, 281, 747, 257, 1329, 295, 3613, 295, 1266, 11, 51864], "temperature": 0.0, "avg_logprob": -0.19266606116479681, "compression_ratio": 1.74609375, "no_speech_prob": 0.004320749081671238}, {"id": 521, "seek": 175408, "start": 1754.08, "end": 1756.3799999999999, "text": " and one way to implement this, it would be as follows.", "tokens": [50365, 293, 472, 636, 281, 4445, 341, 11, 309, 576, 312, 382, 10002, 13, 50480], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 522, "seek": 175408, "start": 1756.3799999999999, "end": 1761.1999999999998, "text": " We can take e, and we can index into it for all the batch elements,", "tokens": [50480, 492, 393, 747, 308, 11, 293, 321, 393, 8186, 666, 309, 337, 439, 264, 15245, 4959, 11, 50721], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 523, "seek": 175408, "start": 1761.72, "end": 1764.4399999999998, "text": " and then just even elements in this dimension,", "tokens": [50747, 293, 550, 445, 754, 4959, 294, 341, 10139, 11, 50883], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 524, "seek": 175408, "start": 1765.02, "end": 1768.1, "text": " so at indexes 0, 2, 4, and 8,", "tokens": [50912, 370, 412, 8186, 279, 1958, 11, 568, 11, 1017, 11, 293, 1649, 11, 51066], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 525, "seek": 175408, "start": 1768.84, "end": 1772.1999999999998, "text": " and then all the parts here from this last dimension,", "tokens": [51103, 293, 550, 439, 264, 3166, 510, 490, 341, 1036, 10139, 11, 51271], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 526, "seek": 175408, "start": 1773.48, "end": 1776.8, "text": " and this gives us the even characters,", "tokens": [51335, 293, 341, 2709, 505, 264, 754, 4342, 11, 51501], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 527, "seek": 175408, "start": 1777.3, "end": 1781.1799999999998, "text": " and then here this gives us all the odd characters.", "tokens": [51526, 293, 550, 510, 341, 2709, 505, 439, 264, 7401, 4342, 13, 51720], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 528, "seek": 175408, "start": 1781.5, "end": 1783.8, "text": " And basically what we want to do is we want to make sure", "tokens": [51736, 400, 1936, 437, 321, 528, 281, 360, 307, 321, 528, 281, 652, 988, 51851], "temperature": 0.0, "avg_logprob": -0.29463719055715915, "compression_ratio": 1.847926267281106, "no_speech_prob": 0.007560941390693188}, {"id": 529, "seek": 178380, "start": 1783.8, "end": 1786.1399999999999, "text": " that these get concatenated in PyTorch,", "tokens": [50365, 300, 613, 483, 1588, 7186, 770, 294, 9953, 51, 284, 339, 11, 50482], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 530, "seek": 178380, "start": 1786.3799999999999, "end": 1789.4199999999998, "text": " and then we want to concatenate these two tensors", "tokens": [50494, 293, 550, 321, 528, 281, 1588, 7186, 473, 613, 732, 10688, 830, 50646], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 531, "seek": 178380, "start": 1789.4199999999998, "end": 1791.2, "text": " along the second dimension.", "tokens": [50646, 2051, 264, 1150, 10139, 13, 50735], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 532, "seek": 178380, "start": 1793.02, "end": 1796.28, "text": " So this and the shape of it would be 4 by 4 by 20.", "tokens": [50826, 407, 341, 293, 264, 3909, 295, 309, 576, 312, 1017, 538, 1017, 538, 945, 13, 50989], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 533, "seek": 178380, "start": 1796.44, "end": 1798.1599999999999, "text": " This is definitely the result we want.", "tokens": [50997, 639, 307, 2138, 264, 1874, 321, 528, 13, 51083], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 534, "seek": 178380, "start": 1798.36, "end": 1802.08, "text": " We are explicitly grabbing the even parts and the odd parts,", "tokens": [51093, 492, 366, 20803, 23771, 264, 754, 3166, 293, 264, 7401, 3166, 11, 51279], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 535, "seek": 178380, "start": 1802.22, "end": 1805.22, "text": " and we're arranging those 4 by 4 by 10", "tokens": [51286, 293, 321, 434, 5539, 9741, 729, 1017, 538, 1017, 538, 1266, 51436], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 536, "seek": 178380, "start": 1805.22, "end": 1807.24, "text": " right next to each other and concatenate.", "tokens": [51436, 558, 958, 281, 1184, 661, 293, 1588, 7186, 473, 13, 51537], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 537, "seek": 178380, "start": 1808.24, "end": 1811.1399999999999, "text": " So this works, but it turns out that what also works", "tokens": [51587, 407, 341, 1985, 11, 457, 309, 4523, 484, 300, 437, 611, 1985, 51732], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 538, "seek": 178380, "start": 1811.1399999999999, "end": 1813.7, "text": " is you can simply use a view again,", "tokens": [51732, 307, 291, 393, 2935, 764, 257, 1910, 797, 11, 51860], "temperature": 0.0, "avg_logprob": -0.13357395262230098, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.003952874336391687}, {"id": 539, "seek": 181380, "start": 1813.8, "end": 1816.04, "text": " and just request the right shape.", "tokens": [50365, 293, 445, 5308, 264, 558, 3909, 13, 50477], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 540, "seek": 181380, "start": 1816.3799999999999, "end": 1818.02, "text": " And it just so happens that in this case,", "tokens": [50494, 400, 309, 445, 370, 2314, 300, 294, 341, 1389, 11, 50576], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 541, "seek": 181380, "start": 1818.54, "end": 1821.48, "text": " those vectors will again end up being arranged", "tokens": [50602, 729, 18875, 486, 797, 917, 493, 885, 18721, 50749], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 542, "seek": 181380, "start": 1821.48, "end": 1822.6399999999999, "text": " exactly the way we want.", "tokens": [50749, 2293, 264, 636, 321, 528, 13, 50807], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 543, "seek": 181380, "start": 1823.26, "end": 1824.6399999999999, "text": " So in particular, if we take e,", "tokens": [50838, 407, 294, 1729, 11, 498, 321, 747, 308, 11, 50907], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 544, "seek": 181380, "start": 1824.76, "end": 1826.9199999999998, "text": " and we just view it as a 4 by 4 by 20,", "tokens": [50913, 293, 321, 445, 1910, 309, 382, 257, 1017, 538, 1017, 538, 945, 11, 51021], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 545, "seek": 181380, "start": 1827.06, "end": 1827.8999999999999, "text": " which is what we want,", "tokens": [51028, 597, 307, 437, 321, 528, 11, 51070], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 546, "seek": 181380, "start": 1828.6, "end": 1830.96, "text": " we can check that this is exactly equal to,", "tokens": [51105, 321, 393, 1520, 300, 341, 307, 2293, 2681, 281, 11, 51223], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 547, "seek": 181380, "start": 1831.68, "end": 1835.02, "text": " let me call this, this is the explicit concatenation, I suppose.", "tokens": [51259, 718, 385, 818, 341, 11, 341, 307, 264, 13691, 1588, 7186, 399, 11, 286, 7297, 13, 51426], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 548, "seek": 181380, "start": 1836.76, "end": 1839.54, "text": " So explicit dot shape is 4 by 4 by 20.", "tokens": [51513, 407, 13691, 5893, 3909, 307, 1017, 538, 1017, 538, 945, 13, 51652], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 549, "seek": 181380, "start": 1840.1, "end": 1841.82, "text": " If you just view it as 4 by 4 by 20,", "tokens": [51680, 759, 291, 445, 1910, 309, 382, 1017, 538, 1017, 538, 945, 11, 51766], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 550, "seek": 181380, "start": 1841.82, "end": 1843.12, "text": " you can check that,", "tokens": [51766, 291, 393, 1520, 300, 11, 51831], "temperature": 0.0, "avg_logprob": -0.20524726154135287, "compression_ratio": 1.784, "no_speech_prob": 0.0006334246718324721}, {"id": 551, "seek": 184312, "start": 1843.12, "end": 1845.12, "text": " when you compare it to explicit,", "tokens": [50365, 562, 291, 6794, 309, 281, 13691, 11, 50465], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 552, "seek": 184312, "start": 1846.2199999999998, "end": 1848.58, "text": " you get a big, this is element-wise operation,", "tokens": [50520, 291, 483, 257, 955, 11, 341, 307, 4478, 12, 3711, 6916, 11, 50638], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 553, "seek": 184312, "start": 1848.84, "end": 1850.34, "text": " so making sure that all of them are true,", "tokens": [50651, 370, 1455, 988, 300, 439, 295, 552, 366, 2074, 11, 50726], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 554, "seek": 184312, "start": 1851.1399999999999, "end": 1851.7199999999998, "text": " values to true.", "tokens": [50766, 4190, 281, 2074, 13, 50795], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 555, "seek": 184312, "start": 1852.62, "end": 1854.1599999999999, "text": " So basically, long story short,", "tokens": [50840, 407, 1936, 11, 938, 1657, 2099, 11, 50917], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 556, "seek": 184312, "start": 1854.2399999999998, "end": 1856.8799999999999, "text": " we don't need to make an explicit call to concatenate, etc.", "tokens": [50921, 321, 500, 380, 643, 281, 652, 364, 13691, 818, 281, 1588, 7186, 473, 11, 5183, 13, 51053], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 557, "seek": 184312, "start": 1857.1999999999998, "end": 1861.2199999999998, "text": " We can simply take this input tensor to flatten,", "tokens": [51069, 492, 393, 2935, 747, 341, 4846, 40863, 281, 24183, 11, 51270], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 558, "seek": 184312, "start": 1861.6799999999998, "end": 1864.1399999999999, "text": " and we can just view it in whatever way we want.", "tokens": [51293, 293, 321, 393, 445, 1910, 309, 294, 2035, 636, 321, 528, 13, 51416], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 559, "seek": 184312, "start": 1865.02, "end": 1866.3999999999999, "text": " And in particular,", "tokens": [51460, 400, 294, 1729, 11, 51529], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 560, "seek": 184312, "start": 1866.52, "end": 1868.7399999999998, "text": " we don't want to stretch things out with negative 1.", "tokens": [51535, 321, 500, 380, 528, 281, 5985, 721, 484, 365, 3671, 502, 13, 51646], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 561, "seek": 184312, "start": 1868.9799999999998, "end": 1871.1, "text": " We want to actually create a three-dimensional array,", "tokens": [51658, 492, 528, 281, 767, 1884, 257, 1045, 12, 18759, 10225, 11, 51764], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 562, "seek": 184312, "start": 1871.3999999999999, "end": 1872.8, "text": " and depending on how many,", "tokens": [51779, 293, 5413, 322, 577, 867, 11, 51849], "temperature": 0.0, "avg_logprob": -0.24656309698619983, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.0007937017362564802}, {"id": 563, "seek": 187280, "start": 1872.8, "end": 1874.8, "text": " vectors that are consecutive,", "tokens": [50365, 18875, 300, 366, 30497, 11, 50465], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 564, "seek": 187280, "start": 1875.28, "end": 1877.7, "text": " we want to fuse,", "tokens": [50489, 321, 528, 281, 31328, 11, 50610], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 565, "seek": 187280, "start": 1877.9199999999998, "end": 1878.9199999999998, "text": " like for example, 2,", "tokens": [50621, 411, 337, 1365, 11, 568, 11, 50671], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 566, "seek": 187280, "start": 1879.58, "end": 1881.96, "text": " then we can just simply ask for this dimension to be 20,", "tokens": [50704, 550, 321, 393, 445, 2935, 1029, 337, 341, 10139, 281, 312, 945, 11, 50823], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 567, "seek": 187280, "start": 1882.52, "end": 1885.32, "text": " and use a negative 1 here,", "tokens": [50851, 293, 764, 257, 3671, 502, 510, 11, 50991], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 568, "seek": 187280, "start": 1885.6599999999999, "end": 1887.94, "text": " and PyTorch will figure out how many groups it needs to pack", "tokens": [51008, 293, 9953, 51, 284, 339, 486, 2573, 484, 577, 867, 3935, 309, 2203, 281, 2844, 51122], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 569, "seek": 187280, "start": 1888.02, "end": 1889.94, "text": " into this additional batch dimension.", "tokens": [51126, 666, 341, 4497, 15245, 10139, 13, 51222], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 570, "seek": 187280, "start": 1890.78, "end": 1893.1599999999999, "text": " So let's now go into flatten and implement this.", "tokens": [51264, 407, 718, 311, 586, 352, 666, 24183, 293, 4445, 341, 13, 51383], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 571, "seek": 187280, "start": 1893.4199999999998, "end": 1895.04, "text": " Okay, so I scrolled up here to flatten,", "tokens": [51396, 1033, 11, 370, 286, 11369, 292, 493, 510, 281, 24183, 11, 51477], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 572, "seek": 187280, "start": 1895.58, "end": 1897.8999999999999, "text": " and what we'd like to do is we'd like to change it now.", "tokens": [51504, 293, 437, 321, 1116, 411, 281, 360, 307, 321, 1116, 411, 281, 1319, 309, 586, 13, 51620], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 573, "seek": 187280, "start": 1898.1399999999999, "end": 1899.36, "text": " So let me create a constructor,", "tokens": [51632, 407, 718, 385, 1884, 257, 47479, 11, 51693], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 574, "seek": 187280, "start": 1899.68, "end": 1902.2, "text": " and take the number of elements that are consecutive,", "tokens": [51709, 293, 747, 264, 1230, 295, 4959, 300, 366, 30497, 11, 51835], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 575, "seek": 187280, "start": 1902.3, "end": 1902.8, "text": " that we would like to use,", "tokens": [51840, 300, 321, 576, 411, 281, 764, 11, 51865], "temperature": 0.2, "avg_logprob": -0.40700659033370346, "compression_ratio": 1.7577854671280277, "no_speech_prob": 0.0014228335348889232}, {"id": 576, "seek": 190280, "start": 1902.8799999999999, "end": 1904.8, "text": " and then we'd like to concatenate now", "tokens": [50369, 293, 550, 321, 1116, 411, 281, 1588, 7186, 473, 586, 50465], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 577, "seek": 190280, "start": 1904.8799999999999, "end": 1906.8799999999999, "text": " in the last dimension of the output.", "tokens": [50469, 294, 264, 1036, 10139, 295, 264, 5598, 13, 50569], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 578, "seek": 190280, "start": 1907.3999999999999, "end": 1909.3999999999999, "text": " So here we're just going to remember,", "tokens": [50595, 407, 510, 321, 434, 445, 516, 281, 1604, 11, 50695], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 579, "seek": 190280, "start": 1909.48, "end": 1910.48, "text": " self.n equals n.", "tokens": [50699, 2698, 13, 77, 6915, 297, 13, 50749], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 580, "seek": 190280, "start": 1911.12, "end": 1913.12, "text": " And then I want to be careful here,", "tokens": [50781, 400, 550, 286, 528, 281, 312, 5026, 510, 11, 50881], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 581, "seek": 190280, "start": 1913.2, "end": 1916.2, "text": " because PyTorch actually has a torch.flatten,", "tokens": [50885, 570, 9953, 51, 284, 339, 767, 575, 257, 27822, 13, 3423, 32733, 11, 51035], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 582, "seek": 190280, "start": 1916.28, "end": 1918.28, "text": " and its keyword arguments are different,", "tokens": [51039, 293, 1080, 20428, 12869, 366, 819, 11, 51139], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 583, "seek": 190280, "start": 1918.36, "end": 1920.36, "text": " and they kind of like function differently.", "tokens": [51143, 293, 436, 733, 295, 411, 2445, 7614, 13, 51243], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 584, "seek": 190280, "start": 1920.44, "end": 1923.44, "text": " So our flatten is going to start to depart from PyTorch flatten.", "tokens": [51247, 407, 527, 24183, 307, 516, 281, 722, 281, 9110, 490, 9953, 51, 284, 339, 24183, 13, 51397], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 585, "seek": 190280, "start": 1923.52, "end": 1926.2, "text": " So let me call it flatten consecutive,", "tokens": [51401, 407, 718, 385, 818, 309, 24183, 30497, 11, 51535], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 586, "seek": 190280, "start": 1926.28, "end": 1927.28, "text": " or something like that,", "tokens": [51539, 420, 746, 411, 300, 11, 51589], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 587, "seek": 190280, "start": 1927.36, "end": 1930.1599999999999, "text": " just to make sure that our APIs are about equal.", "tokens": [51593, 445, 281, 652, 988, 300, 527, 21445, 366, 466, 2681, 13, 51733], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 588, "seek": 190280, "start": 1930.52, "end": 1931.52, "text": " So this,", "tokens": [51751, 407, 341, 11, 51801], "temperature": 0.0, "avg_logprob": -0.39375951780495067, "compression_ratio": 1.7092198581560283, "no_speech_prob": 0.006454153452068567}, {"id": 589, "seek": 193152, "start": 1931.52, "end": 1935.0, "text": " basically flattens only some n consecutive elements,", "tokens": [50365, 1936, 932, 1591, 694, 787, 512, 297, 30497, 4959, 11, 50539], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 590, "seek": 193152, "start": 1935.08, "end": 1937.08, "text": " and puts them into the last dimension.", "tokens": [50543, 293, 8137, 552, 666, 264, 1036, 10139, 13, 50643], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 591, "seek": 193152, "start": 1937.72, "end": 1941.4, "text": " Now here, the shape of x is b by t by c.", "tokens": [50675, 823, 510, 11, 264, 3909, 295, 2031, 307, 272, 538, 256, 538, 269, 13, 50859], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 592, "seek": 193152, "start": 1941.48, "end": 1945.48, "text": " So let me pop those out into variables.", "tokens": [50863, 407, 718, 385, 1665, 729, 484, 666, 9102, 13, 51063], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 593, "seek": 193152, "start": 1945.56, "end": 1947.68, "text": " And recall that in our example down below,", "tokens": [51067, 400, 9901, 300, 294, 527, 1365, 760, 2507, 11, 51173], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 594, "seek": 193152, "start": 1947.76, "end": 1950.76, "text": " b was 4, t was 8, and c was 10.", "tokens": [51177, 272, 390, 1017, 11, 256, 390, 1649, 11, 293, 269, 390, 1266, 13, 51327], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 595, "seek": 193152, "start": 1953.68, "end": 1957.68, "text": " Now, instead of doing x.view of b by negative 1,", "tokens": [51473, 823, 11, 2602, 295, 884, 2031, 13, 1759, 295, 272, 538, 3671, 502, 11, 51673], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 596, "seek": 193152, "start": 1959.6, "end": 1961.48, "text": " right, this is what we had before.", "tokens": [51769, 558, 11, 341, 307, 437, 321, 632, 949, 13, 51863], "temperature": 0.0, "avg_logprob": -0.2920171496937576, "compression_ratio": 1.5114155251141552, "no_speech_prob": 0.002459072507917881}, {"id": 597, "seek": 196152, "start": 1961.56, "end": 1968.56, "text": " We want this to be b by negative 1 by,", "tokens": [50367, 492, 528, 341, 281, 312, 272, 538, 3671, 502, 538, 11, 50717], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 598, "seek": 196152, "start": 1968.6399999999999, "end": 1972.6399999999999, "text": " and basically here, we want c times n.", "tokens": [50721, 293, 1936, 510, 11, 321, 528, 269, 1413, 297, 13, 50921], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 599, "seek": 196152, "start": 1972.72, "end": 1975.72, "text": " That's how many consecutive elements we want.", "tokens": [50925, 663, 311, 577, 867, 30497, 4959, 321, 528, 13, 51075], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 600, "seek": 196152, "start": 1975.8, "end": 1978.04, "text": " And here, instead of negative 1,", "tokens": [51079, 400, 510, 11, 2602, 295, 3671, 502, 11, 51191], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 601, "seek": 196152, "start": 1978.12, "end": 1979.92, "text": " I don't super love the use of negative 1,", "tokens": [51195, 286, 500, 380, 1687, 959, 264, 764, 295, 3671, 502, 11, 51285], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 602, "seek": 196152, "start": 1980.0, "end": 1981.92, "text": " because I like to be very explicit,", "tokens": [51289, 570, 286, 411, 281, 312, 588, 13691, 11, 51385], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 603, "seek": 196152, "start": 1982.0, "end": 1983.0, "text": " so that you get error messages", "tokens": [51389, 370, 300, 291, 483, 6713, 7897, 51439], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 604, "seek": 196152, "start": 1983.08, "end": 1985.32, "text": " when things don't go according to your expectation.", "tokens": [51443, 562, 721, 500, 380, 352, 4650, 281, 428, 14334, 13, 51555], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 605, "seek": 196152, "start": 1985.4, "end": 1986.8799999999999, "text": " So what do we expect here?", "tokens": [51559, 407, 437, 360, 321, 2066, 510, 30, 51633], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 606, "seek": 196152, "start": 1986.96, "end": 1990.36, "text": " We expect this to become t divide n,", "tokens": [51637, 492, 2066, 341, 281, 1813, 256, 9845, 297, 11, 51807], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 607, "seek": 196152, "start": 1990.44, "end": 1991.48, "text": " using integer division here.", "tokens": [51811, 1228, 24922, 10044, 510, 13, 51863], "temperature": 0.0, "avg_logprob": -0.23605224413749498, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.009795820340514183}, {"id": 608, "seek": 199152, "start": 1991.56, "end": 1993.68, "text": " So that's what I expect to happen.", "tokens": [50367, 407, 300, 311, 437, 286, 2066, 281, 1051, 13, 50473], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 609, "seek": 199152, "start": 1993.76, "end": 1996.12, "text": " And then one more thing I want to do here is,", "tokens": [50477, 400, 550, 472, 544, 551, 286, 528, 281, 360, 510, 307, 11, 50595], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 610, "seek": 199152, "start": 1996.2, "end": 1998.6, "text": " remember previously, all the way in the beginning,", "tokens": [50599, 1604, 8046, 11, 439, 264, 636, 294, 264, 2863, 11, 50719], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 611, "seek": 199152, "start": 1998.68, "end": 2002.56, "text": " n was 3, and basically we're concatenating", "tokens": [50723, 297, 390, 805, 11, 293, 1936, 321, 434, 1588, 7186, 990, 50917], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 612, "seek": 199152, "start": 2002.6399999999999, "end": 2005.36, "text": " all the three characters that existed there.", "tokens": [50921, 439, 264, 1045, 4342, 300, 13135, 456, 13, 51057], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 613, "seek": 199152, "start": 2005.44, "end": 2009.04, "text": " So we basically concatenated everything.", "tokens": [51061, 407, 321, 1936, 1588, 7186, 770, 1203, 13, 51241], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 614, "seek": 199152, "start": 2009.12, "end": 2010.96, "text": " And so sometimes that can create", "tokens": [51245, 400, 370, 2171, 300, 393, 1884, 51337], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 615, "seek": 199152, "start": 2011.04, "end": 2012.68, "text": " a spurious dimension of 1 here.", "tokens": [51341, 257, 637, 24274, 10139, 295, 502, 510, 13, 51423], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 616, "seek": 199152, "start": 2012.76, "end": 2016.48, "text": " So if it is the case that x.shapeAt1 is 1,", "tokens": [51427, 407, 498, 309, 307, 264, 1389, 300, 2031, 13, 82, 42406, 18684, 16, 307, 502, 11, 51613], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 617, "seek": 199152, "start": 2016.56, "end": 2018.52, "text": " then it's kind of like a spurious dimension.", "tokens": [51617, 550, 309, 311, 733, 295, 411, 257, 637, 24274, 10139, 13, 51715], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 618, "seek": 199152, "start": 2018.6, "end": 2020.44, "text": " So we don't want to return a 3,", "tokens": [51719, 407, 321, 500, 380, 528, 281, 2736, 257, 805, 11, 51811], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 619, "seek": 199152, "start": 2020.48, "end": 2021.44, "text": " so we don't want to return a 3,", "tokens": [51813, 370, 321, 500, 380, 528, 281, 2736, 257, 805, 11, 51861], "temperature": 0.0, "avg_logprob": -0.27797328028185614, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.002020800719037652}, {"id": 620, "seek": 202144, "start": 2021.48, "end": 2024.6000000000001, "text": " so we don't want to return a 3-dimensional tensor with a 1 here.", "tokens": [50367, 370, 321, 500, 380, 528, 281, 2736, 257, 805, 12, 18759, 40863, 365, 257, 502, 510, 13, 50523], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 621, "seek": 202144, "start": 2024.68, "end": 2026.48, "text": " We just want to return a 2-dimensional tensor", "tokens": [50527, 492, 445, 528, 281, 2736, 257, 568, 12, 18759, 40863, 50617], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 622, "seek": 202144, "start": 2026.56, "end": 2028.64, "text": " exactly as we did before.", "tokens": [50621, 2293, 382, 321, 630, 949, 13, 50725], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 623, "seek": 202144, "start": 2028.72, "end": 2033.56, "text": " So in this case, basically, we will just say x equals x.squeeze,", "tokens": [50729, 407, 294, 341, 1389, 11, 1936, 11, 321, 486, 445, 584, 2031, 6915, 2031, 13, 44516, 10670, 11, 50971], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 624, "seek": 202144, "start": 2033.64, "end": 2037.56, "text": " that is a PyTorch function.", "tokens": [50975, 300, 307, 257, 9953, 51, 284, 339, 2445, 13, 51171], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 625, "seek": 202144, "start": 2037.64, "end": 2041.92, "text": " And squeeze takes a dimension that it either squeezes out", "tokens": [51175, 400, 13578, 2516, 257, 10139, 300, 309, 2139, 22390, 279, 484, 51389], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 626, "seek": 202144, "start": 2042.0, "end": 2044.72, "text": " all the dimensions of a tensor that are 1,", "tokens": [51393, 439, 264, 12819, 295, 257, 40863, 300, 366, 502, 11, 51529], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 627, "seek": 202144, "start": 2044.8, "end": 2047.76, "text": " or you can specify the exact dimension", "tokens": [51533, 420, 291, 393, 16500, 264, 1900, 10139, 51681], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 628, "seek": 202144, "start": 2047.8400000000001, "end": 2049.36, "text": " that you want to be squeezed.", "tokens": [51685, 300, 291, 528, 281, 312, 39470, 13, 51761], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 629, "seek": 202144, "start": 2049.44, "end": 2051.4, "text": " And again, I like to be as explicit as possible,", "tokens": [51765, 400, 797, 11, 286, 411, 281, 312, 382, 13691, 382, 1944, 11, 51863], "temperature": 0.0, "avg_logprob": -0.19248394668102264, "compression_ratio": 1.792, "no_speech_prob": 0.00023017317289486527}, {"id": 630, "seek": 205144, "start": 2051.48, "end": 2055.48, "text": " always, so I expect to squeeze out the first dimension only", "tokens": [50367, 1009, 11, 370, 286, 2066, 281, 13578, 484, 264, 700, 10139, 787, 50567], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 631, "seek": 205144, "start": 2055.56, "end": 2058.88, "text": " of this tensor, this 3-dimensional tensor.", "tokens": [50571, 295, 341, 40863, 11, 341, 805, 12, 18759, 40863, 13, 50737], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 632, "seek": 205144, "start": 2058.96, "end": 2060.4, "text": " And if this dimension here is 1,", "tokens": [50741, 400, 498, 341, 10139, 510, 307, 502, 11, 50813], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 633, "seek": 205144, "start": 2060.48, "end": 2064.12, "text": " then I just want to return b by c times n.", "tokens": [50817, 550, 286, 445, 528, 281, 2736, 272, 538, 269, 1413, 297, 13, 50999], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 634, "seek": 205144, "start": 2064.2000000000003, "end": 2069.12, "text": " And so self.out will be x, and then we return self.out.", "tokens": [51003, 400, 370, 2698, 13, 346, 486, 312, 2031, 11, 293, 550, 321, 2736, 2698, 13, 346, 13, 51249], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 635, "seek": 205144, "start": 2069.2000000000003, "end": 2071.0, "text": " So that's the candidate implementation.", "tokens": [51253, 407, 300, 311, 264, 11532, 11420, 13, 51343], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 636, "seek": 205144, "start": 2071.08, "end": 2074.84, "text": " And of course, this should be self.in instead of just n.", "tokens": [51347, 400, 295, 1164, 11, 341, 820, 312, 2698, 13, 259, 2602, 295, 445, 297, 13, 51535], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 637, "seek": 205144, "start": 2074.92, "end": 2080.52, "text": " So let's run, and let's come here now and take it for a spin.", "tokens": [51539, 407, 718, 311, 1190, 11, 293, 718, 311, 808, 510, 586, 293, 747, 309, 337, 257, 6060, 13, 51819], "temperature": 0.0, "avg_logprob": -0.17354138692220053, "compression_ratio": 1.7236842105263157, "no_speech_prob": 0.0008571926155127585}, {"id": 638, "seek": 208052, "start": 2080.56, "end": 2086.08, "text": " So flattened consecutive, and in the beginning,", "tokens": [50367, 407, 24183, 292, 30497, 11, 293, 294, 264, 2863, 11, 50643], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 639, "seek": 208052, "start": 2086.16, "end": 2087.64, "text": " let's just use 8.", "tokens": [50647, 718, 311, 445, 764, 1649, 13, 50721], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 640, "seek": 208052, "start": 2087.72, "end": 2090.12, "text": " So this should recover the previous behavior.", "tokens": [50725, 407, 341, 820, 8114, 264, 3894, 5223, 13, 50845], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 641, "seek": 208052, "start": 2090.2, "end": 2095.68, "text": " So flattened consecutive of 8, which is the current block size,", "tokens": [50849, 407, 24183, 292, 30497, 295, 1649, 11, 597, 307, 264, 2190, 3461, 2744, 11, 51123], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 642, "seek": 208052, "start": 2095.7599999999998, "end": 2099.32, "text": " we can do this, that should recover the previous behavior.", "tokens": [51127, 321, 393, 360, 341, 11, 300, 820, 8114, 264, 3894, 5223, 13, 51305], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 643, "seek": 208052, "start": 2099.4, "end": 2102.6, "text": " So we should be able to run the model.", "tokens": [51309, 407, 321, 820, 312, 1075, 281, 1190, 264, 2316, 13, 51469], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 644, "seek": 208052, "start": 2102.68, "end": 2106.92, "text": " And here we can inspect, I have a little code snippet here,", "tokens": [51473, 400, 510, 321, 393, 15018, 11, 286, 362, 257, 707, 3089, 35623, 302, 510, 11, 51685], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 645, "seek": 208052, "start": 2107.0, "end": 2110.48, "text": " where I iterate over all the layers, I print the name,", "tokens": [51689, 689, 286, 44497, 670, 439, 264, 7914, 11, 286, 4482, 264, 1315, 11, 51863], "temperature": 0.0, "avg_logprob": -0.2229678744361514, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0025021210312843323}, {"id": 646, "seek": 211052, "start": 2110.56, "end": 2114.68, "text": " of this class, and the shape.", "tokens": [50367, 295, 341, 1508, 11, 293, 264, 3909, 13, 50573], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 647, "seek": 211052, "start": 2114.7599999999998, "end": 2117.72, "text": " And so we see the shapes as we expect them", "tokens": [50577, 400, 370, 321, 536, 264, 10854, 382, 321, 2066, 552, 50725], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 648, "seek": 211052, "start": 2117.8, "end": 2120.6, "text": " after every single layer in its output.", "tokens": [50729, 934, 633, 2167, 4583, 294, 1080, 5598, 13, 50869], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 649, "seek": 211052, "start": 2120.68, "end": 2124.64, "text": " So now let's try to restructure it using our flattened consecutive", "tokens": [50873, 407, 586, 718, 311, 853, 281, 1472, 2885, 309, 1228, 527, 24183, 292, 30497, 51071], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 650, "seek": 211052, "start": 2124.72, "end": 2126.6, "text": " and do it hierarchically.", "tokens": [51075, 293, 360, 309, 35250, 984, 13, 51169], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 651, "seek": 211052, "start": 2126.68, "end": 2129.52, "text": " So in particular, we want to flatten consecutive,", "tokens": [51173, 407, 294, 1729, 11, 321, 528, 281, 24183, 30497, 11, 51315], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 652, "seek": 211052, "start": 2129.6, "end": 2133.0, "text": " not just block size, but just 2.", "tokens": [51319, 406, 445, 3461, 2744, 11, 457, 445, 568, 13, 51489], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 653, "seek": 211052, "start": 2133.08, "end": 2135.28, "text": " And then we want to process this with linear.", "tokens": [51493, 400, 550, 321, 528, 281, 1399, 341, 365, 8213, 13, 51603], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 654, "seek": 211052, "start": 2135.36, "end": 2137.96, "text": " Now, the number of inputs to this linear will not be", "tokens": [51607, 823, 11, 264, 1230, 295, 15743, 281, 341, 8213, 486, 406, 312, 51737], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 655, "seek": 211052, "start": 2138.04, "end": 2140.4, "text": " n embed times block size, it will now only be", "tokens": [51741, 297, 12240, 1413, 3461, 2744, 11, 309, 486, 586, 787, 312, 51859], "temperature": 0.0, "avg_logprob": -0.22174708048502603, "compression_ratio": 1.7114624505928853, "no_speech_prob": 0.001912493142299354}, {"id": 656, "seek": 214040, "start": 2140.44, "end": 2144.2400000000002, "text": " n embed times 2, 20.", "tokens": [50367, 297, 12240, 1413, 568, 11, 945, 13, 50557], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 657, "seek": 214040, "start": 2144.32, "end": 2146.2400000000002, "text": " This goes through the first layer.", "tokens": [50561, 639, 1709, 807, 264, 700, 4583, 13, 50657], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 658, "seek": 214040, "start": 2146.32, "end": 2149.8, "text": " And now we can, in principle, just copy paste this.", "tokens": [50661, 400, 586, 321, 393, 11, 294, 8665, 11, 445, 5055, 9163, 341, 13, 50835], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 659, "seek": 214040, "start": 2149.88, "end": 2154.0, "text": " Now, the next linear layer should expect n hidden times 2.", "tokens": [50839, 823, 11, 264, 958, 8213, 4583, 820, 2066, 297, 7633, 1413, 568, 13, 51045], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 660, "seek": 214040, "start": 2154.08, "end": 2161.56, "text": " And the last piece of it should expect n hidden times 2 again.", "tokens": [51049, 400, 264, 1036, 2522, 295, 309, 820, 2066, 297, 7633, 1413, 568, 797, 13, 51423], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 661, "seek": 214040, "start": 2161.64, "end": 2165.4, "text": " So this is sort of like the naive version of it.", "tokens": [51427, 407, 341, 307, 1333, 295, 411, 264, 29052, 3037, 295, 309, 13, 51615], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 662, "seek": 214040, "start": 2165.48, "end": 2169.04, "text": " So running this, we now have a much, much bigger model.", "tokens": [51619, 407, 2614, 341, 11, 321, 586, 362, 257, 709, 11, 709, 3801, 2316, 13, 51797], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 663, "seek": 214040, "start": 2169.12, "end": 2170.36, "text": " And we should be able to basically just", "tokens": [51801, 400, 321, 820, 312, 1075, 281, 1936, 445, 51863], "temperature": 0.0, "avg_logprob": -0.1598326910109747, "compression_ratio": 1.6622222222222223, "no_speech_prob": 0.001858410076238215}, {"id": 664, "seek": 217040, "start": 2170.44, "end": 2173.64, "text": " just forward the model.", "tokens": [50367, 445, 2128, 264, 2316, 13, 50527], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 665, "seek": 217040, "start": 2173.7200000000003, "end": 2177.7200000000003, "text": " And now we can inspect the numbers in between.", "tokens": [50531, 400, 586, 321, 393, 15018, 264, 3547, 294, 1296, 13, 50731], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 666, "seek": 217040, "start": 2177.8, "end": 2183.2400000000002, "text": " So 4x8x20 was flattened consecutively into 4x4x20.", "tokens": [50735, 407, 1017, 87, 23, 87, 2009, 390, 24183, 292, 27154, 3413, 666, 1017, 87, 19, 87, 2009, 13, 51007], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 667, "seek": 217040, "start": 2183.32, "end": 2186.64, "text": " This was projected into 4x4x200.", "tokens": [51011, 639, 390, 26231, 666, 1017, 87, 19, 87, 7629, 13, 51177], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 668, "seek": 217040, "start": 2186.7200000000003, "end": 2190.2400000000002, "text": " And then BatchNorm just worked out of the box.", "tokens": [51181, 400, 550, 363, 852, 45, 687, 445, 2732, 484, 295, 264, 2424, 13, 51357], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 669, "seek": 217040, "start": 2190.32, "end": 2192.4, "text": " We have to verify that BatchNorm does the correct thing,", "tokens": [51361, 492, 362, 281, 16888, 300, 363, 852, 45, 687, 775, 264, 3006, 551, 11, 51465], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 670, "seek": 217040, "start": 2192.48, "end": 2193.96, "text": " even though it takes a three-dimensional input", "tokens": [51469, 754, 1673, 309, 2516, 257, 1045, 12, 18759, 4846, 51543], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 671, "seek": 217040, "start": 2194.04, "end": 2196.48, "text": " instead of two-dimensional input.", "tokens": [51547, 2602, 295, 732, 12, 18759, 4846, 13, 51669], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 672, "seek": 217040, "start": 2196.56, "end": 2198.8, "text": " Then we have 10H, which is element-wise.", "tokens": [51673, 1396, 321, 362, 1266, 39, 11, 597, 307, 4478, 12, 3711, 13, 51785], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 673, "seek": 217040, "start": 2198.88, "end": 2200.28, "text": " Then we crushed it again.", "tokens": [51789, 1396, 321, 19889, 309, 797, 13, 51859], "temperature": 0.0, "avg_logprob": -0.19393672640361484, "compression_ratio": 1.5984251968503937, "no_speech_prob": 0.001623505144380033}, {"id": 674, "seek": 220028, "start": 2200.32, "end": 2204.4, "text": " So we flattened consecutively and ended up with a 4x2x400 now.", "tokens": [50367, 407, 321, 24183, 292, 27154, 3413, 293, 4590, 493, 365, 257, 1017, 87, 17, 87, 13741, 586, 13, 50571], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 675, "seek": 220028, "start": 2204.48, "end": 2208.36, "text": " Then linear brought it back down to 200, BatchNorm 10H.", "tokens": [50575, 1396, 8213, 3038, 309, 646, 760, 281, 2331, 11, 363, 852, 45, 687, 1266, 39, 13, 50769], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 676, "seek": 220028, "start": 2208.44, "end": 2210.8, "text": " And lastly, we get a 4x400.", "tokens": [50773, 400, 16386, 11, 321, 483, 257, 1017, 87, 13741, 13, 50891], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 677, "seek": 220028, "start": 2210.88, "end": 2213.84, "text": " And we see that the flattened consecutive for the last flatten here,", "tokens": [50895, 400, 321, 536, 300, 264, 24183, 292, 30497, 337, 264, 1036, 24183, 510, 11, 51043], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 678, "seek": 220028, "start": 2213.92, "end": 2216.8, "text": " it squeezed out that dimension of 1.", "tokens": [51047, 309, 39470, 484, 300, 10139, 295, 502, 13, 51191], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 679, "seek": 220028, "start": 2216.88, "end": 2218.92, "text": " So we only ended up with 4x400.", "tokens": [51195, 407, 321, 787, 4590, 493, 365, 1017, 87, 13741, 13, 51297], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 680, "seek": 220028, "start": 2219.0, "end": 2224.44, "text": " And then linear BatchNorm 10H and the last linear layer to get our logits.", "tokens": [51301, 400, 550, 8213, 363, 852, 45, 687, 1266, 39, 293, 264, 1036, 8213, 4583, 281, 483, 527, 3565, 1208, 13, 51573], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 681, "seek": 220028, "start": 2224.52, "end": 2227.6000000000004, "text": " And so the logits end up in the same shape as they were before.", "tokens": [51577, 400, 370, 264, 3565, 1208, 917, 493, 294, 264, 912, 3909, 382, 436, 645, 949, 13, 51731], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 682, "seek": 220028, "start": 2227.6800000000003, "end": 2229.36, "text": " But now we actually have a nice three-layer,", "tokens": [51735, 583, 586, 321, 767, 362, 257, 1481, 1045, 12, 8376, 260, 11, 51819], "temperature": 0.0, "avg_logprob": -0.20984458923339844, "compression_ratio": 1.7660377358490567, "no_speech_prob": 0.002581630600616336}, {"id": 683, "seek": 222936, "start": 2229.36, "end": 2231.4, "text": " neural net.", "tokens": [50365, 18161, 2533, 13, 50467], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 684, "seek": 222936, "start": 2231.48, "end": 2234.6, "text": " And it basically corresponds to, whoops, sorry.", "tokens": [50471, 400, 309, 1936, 23249, 281, 11, 567, 3370, 11, 2597, 13, 50627], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 685, "seek": 222936, "start": 2234.6800000000003, "end": 2237.6, "text": " It basically corresponds exactly to this network now,", "tokens": [50631, 467, 1936, 23249, 2293, 281, 341, 3209, 586, 11, 50777], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 686, "seek": 222936, "start": 2237.6800000000003, "end": 2241.0, "text": " except only this piece here because we only have three layers.", "tokens": [50781, 3993, 787, 341, 2522, 510, 570, 321, 787, 362, 1045, 7914, 13, 50947], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 687, "seek": 222936, "start": 2241.08, "end": 2244.76, "text": " Whereas here in this example, there's four layers", "tokens": [50951, 13813, 510, 294, 341, 1365, 11, 456, 311, 1451, 7914, 51135], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 688, "seek": 222936, "start": 2244.84, "end": 2248.48, "text": " with a total receptive field size of 16 characters", "tokens": [51139, 365, 257, 3217, 45838, 2519, 2744, 295, 3165, 4342, 51321], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 689, "seek": 222936, "start": 2248.56, "end": 2250.36, "text": " instead of just eight characters.", "tokens": [51325, 2602, 295, 445, 3180, 4342, 13, 51415], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 690, "seek": 222936, "start": 2250.44, "end": 2252.6400000000003, "text": " So the block size here is 16.", "tokens": [51419, 407, 264, 3461, 2744, 510, 307, 3165, 13, 51529], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 691, "seek": 222936, "start": 2252.7200000000003, "end": 2256.96, "text": " So this piece of it is basically implemented here.", "tokens": [51533, 407, 341, 2522, 295, 309, 307, 1936, 12270, 510, 13, 51745], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 692, "seek": 222936, "start": 2257.04, "end": 2259.32, "text": " Now we just have to kind of figure out some good,", "tokens": [51749, 823, 321, 445, 362, 281, 733, 295, 2573, 484, 512, 665, 11, 51863], "temperature": 0.0, "avg_logprob": -0.1817704965402414, "compression_ratio": 1.7265625, "no_speech_prob": 0.0016560557996854186}, {"id": 693, "seek": 225936, "start": 2259.4, "end": 2261.36, "text": " channel numbers to use here.", "tokens": [50367, 2269, 3547, 281, 764, 510, 13, 50465], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 694, "seek": 225936, "start": 2261.44, "end": 2265.32, "text": " Now in particular, I changed the number of hidden units to be 68", "tokens": [50469, 823, 294, 1729, 11, 286, 3105, 264, 1230, 295, 7633, 6815, 281, 312, 23317, 50663], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 695, "seek": 225936, "start": 2265.4, "end": 2267.56, "text": " in this architecture because when I use 68,", "tokens": [50667, 294, 341, 9482, 570, 562, 286, 764, 23317, 11, 50775], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 696, "seek": 225936, "start": 2267.6400000000003, "end": 2270.2000000000003, "text": " the number of parameters comes out to be 22,000.", "tokens": [50779, 264, 1230, 295, 9834, 1487, 484, 281, 312, 5853, 11, 1360, 13, 50907], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 697, "seek": 225936, "start": 2270.28, "end": 2272.7200000000003, "text": " So that's exactly the same that we had before.", "tokens": [50911, 407, 300, 311, 2293, 264, 912, 300, 321, 632, 949, 13, 51033], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 698, "seek": 225936, "start": 2272.8, "end": 2275.6, "text": " And we have the same amount of capacity at this neural net", "tokens": [51037, 400, 321, 362, 264, 912, 2372, 295, 6042, 412, 341, 18161, 2533, 51177], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 699, "seek": 225936, "start": 2275.6800000000003, "end": 2277.36, "text": " in terms of the number of parameters.", "tokens": [51181, 294, 2115, 295, 264, 1230, 295, 9834, 13, 51265], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 700, "seek": 225936, "start": 2277.44, "end": 2279.56, "text": " But the question is whether we are utilizing those parameters", "tokens": [51269, 583, 264, 1168, 307, 1968, 321, 366, 26775, 729, 9834, 51375], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 701, "seek": 225936, "start": 2279.6400000000003, "end": 2281.48, "text": " in a more efficient architecture.", "tokens": [51379, 294, 257, 544, 7148, 9482, 13, 51471], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 702, "seek": 225936, "start": 2281.56, "end": 2285.6400000000003, "text": " So what I did then is I got rid of a lot of the debugging cells here", "tokens": [51475, 407, 437, 286, 630, 550, 307, 286, 658, 3973, 295, 257, 688, 295, 264, 45592, 5438, 510, 51679], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 703, "seek": 225936, "start": 2285.7200000000003, "end": 2287.36, "text": " and I rerun the optimization.", "tokens": [51683, 293, 286, 43819, 409, 264, 19618, 13, 51765], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 704, "seek": 225936, "start": 2287.44, "end": 2289.08, "text": " And scrolling down to the result,", "tokens": [51769, 400, 29053, 760, 281, 264, 1874, 11, 51851], "temperature": 0.0, "avg_logprob": -0.1201849444159146, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0012554380809888244}, {"id": 705, "seek": 228908, "start": 2289.12, "end": 2292.7599999999998, "text": " we see that we get the identical performance roughly.", "tokens": [50367, 321, 536, 300, 321, 483, 264, 14800, 3389, 9810, 13, 50549], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 706, "seek": 228908, "start": 2292.84, "end": 2297.68, "text": " So our validation loss now is 2.029 and previously it was 2.027.", "tokens": [50553, 407, 527, 24071, 4470, 586, 307, 568, 13, 15, 11871, 293, 8046, 309, 390, 568, 13, 15, 10076, 13, 50795], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 707, "seek": 228908, "start": 2297.7599999999998, "end": 2299.64, "text": " So controlling for the number of parameters,", "tokens": [50799, 407, 14905, 337, 264, 1230, 295, 9834, 11, 50893], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 708, "seek": 228908, "start": 2299.72, "end": 2303.48, "text": " changing from the flat to hierarchical is not giving us anything yet.", "tokens": [50897, 4473, 490, 264, 4962, 281, 35250, 804, 307, 406, 2902, 505, 1340, 1939, 13, 51085], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 709, "seek": 228908, "start": 2303.56, "end": 2306.6, "text": " That said, there are two things to point out.", "tokens": [51089, 663, 848, 11, 456, 366, 732, 721, 281, 935, 484, 13, 51241], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 710, "seek": 228908, "start": 2306.68, "end": 2310.08, "text": " Number one, we didn't really torture the architecture here very much.", "tokens": [51245, 5118, 472, 11, 321, 994, 380, 534, 20711, 264, 9482, 510, 588, 709, 13, 51415], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 711, "seek": 228908, "start": 2310.16, "end": 2311.48, "text": " This is just my first guess.", "tokens": [51419, 639, 307, 445, 452, 700, 2041, 13, 51485], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 712, "seek": 228908, "start": 2311.56, "end": 2314.0, "text": " And there's a bunch of hyperparameter search that we could do", "tokens": [51489, 400, 456, 311, 257, 3840, 295, 9848, 2181, 335, 2398, 3164, 300, 321, 727, 360, 51611], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 713, "seek": 228908, "start": 2314.08, "end": 2318.92, "text": " in terms of how we allocate our budget of parameters to what layers.", "tokens": [51615, 294, 2115, 295, 577, 321, 35713, 527, 4706, 295, 9834, 281, 437, 7914, 13, 51857], "temperature": 0.0, "avg_logprob": -0.10996694783218035, "compression_ratio": 1.6525974025974026, "no_speech_prob": 0.0017532299971207976}, {"id": 714, "seek": 231908, "start": 2319.08, "end": 2323.72, "text": " Number two, we still may have a bug inside the BatchNorm1D layer.", "tokens": [50365, 5118, 732, 11, 321, 920, 815, 362, 257, 7426, 1854, 264, 363, 852, 45, 687, 16, 35, 4583, 13, 50597], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 715, "seek": 231908, "start": 2323.7999999999997, "end": 2330.64, "text": " So let's take a look at that because it runs but doesn't do the right thing.", "tokens": [50601, 407, 718, 311, 747, 257, 574, 412, 300, 570, 309, 6676, 457, 1177, 380, 360, 264, 558, 551, 13, 50943], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 716, "seek": 231908, "start": 2330.72, "end": 2334.44, "text": " So I pulled up the layer inspector sort of that we have here", "tokens": [50947, 407, 286, 7373, 493, 264, 4583, 34564, 1333, 295, 300, 321, 362, 510, 51133], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 717, "seek": 231908, "start": 2334.52, "end": 2336.2, "text": " and printed out the shape along the way.", "tokens": [51137, 293, 13567, 484, 264, 3909, 2051, 264, 636, 13, 51221], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 718, "seek": 231908, "start": 2336.2799999999997, "end": 2338.96, "text": " And currently it looks like the BatchNorm is receiving an input", "tokens": [51225, 400, 4362, 309, 1542, 411, 264, 363, 852, 45, 687, 307, 10040, 364, 4846, 51359], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 719, "seek": 231908, "start": 2339.04, "end": 2342.48, "text": " that is 32 by 4 by 68, right?", "tokens": [51363, 300, 307, 8858, 538, 1017, 538, 23317, 11, 558, 30, 51535], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 720, "seek": 231908, "start": 2342.56, "end": 2345.12, "text": " And here on the right, I have the current implementation of BatchNorm", "tokens": [51539, 400, 510, 322, 264, 558, 11, 286, 362, 264, 2190, 11420, 295, 363, 852, 45, 687, 51667], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 721, "seek": 231908, "start": 2345.2, "end": 2346.52, "text": " that we have right now.", "tokens": [51671, 300, 321, 362, 558, 586, 13, 51737], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 722, "seek": 231908, "start": 2346.6, "end": 2348.92, "text": " Now, this BatchNorm assumed, in the way", "tokens": [51741, 823, 11, 341, 363, 852, 45, 687, 15895, 11, 294, 264, 636, 51857], "temperature": 0.0, "avg_logprob": -0.19040335779604706, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0009605722734704614}, {"id": 723, "seek": 234892, "start": 2348.92, "end": 2352.04, "text": " we wrote it and at the time, that X is two-dimensional.", "tokens": [50365, 321, 4114, 309, 293, 412, 264, 565, 11, 300, 1783, 307, 732, 12, 18759, 13, 50521], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 724, "seek": 234892, "start": 2352.12, "end": 2356.04, "text": " So it was N by D, where N was the batch size.", "tokens": [50525, 407, 309, 390, 426, 538, 413, 11, 689, 426, 390, 264, 15245, 2744, 13, 50721], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 725, "seek": 234892, "start": 2356.12, "end": 2360.56, "text": " So that's why we only reduced the mean and the variance over the zeroth dimension.", "tokens": [50725, 407, 300, 311, 983, 321, 787, 9212, 264, 914, 293, 264, 21977, 670, 264, 44746, 900, 10139, 13, 50947], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 726, "seek": 234892, "start": 2360.64, "end": 2363.08, "text": " But now X will basically become three-dimensional.", "tokens": [50951, 583, 586, 1783, 486, 1936, 1813, 1045, 12, 18759, 13, 51073], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 727, "seek": 234892, "start": 2363.16, "end": 2365.0, "text": " So what's happening inside the BatchNorm layer right now?", "tokens": [51077, 407, 437, 311, 2737, 1854, 264, 363, 852, 45, 687, 4583, 558, 586, 30, 51169], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 728, "seek": 234892, "start": 2365.08, "end": 2368.2000000000003, "text": " And how come it's working at all and not giving any errors?", "tokens": [51173, 400, 577, 808, 309, 311, 1364, 412, 439, 293, 406, 2902, 604, 13603, 30, 51329], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 729, "seek": 234892, "start": 2368.28, "end": 2371.56, "text": " The reason for that is basically because everything broadcasts properly,", "tokens": [51333, 440, 1778, 337, 300, 307, 1936, 570, 1203, 9975, 82, 6108, 11, 51497], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 730, "seek": 234892, "start": 2371.64, "end": 2375.56, "text": " but the BatchNorm is not doing what we want it to do.", "tokens": [51501, 457, 264, 363, 852, 45, 687, 307, 406, 884, 437, 321, 528, 309, 281, 360, 13, 51697], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 731, "seek": 234892, "start": 2375.64, "end": 2378.2000000000003, "text": " So in particular, let's basically think through what's happening", "tokens": [51701, 407, 294, 1729, 11, 718, 311, 1936, 519, 807, 437, 311, 2737, 51829], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 732, "seek": 234892, "start": 2378.28, "end": 2378.52, "text": " inside the BatchNorm.", "tokens": [51833, 1854, 264, 363, 852, 45, 687, 13, 51845], "temperature": 0.0, "avg_logprob": -0.15142720041711347, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.0010816794820129871}, {"id": 733, "seek": 237892, "start": 2378.92, "end": 2383.76, "text": " I'm looking at what's happening here.", "tokens": [50365, 286, 478, 1237, 412, 437, 311, 2737, 510, 13, 50607], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 734, "seek": 237892, "start": 2383.84, "end": 2385.44, "text": " I have the code here.", "tokens": [50611, 286, 362, 264, 3089, 510, 13, 50691], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 735, "seek": 237892, "start": 2385.52, "end": 2389.6, "text": " So we're receiving an input of 32 by 4 by 68.", "tokens": [50695, 407, 321, 434, 10040, 364, 4846, 295, 8858, 538, 1017, 538, 23317, 13, 50899], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 736, "seek": 237892, "start": 2389.6800000000003, "end": 2392.7200000000003, "text": " And then we are doing here, X dot mean.", "tokens": [50903, 400, 550, 321, 366, 884, 510, 11, 1783, 5893, 914, 13, 51055], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 737, "seek": 237892, "start": 2392.8, "end": 2394.6, "text": " Here I have E instead of X.", "tokens": [51059, 1692, 286, 362, 462, 2602, 295, 1783, 13, 51149], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 738, "seek": 237892, "start": 2394.6800000000003, "end": 2397.16, "text": " But we're doing the mean over zero.", "tokens": [51153, 583, 321, 434, 884, 264, 914, 670, 4018, 13, 51277], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 739, "seek": 237892, "start": 2397.2400000000002, "end": 2399.6800000000003, "text": " And that's actually giving us 1 by 4 by 68.", "tokens": [51281, 400, 300, 311, 767, 2902, 505, 502, 538, 1017, 538, 23317, 13, 51403], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 740, "seek": 237892, "start": 2399.76, "end": 2402.64, "text": " So we're doing the mean only over the very first dimension.", "tokens": [51407, 407, 321, 434, 884, 264, 914, 787, 670, 264, 588, 700, 10139, 13, 51551], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 741, "seek": 237892, "start": 2402.7200000000003, "end": 2407.88, "text": " And it's giving us a mean and a variance that still maintain this dimension here.", "tokens": [51555, 400, 309, 311, 2902, 505, 257, 914, 293, 257, 21977, 300, 920, 6909, 341, 10139, 510, 13, 51813], "temperature": 0.0, "avg_logprob": -0.18478349911964545, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.0009514436242170632}, {"id": 742, "seek": 240788, "start": 2407.88, "end": 2412.1600000000003, "text": " So these means are only taken over 32 numbers in the first dimension.", "tokens": [50365, 407, 613, 1355, 366, 787, 2726, 670, 8858, 3547, 294, 264, 700, 10139, 13, 50579], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 743, "seek": 240788, "start": 2412.2400000000002, "end": 2417.0, "text": " And then when we perform this, everything broadcasts correctly still.", "tokens": [50583, 400, 550, 562, 321, 2042, 341, 11, 1203, 9975, 82, 8944, 920, 13, 50821], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 744, "seek": 240788, "start": 2417.08, "end": 2426.2000000000003, "text": " But basically what ends up happening is when we also look at the running mean,", "tokens": [50825, 583, 1936, 437, 5314, 493, 2737, 307, 562, 321, 611, 574, 412, 264, 2614, 914, 11, 51281], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 745, "seek": 240788, "start": 2426.28, "end": 2426.88, "text": " the shape of it.", "tokens": [51285, 264, 3909, 295, 309, 13, 51315], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 746, "seek": 240788, "start": 2426.96, "end": 2428.48, "text": " So I'm looking at the model that layers the three,", "tokens": [51319, 407, 286, 478, 1237, 412, 264, 2316, 300, 7914, 264, 1045, 11, 51395], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 747, "seek": 240788, "start": 2428.56, "end": 2429.92, "text": " which is the first BatchNorm layer,", "tokens": [51399, 597, 307, 264, 700, 363, 852, 45, 687, 4583, 11, 51467], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 748, "seek": 240788, "start": 2430.0, "end": 2434.12, "text": " and then looking at whatever the running mean became and its shape.", "tokens": [51471, 293, 550, 1237, 412, 2035, 264, 2614, 914, 3062, 293, 1080, 3909, 13, 51677], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 749, "seek": 240788, "start": 2434.2000000000003, "end": 2437.6400000000003, "text": " The shape of this running mean now is 1 by 4 by 68.", "tokens": [51681, 440, 3909, 295, 341, 2614, 914, 586, 307, 502, 538, 1017, 538, 23317, 13, 51853], "temperature": 0.0, "avg_logprob": -0.17281465363084225, "compression_ratio": 1.7265625, "no_speech_prob": 0.00048521917778998613}, {"id": 750, "seek": 243788, "start": 2437.88, "end": 2444.48, "text": " Instead of it being just size of dimension, because we have 68 channels,", "tokens": [50365, 7156, 295, 309, 885, 445, 2744, 295, 10139, 11, 570, 321, 362, 23317, 9235, 11, 50695], "temperature": 0.0, "avg_logprob": -0.26693088492167366, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015859799459576607}, {"id": 751, "seek": 243788, "start": 2444.56, "end": 2448.32, "text": " we expect to have 68 means and variances that we're maintaining.", "tokens": [50699, 321, 2066, 281, 362, 23317, 1355, 293, 1374, 21518, 300, 321, 434, 14916, 13, 50887], "temperature": 0.0, "avg_logprob": -0.26693088492167366, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015859799459576607}, {"id": 752, "seek": 243788, "start": 2448.4, "end": 2450.96, "text": " But actually we have an array of 4 by 68.", "tokens": [50891, 583, 767, 321, 362, 364, 10225, 295, 1017, 538, 23317, 13, 51019], "temperature": 0.0, "avg_logprob": -0.26693088492167366, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015859799459576607}, {"id": 753, "seek": 243788, "start": 2451.04, "end": 2455.12, "text": " And so basically what this is telling us is this BatchNorm is only...", "tokens": [51023, 400, 370, 1936, 437, 341, 307, 3585, 505, 307, 341, 363, 852, 45, 687, 307, 787, 485, 51227], "temperature": 0.0, "avg_logprob": -0.26693088492167366, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015859799459576607}, {"id": 754, "seek": 243788, "start": 2455.2000000000003, "end": 2465.96, "text": " This BatchNorm is currently working in parallel over 4 times 68 instead of just 68 channels.", "tokens": [51231, 639, 363, 852, 45, 687, 307, 4362, 1364, 294, 8952, 670, 1017, 1413, 23317, 2602, 295, 445, 23317, 9235, 13, 51769], "temperature": 0.0, "avg_logprob": -0.26693088492167366, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015859799459576607}, {"id": 755, "seek": 243788, "start": 2466.04, "end": 2467.2000000000003, "text": " So basically we are maintaining this.", "tokens": [51773, 407, 1936, 321, 366, 14916, 341, 13, 51831], "temperature": 0.0, "avg_logprob": -0.26693088492167366, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015859799459576607}, {"id": 756, "seek": 246788, "start": 2467.88, "end": 2473.6800000000003, "text": " We are maintaining statistics for every one of these four positions individually and independently.", "tokens": [50365, 492, 366, 14916, 12523, 337, 633, 472, 295, 613, 1451, 8432, 16652, 293, 21761, 13, 50655], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 757, "seek": 246788, "start": 2473.76, "end": 2477.88, "text": " And instead what we want to do is we want to treat this 4 as a Batch dimension,", "tokens": [50659, 400, 2602, 437, 321, 528, 281, 360, 307, 321, 528, 281, 2387, 341, 1017, 382, 257, 363, 852, 10139, 11, 50865], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 758, "seek": 246788, "start": 2477.96, "end": 2479.84, "text": " just like the 0th dimension.", "tokens": [50869, 445, 411, 264, 1958, 392, 10139, 13, 50963], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 759, "seek": 246788, "start": 2479.92, "end": 2482.8, "text": " So as far as the BatchNorm is concerned,", "tokens": [50967, 407, 382, 1400, 382, 264, 363, 852, 45, 687, 307, 5922, 11, 51111], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 760, "seek": 246788, "start": 2482.88, "end": 2484.44, "text": " it doesn't want to average...", "tokens": [51115, 309, 1177, 380, 528, 281, 4274, 485, 51193], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 761, "seek": 246788, "start": 2484.52, "end": 2486.44, "text": " We don't want to average over 32 numbers.", "tokens": [51197, 492, 500, 380, 528, 281, 4274, 670, 8858, 3547, 13, 51293], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 762, "seek": 246788, "start": 2486.52, "end": 2492.7200000000003, "text": " We want to now average over 32 times 4 numbers for every single one of these 68 channels.", "tokens": [51297, 492, 528, 281, 586, 4274, 670, 8858, 1413, 1017, 3547, 337, 633, 2167, 472, 295, 613, 23317, 9235, 13, 51607], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 763, "seek": 246788, "start": 2492.8, "end": 2497.08, "text": " And so let me now remove this.", "tokens": [51611, 400, 370, 718, 385, 586, 4159, 341, 13, 51825], "temperature": 0.0, "avg_logprob": -0.1461248068973936, "compression_ratio": 1.782258064516129, "no_speech_prob": 0.002038839738816023}, {"id": 764, "seek": 249708, "start": 2497.08, "end": 2502.3199999999997, "text": " It turns out that when you look at the documentation of Torch.mean...", "tokens": [50365, 467, 4523, 484, 300, 562, 291, 574, 412, 264, 14333, 295, 7160, 339, 13, 1398, 282, 485, 50627], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 765, "seek": 249708, "start": 2502.4, "end": 2509.6, "text": " So let's go to Torch.mean.", "tokens": [50631, 407, 718, 311, 352, 281, 7160, 339, 13, 1398, 282, 13, 50991], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 766, "seek": 249708, "start": 2509.68, "end": 2513.2, "text": " In one of its signatures, when we specify the dimension,", "tokens": [50995, 682, 472, 295, 1080, 32322, 11, 562, 321, 16500, 264, 10139, 11, 51171], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 767, "seek": 249708, "start": 2513.2799999999997, "end": 2515.2, "text": " we see that the dimension here is not just...", "tokens": [51175, 321, 536, 300, 264, 10139, 510, 307, 406, 445, 485, 51271], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 768, "seek": 249708, "start": 2515.2799999999997, "end": 2518.4, "text": " It can be int or it can also be a tuple of ints.", "tokens": [51275, 467, 393, 312, 560, 420, 309, 393, 611, 312, 257, 2604, 781, 295, 560, 82, 13, 51431], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 769, "seek": 249708, "start": 2518.48, "end": 2521.88, "text": " So we can reduce over multiple integers at the same time,", "tokens": [51435, 407, 321, 393, 5407, 670, 3866, 41674, 412, 264, 912, 565, 11, 51605], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 770, "seek": 249708, "start": 2521.96, "end": 2523.7599999999998, "text": " over multiple dimensions at the same time.", "tokens": [51609, 670, 3866, 12819, 412, 264, 912, 565, 13, 51699], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 771, "seek": 249708, "start": 2523.84, "end": 2526.84, "text": " So instead of just reducing over 0, we can pass in a tuple,", "tokens": [51703, 407, 2602, 295, 445, 12245, 670, 1958, 11, 321, 393, 1320, 294, 257, 2604, 781, 11, 51853], "temperature": 0.0, "avg_logprob": -0.1588758150736491, "compression_ratio": 1.8590909090909091, "no_speech_prob": 0.0008686914225108922}, {"id": 772, "seek": 252684, "start": 2526.84, "end": 2530.4, "text": " 0, 1, and here 0, 1 as well.", "tokens": [50365, 1958, 11, 502, 11, 293, 510, 1958, 11, 502, 382, 731, 13, 50543], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 773, "seek": 252684, "start": 2530.48, "end": 2533.84, "text": " And then what's going to happen is the output, of course, is going to be the same.", "tokens": [50547, 400, 550, 437, 311, 516, 281, 1051, 307, 264, 5598, 11, 295, 1164, 11, 307, 516, 281, 312, 264, 912, 13, 50715], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 774, "seek": 252684, "start": 2533.92, "end": 2537.2400000000002, "text": " But now what's going to happen is because we reduce over 0 and 1,", "tokens": [50719, 583, 586, 437, 311, 516, 281, 1051, 307, 570, 321, 5407, 670, 1958, 293, 502, 11, 50885], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 775, "seek": 252684, "start": 2537.32, "end": 2542.44, "text": " if we look at inmean.shape, we see that now we've reduced.", "tokens": [50889, 498, 321, 574, 412, 294, 1398, 282, 13, 82, 42406, 11, 321, 536, 300, 586, 321, 600, 9212, 13, 51145], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 776, "seek": 252684, "start": 2542.52, "end": 2546.84, "text": " We took the mean over both the 0th and the first dimension.", "tokens": [51149, 492, 1890, 264, 914, 670, 1293, 264, 1958, 392, 293, 264, 700, 10139, 13, 51365], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 777, "seek": 252684, "start": 2546.92, "end": 2550.92, "text": " So we're just getting 68 numbers and a bunch of spurious dimensions here.", "tokens": [51369, 407, 321, 434, 445, 1242, 23317, 3547, 293, 257, 3840, 295, 637, 24274, 12819, 510, 13, 51569], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 778, "seek": 252684, "start": 2551.0, "end": 2553.6400000000003, "text": " So now this becomes 1 by 1 by 68.", "tokens": [51573, 407, 586, 341, 3643, 502, 538, 502, 538, 23317, 13, 51705], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 779, "seek": 252684, "start": 2553.7200000000003, "end": 2556.1600000000003, "text": " And the running mean and the running variance,", "tokens": [51709, 400, 264, 2614, 914, 293, 264, 2614, 21977, 11, 51831], "temperature": 0.0, "avg_logprob": -0.14216831348560474, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0004888061084784567}, {"id": 780, "seek": 255616, "start": 2556.16, "end": 2558.7999999999997, "text": " analogously, will become 1 by 1 by 68.", "tokens": [50365, 16660, 5098, 11, 486, 1813, 502, 538, 502, 538, 23317, 13, 50497], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 781, "seek": 255616, "start": 2558.8799999999997, "end": 2561.04, "text": " So even though there are the spurious dimensions,", "tokens": [50501, 407, 754, 1673, 456, 366, 264, 637, 24274, 12819, 11, 50609], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 782, "seek": 255616, "start": 2561.12, "end": 2566.2, "text": " the correct thing will happen in that we are only maintaining means and variances", "tokens": [50613, 264, 3006, 551, 486, 1051, 294, 300, 321, 366, 787, 14916, 1355, 293, 1374, 21518, 50867], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 783, "seek": 255616, "start": 2566.2799999999997, "end": 2569.64, "text": " for 68 channels.", "tokens": [50871, 337, 23317, 9235, 13, 51039], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 784, "seek": 255616, "start": 2569.72, "end": 2574.2, "text": " And we're now calculating the mean and variance across 32 times 4 dimensions.", "tokens": [51043, 400, 321, 434, 586, 28258, 264, 914, 293, 21977, 2108, 8858, 1413, 1017, 12819, 13, 51267], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 785, "seek": 255616, "start": 2574.2799999999997, "end": 2576.04, "text": " So that's exactly what we want.", "tokens": [51271, 407, 300, 311, 2293, 437, 321, 528, 13, 51359], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 786, "seek": 255616, "start": 2576.12, "end": 2579.72, "text": " And let's change the implementation of BatchNorm1D that we have", "tokens": [51363, 400, 718, 311, 1319, 264, 11420, 295, 363, 852, 45, 687, 16, 35, 300, 321, 362, 51543], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 787, "seek": 255616, "start": 2579.7999999999997, "end": 2583.3999999999996, "text": " so that it can take in two-dimensional or three-dimensional inputs", "tokens": [51547, 370, 300, 309, 393, 747, 294, 732, 12, 18759, 420, 1045, 12, 18759, 15743, 51727], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 788, "seek": 255616, "start": 2583.48, "end": 2585.24, "text": " and perform accordingly.", "tokens": [51731, 293, 2042, 19717, 13, 51819], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 789, "seek": 255616, "start": 2585.3199999999997, "end": 2585.96, "text": " So at the end of the day,", "tokens": [51823, 407, 412, 264, 917, 295, 264, 786, 11, 51855], "temperature": 0.0, "avg_logprob": -0.15785789489746094, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.000980142503976822}, {"id": 790, "seek": 258596, "start": 2585.96, "end": 2588.0, "text": " the fix is relatively straightforward.", "tokens": [50365, 264, 3191, 307, 7226, 15325, 13, 50467], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 791, "seek": 258596, "start": 2588.08, "end": 2592.2400000000002, "text": " Basically, the dimension we want to reduce over is either 0", "tokens": [50471, 8537, 11, 264, 10139, 321, 528, 281, 5407, 670, 307, 2139, 1958, 50679], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 792, "seek": 258596, "start": 2592.32, "end": 2595.4, "text": " or the tuple 0 and 1, depending on the dimensionality of x.", "tokens": [50683, 420, 264, 2604, 781, 1958, 293, 502, 11, 5413, 322, 264, 10139, 1860, 295, 2031, 13, 50837], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 793, "seek": 258596, "start": 2595.48, "end": 2599.28, "text": " So if x.ndim is 2, so it's a two-dimensional tensor,", "tokens": [50841, 407, 498, 2031, 13, 273, 332, 307, 568, 11, 370, 309, 311, 257, 732, 12, 18759, 40863, 11, 51031], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 794, "seek": 258596, "start": 2599.36, "end": 2602.52, "text": " then the dimension we want to reduce over is just the integer 0.", "tokens": [51035, 550, 264, 10139, 321, 528, 281, 5407, 670, 307, 445, 264, 24922, 1958, 13, 51193], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 795, "seek": 258596, "start": 2602.6, "end": 2605.84, "text": " And if x.ndim is 3, so it's a three-dimensional tensor,", "tokens": [51197, 400, 498, 2031, 13, 273, 332, 307, 805, 11, 370, 309, 311, 257, 1045, 12, 18759, 40863, 11, 51359], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 796, "seek": 258596, "start": 2605.92, "end": 2611.44, "text": " then the dims we're going to assume are 0 and 1 that we want to reduce over.", "tokens": [51363, 550, 264, 5013, 82, 321, 434, 516, 281, 6552, 366, 1958, 293, 502, 300, 321, 528, 281, 5407, 670, 13, 51639], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 797, "seek": 258596, "start": 2611.52, "end": 2613.88, "text": " And then here, we just pass in dim.", "tokens": [51643, 400, 550, 510, 11, 321, 445, 1320, 294, 5013, 13, 51761], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 798, "seek": 258596, "start": 2613.96, "end": 2615.68, "text": " And if the dimensionality of x is anything else,", "tokens": [51765, 400, 498, 264, 10139, 1860, 295, 2031, 307, 1340, 1646, 11, 51851], "temperature": 0.0, "avg_logprob": -0.12383950332115437, "compression_ratio": 2.1201716738197427, "no_speech_prob": 0.001155613106675446}, {"id": 799, "seek": 261568, "start": 2615.68, "end": 2617.8399999999997, "text": " we're going to get an error, which is good.", "tokens": [50365, 321, 434, 516, 281, 483, 364, 6713, 11, 597, 307, 665, 13, 50473], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 800, "seek": 261568, "start": 2617.9199999999996, "end": 2620.56, "text": " So that should be the fix.", "tokens": [50477, 407, 300, 820, 312, 264, 3191, 13, 50609], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 801, "seek": 261568, "start": 2620.64, "end": 2622.3199999999997, "text": " Now, I want to point out one more thing.", "tokens": [50613, 823, 11, 286, 528, 281, 935, 484, 472, 544, 551, 13, 50697], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 802, "seek": 261568, "start": 2622.3999999999996, "end": 2625.72, "text": " We're actually departing from the API of PyTorch here a little bit,", "tokens": [50701, 492, 434, 767, 9110, 278, 490, 264, 9362, 295, 9953, 51, 284, 339, 510, 257, 707, 857, 11, 50867], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 803, "seek": 261568, "start": 2625.7999999999997, "end": 2628.56, "text": " because when you come to BatchNorm1D in PyTorch,", "tokens": [50871, 570, 562, 291, 808, 281, 363, 852, 45, 687, 16, 35, 294, 9953, 51, 284, 339, 11, 51009], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 804, "seek": 261568, "start": 2628.64, "end": 2631.72, "text": " you can scroll down and you can see that the input to this layer", "tokens": [51013, 291, 393, 11369, 760, 293, 291, 393, 536, 300, 264, 4846, 281, 341, 4583, 51167], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 805, "seek": 261568, "start": 2631.7999999999997, "end": 2634.68, "text": " can either be n by c, where n is the batch size", "tokens": [51171, 393, 2139, 312, 297, 538, 269, 11, 689, 297, 307, 264, 15245, 2744, 51315], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 806, "seek": 261568, "start": 2634.7599999999998, "end": 2636.8399999999997, "text": " and c is the number of features or channels,", "tokens": [51319, 293, 269, 307, 264, 1230, 295, 4122, 420, 9235, 11, 51423], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 807, "seek": 261568, "start": 2636.9199999999996, "end": 2639.6, "text": " or it actually does accept three-dimensional inputs,", "tokens": [51427, 420, 309, 767, 775, 3241, 1045, 12, 18759, 15743, 11, 51561], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 808, "seek": 261568, "start": 2639.68, "end": 2642.72, "text": " but it expects it to be n by c by l,", "tokens": [51565, 457, 309, 33280, 309, 281, 312, 297, 538, 269, 538, 287, 11, 51717], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 809, "seek": 261568, "start": 2642.7999999999997, "end": 2645.52, "text": " where l is, say, like the sequence length or something like that.", "tokens": [51721, 689, 287, 307, 11, 584, 11, 411, 264, 8310, 4641, 420, 746, 411, 300, 13, 51857], "temperature": 0.0, "avg_logprob": -0.16406398349338108, "compression_ratio": 1.6884735202492211, "no_speech_prob": 0.0008149543427862227}, {"id": 810, "seek": 264568, "start": 2645.68, "end": 2651.04, "text": " So this is a problem because you see how c is nested here in the middle.", "tokens": [50365, 407, 341, 307, 257, 1154, 570, 291, 536, 577, 269, 307, 15646, 292, 510, 294, 264, 2808, 13, 50633], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 811, "seek": 264568, "start": 2651.12, "end": 2654.0, "text": " And so when it gets three-dimensional inputs,", "tokens": [50637, 400, 370, 562, 309, 2170, 1045, 12, 18759, 15743, 11, 50781], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 812, "seek": 264568, "start": 2654.08, "end": 2659.12, "text": " this BatchNorm layer will reduce over 0 and 2 instead of 0 and 1.", "tokens": [50785, 341, 363, 852, 45, 687, 4583, 486, 5407, 670, 1958, 293, 568, 2602, 295, 1958, 293, 502, 13, 51037], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 813, "seek": 264568, "start": 2659.2, "end": 2666.3999999999996, "text": " So basically, PyTorch BatchNorm1D layer assumes that c will always be the first dimension,", "tokens": [51041, 407, 1936, 11, 9953, 51, 284, 339, 363, 852, 45, 687, 16, 35, 4583, 37808, 300, 269, 486, 1009, 312, 264, 700, 10139, 11, 51401], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 814, "seek": 264568, "start": 2666.48, "end": 2670.24, "text": " whereas we assume here that c is the last dimension,", "tokens": [51405, 9735, 321, 6552, 510, 300, 269, 307, 264, 1036, 10139, 11, 51593], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 815, "seek": 264568, "start": 2670.3199999999997, "end": 2672.56, "text": " and there are some number of batch dimensions beforehand.", "tokens": [51597, 293, 456, 366, 512, 1230, 295, 15245, 12819, 22893, 13, 51709], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 816, "seek": 264568, "start": 2672.64, "end": 2675.44, "text": " And so,", "tokens": [51713, 400, 370, 11, 51853], "temperature": 0.0, "avg_logprob": -0.22138925032182175, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.0005346463876776397}, {"id": 817, "seek": 267544, "start": 2675.44, "end": 2677.44, "text": " it expects n by c or n by c by l.", "tokens": [50365, 309, 33280, 297, 538, 269, 420, 297, 538, 269, 538, 287, 13, 50465], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 818, "seek": 267544, "start": 2677.52, "end": 2679.44, "text": " We expect n by c or n by l by c.", "tokens": [50469, 492, 2066, 297, 538, 269, 420, 297, 538, 287, 538, 269, 13, 50565], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 819, "seek": 267544, "start": 2679.52, "end": 2681.52, "text": " And so, it's a deviation.", "tokens": [50569, 400, 370, 11, 309, 311, 257, 25163, 13, 50669], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 820, "seek": 267544, "start": 2681.6, "end": 2683.6, "text": " I think it's okay.", "tokens": [50673, 286, 519, 309, 311, 1392, 13, 50773], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 821, "seek": 267544, "start": 2683.68, "end": 2685.68, "text": " I prefer it this way, honestly,", "tokens": [50777, 286, 4382, 309, 341, 636, 11, 6095, 11, 50877], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 822, "seek": 267544, "start": 2685.76, "end": 2688.4, "text": " so this is the way that we will keep it for our purposes.", "tokens": [50881, 370, 341, 307, 264, 636, 300, 321, 486, 1066, 309, 337, 527, 9932, 13, 51013], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 823, "seek": 267544, "start": 2688.48, "end": 2691.2000000000003, "text": " So I redefined the layers, reinitialized the neural net,", "tokens": [51017, 407, 286, 38818, 2001, 264, 7914, 11, 6561, 270, 831, 1602, 264, 18161, 2533, 11, 51153], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 824, "seek": 267544, "start": 2691.28, "end": 2694.48, "text": " and did a single forward pass with a break just for one step.", "tokens": [51157, 293, 630, 257, 2167, 2128, 1320, 365, 257, 1821, 445, 337, 472, 1823, 13, 51317], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 825, "seek": 267544, "start": 2694.56, "end": 2697.6, "text": " Looking at the shapes along the way, they're, of course, identical.", "tokens": [51321, 11053, 412, 264, 10854, 2051, 264, 636, 11, 436, 434, 11, 295, 1164, 11, 14800, 13, 51473], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 826, "seek": 267544, "start": 2697.68, "end": 2699.28, "text": " All the shapes are the same,", "tokens": [51477, 1057, 264, 10854, 366, 264, 912, 11, 51557], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 827, "seek": 267544, "start": 2699.36, "end": 2702.56, "text": " but the way we see that things are actually working as we want them to,", "tokens": [51561, 457, 264, 636, 321, 536, 300, 721, 366, 767, 1364, 382, 321, 528, 552, 281, 11, 51721], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 828, "seek": 267544, "start": 2702.64, "end": 2704.88, "text": " is that we can actually do the same thing.", "tokens": [51725, 307, 300, 321, 393, 767, 360, 264, 912, 551, 13, 51837], "temperature": 0.0, "avg_logprob": -0.3483796701198671, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.0015016146935522556}, {"id": 829, "seek": 270488, "start": 2704.88, "end": 2706.88, "text": " So the way we see that things are actually working as we want them to now", "tokens": [50365, 407, 264, 636, 321, 536, 300, 721, 366, 767, 1364, 382, 321, 528, 552, 281, 586, 50465], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 830, "seek": 270488, "start": 2706.96, "end": 2708.88, "text": " is that when we look at the BatchNorm layer,", "tokens": [50469, 307, 300, 562, 321, 574, 412, 264, 363, 852, 45, 687, 4583, 11, 50565], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 831, "seek": 270488, "start": 2708.96, "end": 2710.88, "text": " the running mean shape is now 1 by 1 by 68.", "tokens": [50569, 264, 2614, 914, 3909, 307, 586, 502, 538, 502, 538, 23317, 13, 50665], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 832, "seek": 270488, "start": 2710.96, "end": 2714.8, "text": " So we're only maintaining 68 means for every one of our channels,", "tokens": [50669, 407, 321, 434, 787, 14916, 23317, 1355, 337, 633, 472, 295, 527, 9235, 11, 50861], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 833, "seek": 270488, "start": 2714.88, "end": 2718.8, "text": " and we're treating both the 0th and the first dimension as a batch dimension,", "tokens": [50865, 293, 321, 434, 15083, 1293, 264, 1958, 392, 293, 264, 700, 10139, 382, 257, 15245, 10139, 11, 51061], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 834, "seek": 270488, "start": 2718.88, "end": 2720.8, "text": " which is exactly what we want.", "tokens": [51065, 597, 307, 2293, 437, 321, 528, 13, 51161], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 835, "seek": 270488, "start": 2720.88, "end": 2722.8, "text": " So let me retrain the neural net now.", "tokens": [51165, 407, 718, 385, 1533, 7146, 264, 18161, 2533, 586, 13, 51261], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 836, "seek": 270488, "start": 2722.88, "end": 2724.8, "text": " Okay, so I've retrained the neural net with the bug fix.", "tokens": [51265, 1033, 11, 370, 286, 600, 1533, 31774, 264, 18161, 2533, 365, 264, 7426, 3191, 13, 51361], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 837, "seek": 270488, "start": 2724.88, "end": 2726.8, "text": " We get a nice curve.", "tokens": [51365, 492, 483, 257, 1481, 7605, 13, 51461], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 838, "seek": 270488, "start": 2726.88, "end": 2728.8, "text": " And when we look at the validation performance,", "tokens": [51465, 400, 562, 321, 574, 412, 264, 24071, 3389, 11, 51561], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 839, "seek": 270488, "start": 2728.88, "end": 2730.8, "text": " we do actually see a slight improvement.", "tokens": [51565, 321, 360, 767, 536, 257, 4036, 10444, 13, 51661], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 840, "seek": 270488, "start": 2730.88, "end": 2732.8, "text": " So it went from 2.029 to 2.022.", "tokens": [51665, 407, 309, 1437, 490, 568, 13, 15, 11871, 281, 568, 13, 15, 7490, 13, 51761], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 841, "seek": 270488, "start": 2732.88, "end": 2734.8, "text": " So basically, the bug inside the BatchNorm was holding us back, like,", "tokens": [51765, 407, 1936, 11, 264, 7426, 1854, 264, 363, 852, 45, 687, 390, 5061, 505, 646, 11, 411, 11, 51861], "temperature": 0.0, "avg_logprob": -0.18994095504924816, "compression_ratio": 1.7741046831955922, "no_speech_prob": 0.000998533214442432}, {"id": 842, "seek": 273488, "start": 2734.88, "end": 2736.8, "text": " a little bit, it looks like.", "tokens": [50365, 257, 707, 857, 11, 309, 1542, 411, 13, 50461], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 843, "seek": 273488, "start": 2736.88, "end": 2738.8, "text": " And we are getting a tiny improvement now,", "tokens": [50465, 400, 321, 366, 1242, 257, 5870, 10444, 586, 11, 50561], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 844, "seek": 273488, "start": 2738.88, "end": 2742.8, "text": " but it's not clear if this is statistically significant.", "tokens": [50565, 457, 309, 311, 406, 1850, 498, 341, 307, 36478, 4776, 13, 50761], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 845, "seek": 273488, "start": 2742.88, "end": 2744.8, "text": " And the reason we slightly expect an improvement", "tokens": [50765, 400, 264, 1778, 321, 4748, 2066, 364, 10444, 50861], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 846, "seek": 273488, "start": 2744.88, "end": 2748.8, "text": " is because we're not maintaining so many different means and variances", "tokens": [50865, 307, 570, 321, 434, 406, 14916, 370, 867, 819, 1355, 293, 1374, 21518, 51061], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 847, "seek": 273488, "start": 2748.88, "end": 2750.8, "text": " that are only estimated using 32 numbers, effectively.", "tokens": [51065, 300, 366, 787, 14109, 1228, 8858, 3547, 11, 8659, 13, 51161], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 848, "seek": 273488, "start": 2750.88, "end": 2754.8, "text": " Now we are estimating them using 32 times 4 numbers.", "tokens": [51165, 823, 321, 366, 8017, 990, 552, 1228, 8858, 1413, 1017, 3547, 13, 51361], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 849, "seek": 273488, "start": 2754.88, "end": 2756.8, "text": " So you just have a lot more numbers", "tokens": [51365, 407, 291, 445, 362, 257, 688, 544, 3547, 51461], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 850, "seek": 273488, "start": 2756.88, "end": 2758.8, "text": " that go into any one estimate of the mean and variance.", "tokens": [51465, 300, 352, 666, 604, 472, 12539, 295, 264, 914, 293, 21977, 13, 51561], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 851, "seek": 273488, "start": 2758.88, "end": 2762.8, "text": " And it allows things to be a bit more stable and less wiggly", "tokens": [51565, 400, 309, 4045, 721, 281, 312, 257, 857, 544, 8351, 293, 1570, 261, 46737, 51761], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 852, "seek": 273488, "start": 2762.88, "end": 2764.8, "text": " inside those estimates of the BatchNorm.", "tokens": [51765, 1854, 729, 20561, 295, 264, 363, 852, 45, 687, 13, 51861], "temperature": 0.0, "avg_logprob": -0.08843465473340906, "compression_ratio": 1.7571884984025559, "no_speech_prob": 0.0003908985818270594}, {"id": 853, "seek": 276488, "start": 2764.88, "end": 2766.8, "text": " So pretty nice.", "tokens": [50365, 407, 1238, 1481, 13, 50461], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 854, "seek": 276488, "start": 2766.88, "end": 2768.8, "text": " With this more general architecture in place,", "tokens": [50465, 2022, 341, 544, 2674, 9482, 294, 1081, 11, 50561], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 855, "seek": 276488, "start": 2768.88, "end": 2770.8, "text": " we are now set up to push the performance further", "tokens": [50565, 321, 366, 586, 992, 493, 281, 2944, 264, 3389, 3052, 50661], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 856, "seek": 276488, "start": 2770.88, "end": 2772.8, "text": " by increasing the size of the network.", "tokens": [50665, 538, 5662, 264, 2744, 295, 264, 3209, 13, 50761], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 857, "seek": 276488, "start": 2772.88, "end": 2774.8, "text": " So, for example,", "tokens": [50765, 407, 11, 337, 1365, 11, 50861], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 858, "seek": 276488, "start": 2774.88, "end": 2776.8, "text": " I've bumped up the number of embeddings to 24 instead of 10,", "tokens": [50865, 286, 600, 42696, 493, 264, 1230, 295, 12240, 29432, 281, 4022, 2602, 295, 1266, 11, 50961], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 859, "seek": 276488, "start": 2776.88, "end": 2778.8, "text": " and also increased the number of hidden units.", "tokens": [50965, 293, 611, 6505, 264, 1230, 295, 7633, 6815, 13, 51061], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 860, "seek": 276488, "start": 2778.88, "end": 2780.8, "text": " But using the exact same architecture,", "tokens": [51065, 583, 1228, 264, 1900, 912, 9482, 11, 51161], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 861, "seek": 276488, "start": 2780.88, "end": 2782.8, "text": " we now have 76,000 parameters,", "tokens": [51165, 321, 586, 362, 24733, 11, 1360, 9834, 11, 51261], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 862, "seek": 276488, "start": 2782.88, "end": 2784.8, "text": " and the training takes a lot longer,", "tokens": [51265, 293, 264, 3097, 2516, 257, 688, 2854, 11, 51361], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 863, "seek": 276488, "start": 2784.88, "end": 2786.8, "text": " but we do get a nice curve.", "tokens": [51365, 457, 321, 360, 483, 257, 1481, 7605, 13, 51461], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 864, "seek": 276488, "start": 2786.88, "end": 2788.8, "text": " And then when you actually evaluate the performance,", "tokens": [51465, 400, 550, 562, 291, 767, 13059, 264, 3389, 11, 51561], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 865, "seek": 276488, "start": 2788.88, "end": 2790.8, "text": " we are now getting validation performance of 1.993.", "tokens": [51565, 321, 366, 586, 1242, 24071, 3389, 295, 502, 13, 8494, 18, 13, 51661], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 866, "seek": 276488, "start": 2790.88, "end": 2792.8, "text": " So we've crossed over 1.993.", "tokens": [51665, 407, 321, 600, 14622, 670, 502, 13, 8494, 18, 13, 51761], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 867, "seek": 276488, "start": 2792.88, "end": 2794.8, "text": " So we've crossed over 1.993.", "tokens": [51765, 407, 321, 600, 14622, 670, 502, 13, 8494, 18, 13, 51861], "temperature": 0.0, "avg_logprob": -0.16352626423776886, "compression_ratio": 1.7962382445141065, "no_speech_prob": 0.0009770127944648266}, {"id": 868, "seek": 279488, "start": 2794.88, "end": 2796.8, "text": " We've crossed over the 2.0 sort of territory.", "tokens": [50365, 492, 600, 14622, 670, 264, 568, 13, 15, 1333, 295, 11360, 13, 50461], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 869, "seek": 279488, "start": 2796.88, "end": 2798.8, "text": " And we're at about 1.99.", "tokens": [50465, 400, 321, 434, 412, 466, 502, 13, 8494, 13, 50561], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 870, "seek": 279488, "start": 2798.88, "end": 2800.8, "text": " But we are starting to have to wait quite a bit longer.", "tokens": [50565, 583, 321, 366, 2891, 281, 362, 281, 1699, 1596, 257, 857, 2854, 13, 50661], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 871, "seek": 279488, "start": 2800.88, "end": 2802.8, "text": " But we are starting to have to wait quite a bit longer.", "tokens": [50665, 583, 321, 366, 2891, 281, 362, 281, 1699, 1596, 257, 857, 2854, 13, 50761], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 872, "seek": 279488, "start": 2802.88, "end": 2804.8, "text": " And we're a little bit in the dark", "tokens": [50765, 400, 321, 434, 257, 707, 857, 294, 264, 2877, 50861], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 873, "seek": 279488, "start": 2804.88, "end": 2806.8, "text": " with respect to the correct setting of the hyperparameters here", "tokens": [50865, 365, 3104, 281, 264, 3006, 3287, 295, 264, 9848, 2181, 335, 6202, 510, 50961], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 874, "seek": 279488, "start": 2806.88, "end": 2808.8, "text": " and the learning rates and so on,", "tokens": [50965, 293, 264, 2539, 6846, 293, 370, 322, 11, 51061], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 875, "seek": 279488, "start": 2808.88, "end": 2810.8, "text": " because the experiments are starting to take longer to train.", "tokens": [51065, 570, 264, 12050, 366, 2891, 281, 747, 2854, 281, 3847, 13, 51161], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 876, "seek": 279488, "start": 2810.88, "end": 2812.8, "text": " And so we are missing sort of like an experimental harness", "tokens": [51165, 400, 370, 321, 366, 5361, 1333, 295, 411, 364, 17069, 19700, 51261], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 877, "seek": 279488, "start": 2812.88, "end": 2814.8, "text": " on which we could run a number of experiments", "tokens": [51265, 322, 597, 321, 727, 1190, 257, 1230, 295, 12050, 51361], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 878, "seek": 279488, "start": 2814.88, "end": 2816.8, "text": " on which we could run a number of experiments", "tokens": [51365, 322, 597, 321, 727, 1190, 257, 1230, 295, 12050, 51461], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 879, "seek": 279488, "start": 2816.88, "end": 2818.8, "text": " and really tune this architecture very well.", "tokens": [51465, 293, 534, 10864, 341, 9482, 588, 731, 13, 51561], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 880, "seek": 279488, "start": 2818.88, "end": 2820.8, "text": " So I'd like to conclude now with a few notes.", "tokens": [51565, 407, 286, 1116, 411, 281, 16886, 586, 365, 257, 1326, 5570, 13, 51661], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 881, "seek": 279488, "start": 2820.88, "end": 2822.8, "text": " We basically improved our performance", "tokens": [51665, 492, 1936, 9689, 527, 3389, 51761], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 882, "seek": 279488, "start": 2822.88, "end": 2824.8, "text": " from a starting of 2.1", "tokens": [51765, 490, 257, 2891, 295, 568, 13, 16, 51861], "temperature": 0.0, "avg_logprob": -0.07506109206057385, "compression_ratio": 1.9941348973607038, "no_speech_prob": 0.0012745308922603726}, {"id": 883, "seek": 282480, "start": 2824.8, "end": 2826.7200000000003, "text": " to 2.9.", "tokens": [50365, 281, 568, 13, 24, 13, 50461], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 884, "seek": 282480, "start": 2826.8, "end": 2828.7200000000003, "text": " But I don't want that to be the focus", "tokens": [50465, 583, 286, 500, 380, 528, 300, 281, 312, 264, 1879, 50561], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 885, "seek": 282480, "start": 2828.8, "end": 2830.7200000000003, "text": " because honestly we're kind of in the dark.", "tokens": [50565, 570, 6095, 321, 434, 733, 295, 294, 264, 2877, 13, 50661], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 886, "seek": 282480, "start": 2830.8, "end": 2832.7200000000003, "text": " We have no experimental harness.", "tokens": [50665, 492, 362, 572, 17069, 19700, 13, 50761], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 887, "seek": 282480, "start": 2832.8, "end": 2834.7200000000003, "text": " We're just guessing and checking.", "tokens": [50765, 492, 434, 445, 17939, 293, 8568, 13, 50861], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 888, "seek": 282480, "start": 2834.8, "end": 2836.7200000000003, "text": " And this whole thing is terrible.", "tokens": [50865, 400, 341, 1379, 551, 307, 6237, 13, 50961], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 889, "seek": 282480, "start": 2836.8, "end": 2838.7200000000003, "text": " We're just looking at the training loss.", "tokens": [50965, 492, 434, 445, 1237, 412, 264, 3097, 4470, 13, 51061], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 890, "seek": 282480, "start": 2838.8, "end": 2840.7200000000003, "text": " Normally you want to look at both the training", "tokens": [51065, 17424, 291, 528, 281, 574, 412, 1293, 264, 3097, 51161], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 891, "seek": 282480, "start": 2840.8, "end": 2842.7200000000003, "text": " and the validation loss together.", "tokens": [51165, 293, 264, 24071, 4470, 1214, 13, 51261], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 892, "seek": 282480, "start": 2842.8, "end": 2844.7200000000003, "text": " The whole thing looks different", "tokens": [51265, 440, 1379, 551, 1542, 819, 51361], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 893, "seek": 282480, "start": 2844.8, "end": 2846.7200000000003, "text": " if you're actually trying to squeeze out numbers.", "tokens": [51365, 498, 291, 434, 767, 1382, 281, 13578, 484, 3547, 13, 51461], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 894, "seek": 282480, "start": 2846.8, "end": 2848.7200000000003, "text": " That said, we did implement this architecture", "tokens": [51465, 663, 848, 11, 321, 630, 4445, 341, 9482, 51561], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 895, "seek": 282480, "start": 2848.8, "end": 2850.7200000000003, "text": " from the WaveNet paper.", "tokens": [51565, 490, 264, 28530, 31890, 3035, 13, 51661], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 896, "seek": 282480, "start": 2850.8, "end": 2852.7200000000003, "text": " But we did not implement this specific forward pass of it", "tokens": [51665, 583, 321, 630, 406, 4445, 341, 2685, 2128, 1320, 295, 309, 51761], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 897, "seek": 282480, "start": 2852.8, "end": 2854.7200000000003, "text": " where you have a more complicated", "tokens": [51765, 689, 291, 362, 257, 544, 6179, 51861], "temperature": 0.0, "avg_logprob": -0.07729444698411592, "compression_ratio": 1.7375, "no_speech_prob": 0.0022067115642130375}, {"id": 898, "seek": 285472, "start": 2854.72, "end": 2856.64, "text": " structure that is this gated", "tokens": [50365, 3877, 300, 307, 341, 290, 770, 50461], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 899, "seek": 285472, "start": 2856.72, "end": 2858.64, "text": " linear layer kind of.", "tokens": [50465, 8213, 4583, 733, 295, 13, 50561], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 900, "seek": 285472, "start": 2858.72, "end": 2860.64, "text": " And there's residual connections and skip connections", "tokens": [50565, 400, 456, 311, 27980, 9271, 293, 10023, 9271, 50661], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 901, "seek": 285472, "start": 2860.72, "end": 2862.64, "text": " and so on. So we did not implement that.", "tokens": [50665, 293, 370, 322, 13, 407, 321, 630, 406, 4445, 300, 13, 50761], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 902, "seek": 285472, "start": 2862.72, "end": 2864.64, "text": " We just implemented this structure.", "tokens": [50765, 492, 445, 12270, 341, 3877, 13, 50861], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 903, "seek": 285472, "start": 2864.72, "end": 2866.64, "text": " I would like to briefly hint or preview", "tokens": [50865, 286, 576, 411, 281, 10515, 12075, 420, 14281, 50961], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 904, "seek": 285472, "start": 2866.72, "end": 2868.64, "text": " how what we've done here relates", "tokens": [50965, 577, 437, 321, 600, 1096, 510, 16155, 51061], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 905, "seek": 285472, "start": 2868.72, "end": 2870.64, "text": " to convolutional neural networks", "tokens": [51065, 281, 45216, 304, 18161, 9590, 51161], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 906, "seek": 285472, "start": 2870.72, "end": 2872.64, "text": " as used in the WaveNet paper.", "tokens": [51165, 382, 1143, 294, 264, 28530, 31890, 3035, 13, 51261], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 907, "seek": 285472, "start": 2872.72, "end": 2874.64, "text": " And basically the use of convolutions", "tokens": [51265, 400, 1936, 264, 764, 295, 3754, 15892, 51361], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 908, "seek": 285472, "start": 2874.72, "end": 2876.64, "text": " is strictly for efficiency.", "tokens": [51365, 307, 20792, 337, 10493, 13, 51461], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 909, "seek": 285472, "start": 2876.72, "end": 2878.64, "text": " It doesn't actually change the model we've implemented.", "tokens": [51465, 467, 1177, 380, 767, 1319, 264, 2316, 321, 600, 12270, 13, 51561], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 910, "seek": 285472, "start": 2878.72, "end": 2880.64, "text": " So here for example,", "tokens": [51565, 407, 510, 337, 1365, 11, 51661], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 911, "seek": 285472, "start": 2880.72, "end": 2882.64, "text": " let me look at a specific name", "tokens": [51665, 718, 385, 574, 412, 257, 2685, 1315, 51761], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 912, "seek": 285472, "start": 2882.72, "end": 2884.64, "text": " to work with an example.", "tokens": [51765, 281, 589, 365, 364, 1365, 13, 51861], "temperature": 0.0, "avg_logprob": -0.09969384826882913, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.0006571089616045356}, {"id": 913, "seek": 288464, "start": 2884.64, "end": 2886.56, "text": " So we have a name in our training set", "tokens": [50365, 407, 321, 362, 257, 1315, 294, 527, 3097, 992, 50461], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 914, "seek": 288464, "start": 2886.64, "end": 2888.56, "text": " and it's D'Andre.", "tokens": [50465, 293, 309, 311, 413, 6, 5289, 265, 13, 50561], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 915, "seek": 288464, "start": 2888.64, "end": 2890.56, "text": " And it has seven letters.", "tokens": [50565, 400, 309, 575, 3407, 7825, 13, 50661], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 916, "seek": 288464, "start": 2890.64, "end": 2892.56, "text": " So that is eight independent examples in our model.", "tokens": [50665, 407, 300, 307, 3180, 6695, 5110, 294, 527, 2316, 13, 50761], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 917, "seek": 288464, "start": 2892.64, "end": 2894.56, "text": " So all these rows here", "tokens": [50765, 407, 439, 613, 13241, 510, 50861], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 918, "seek": 288464, "start": 2894.64, "end": 2896.56, "text": " are independent examples of D'Andre.", "tokens": [50865, 366, 6695, 5110, 295, 413, 6, 5289, 265, 13, 50961], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 919, "seek": 288464, "start": 2896.64, "end": 2898.56, "text": " Now you can forward of course", "tokens": [50965, 823, 291, 393, 2128, 295, 1164, 51061], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 920, "seek": 288464, "start": 2898.64, "end": 2900.56, "text": " any one of these rows independently.", "tokens": [51065, 604, 472, 295, 613, 13241, 21761, 13, 51161], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 921, "seek": 288464, "start": 2900.64, "end": 2902.56, "text": " So I can take my model", "tokens": [51165, 407, 286, 393, 747, 452, 2316, 51261], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 922, "seek": 288464, "start": 2902.64, "end": 2904.56, "text": " and call it on", "tokens": [51265, 293, 818, 309, 322, 51361], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 923, "seek": 288464, "start": 2904.64, "end": 2906.56, "text": " any individual index.", "tokens": [51365, 604, 2609, 8186, 13, 51461], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 924, "seek": 288464, "start": 2906.64, "end": 2908.56, "text": " Notice by the way here", "tokens": [51465, 13428, 538, 264, 636, 510, 51561], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 925, "seek": 288464, "start": 2908.64, "end": 2910.56, "text": " I'm being a little bit tricky.", "tokens": [51565, 286, 478, 885, 257, 707, 857, 12414, 13, 51661], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 926, "seek": 288464, "start": 2910.64, "end": 2912.56, "text": " The reason for this is that", "tokens": [51665, 440, 1778, 337, 341, 307, 300, 51761], "temperature": 0.0, "avg_logprob": -0.12474998226010703, "compression_ratio": 1.6962025316455696, "no_speech_prob": 0.002250876510515809}, {"id": 927, "seek": 291256, "start": 2912.56, "end": 2916.48, "text": " it's a one dimensional array of eight.", "tokens": [50365, 309, 311, 257, 472, 18795, 10225, 295, 3180, 13, 50561], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 928, "seek": 291256, "start": 2916.56, "end": 2918.48, "text": " So you can't actually call the model on it.", "tokens": [50565, 407, 291, 393, 380, 767, 818, 264, 2316, 322, 309, 13, 50661], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 929, "seek": 291256, "start": 2918.56, "end": 2920.48, "text": " You're going to get an error", "tokens": [50665, 509, 434, 516, 281, 483, 364, 6713, 50761], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 930, "seek": 291256, "start": 2920.56, "end": 2922.48, "text": " because there's no batch dimension.", "tokens": [50765, 570, 456, 311, 572, 15245, 10139, 13, 50861], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 931, "seek": 291256, "start": 2922.56, "end": 2924.48, "text": " So when you do extra at", "tokens": [50865, 407, 562, 291, 360, 2857, 412, 50961], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 932, "seek": 291256, "start": 2924.56, "end": 2926.48, "text": " a list of seven", "tokens": [50965, 257, 1329, 295, 3407, 51061], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 933, "seek": 291256, "start": 2926.56, "end": 2928.48, "text": " then the shape of this becomes one by eight.", "tokens": [51065, 550, 264, 3909, 295, 341, 3643, 472, 538, 3180, 13, 51161], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 934, "seek": 291256, "start": 2928.56, "end": 2930.48, "text": " So I get an extra batch dimension", "tokens": [51165, 407, 286, 483, 364, 2857, 15245, 10139, 51261], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 935, "seek": 291256, "start": 2930.56, "end": 2932.48, "text": " of one and then we can forward the model.", "tokens": [51265, 295, 472, 293, 550, 321, 393, 2128, 264, 2316, 13, 51361], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 936, "seek": 291256, "start": 2932.56, "end": 2934.48, "text": " So", "tokens": [51365, 407, 51461], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 937, "seek": 291256, "start": 2934.56, "end": 2936.48, "text": " that forwards a single example", "tokens": [51465, 300, 30126, 257, 2167, 1365, 51561], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 938, "seek": 291256, "start": 2936.56, "end": 2938.48, "text": " and you might imagine that you actually", "tokens": [51565, 293, 291, 1062, 3811, 300, 291, 767, 51661], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 939, "seek": 291256, "start": 2938.56, "end": 2940.48, "text": " may want to forward all of these eight", "tokens": [51665, 815, 528, 281, 2128, 439, 295, 613, 3180, 51761], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 940, "seek": 291256, "start": 2940.56, "end": 2942.48, "text": " at the same time.", "tokens": [51765, 412, 264, 912, 565, 13, 51861], "temperature": 0.0, "avg_logprob": -0.11291935687928688, "compression_ratio": 1.8215767634854771, "no_speech_prob": 0.0007760677835904062}, {"id": 941, "seek": 294248, "start": 2942.48, "end": 2944.4, "text": " So pre-allocating some memory", "tokens": [50365, 407, 659, 12, 336, 905, 990, 512, 4675, 50461], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 942, "seek": 294248, "start": 2944.48, "end": 2946.4, "text": " and then doing a for loop", "tokens": [50465, 293, 550, 884, 257, 337, 6367, 50561], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 943, "seek": 294248, "start": 2946.48, "end": 2948.4, "text": " eight times and forwarding all of those", "tokens": [50565, 3180, 1413, 293, 2128, 278, 439, 295, 729, 50661], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 944, "seek": 294248, "start": 2948.48, "end": 2950.4, "text": " eight here will give us", "tokens": [50665, 3180, 510, 486, 976, 505, 50761], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 945, "seek": 294248, "start": 2950.48, "end": 2952.4, "text": " all the logits in all these different cases.", "tokens": [50765, 439, 264, 3565, 1208, 294, 439, 613, 819, 3331, 13, 50861], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 946, "seek": 294248, "start": 2952.48, "end": 2954.4, "text": " Now for us with the model", "tokens": [50865, 823, 337, 505, 365, 264, 2316, 50961], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 947, "seek": 294248, "start": 2954.48, "end": 2956.4, "text": " as we've implemented it right now", "tokens": [50965, 382, 321, 600, 12270, 309, 558, 586, 51061], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 948, "seek": 294248, "start": 2956.48, "end": 2958.4, "text": " this is eight independent calls to our model.", "tokens": [51065, 341, 307, 3180, 6695, 5498, 281, 527, 2316, 13, 51161], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 949, "seek": 294248, "start": 2958.48, "end": 2960.4, "text": " But what convolutions allow you to do", "tokens": [51165, 583, 437, 3754, 15892, 2089, 291, 281, 360, 51261], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 950, "seek": 294248, "start": 2960.48, "end": 2962.4, "text": " is it allow you to basically slide", "tokens": [51265, 307, 309, 2089, 291, 281, 1936, 4137, 51361], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 951, "seek": 294248, "start": 2962.48, "end": 2964.4, "text": " this model efficiently", "tokens": [51365, 341, 2316, 19621, 51461], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 952, "seek": 294248, "start": 2964.48, "end": 2966.4, "text": " over the input sequence.", "tokens": [51465, 670, 264, 4846, 8310, 13, 51561], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 953, "seek": 294248, "start": 2966.48, "end": 2968.4, "text": " And so this for loop can be done", "tokens": [51565, 400, 370, 341, 337, 6367, 393, 312, 1096, 51661], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 954, "seek": 294248, "start": 2968.48, "end": 2970.4, "text": " not outside in Python", "tokens": [51665, 406, 2380, 294, 15329, 51761], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 955, "seek": 294248, "start": 2970.48, "end": 2972.4, "text": " but inside of kernels in CUDA.", "tokens": [51765, 457, 1854, 295, 23434, 1625, 294, 29777, 7509, 13, 51861], "temperature": 0.0, "avg_logprob": -0.06912591564121531, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0009895398980006576}, {"id": 956, "seek": 297248, "start": 2972.48, "end": 2974.4, "text": " And so this for loop gets hidden into the convolution.", "tokens": [50365, 400, 370, 341, 337, 6367, 2170, 7633, 666, 264, 45216, 13, 50461], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 957, "seek": 297248, "start": 2974.48, "end": 2976.4, "text": " So the convolution", "tokens": [50465, 407, 264, 45216, 50561], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 958, "seek": 297248, "start": 2976.48, "end": 2978.4, "text": " basically you can think of it as", "tokens": [50565, 1936, 291, 393, 519, 295, 309, 382, 50661], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 959, "seek": 297248, "start": 2978.48, "end": 2980.4, "text": " it's a for loop applying a little linear", "tokens": [50665, 309, 311, 257, 337, 6367, 9275, 257, 707, 8213, 50761], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 960, "seek": 297248, "start": 2980.48, "end": 2982.4, "text": " filter over space", "tokens": [50765, 6608, 670, 1901, 50861], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 961, "seek": 297248, "start": 2982.48, "end": 2984.4, "text": " of some input sequence.", "tokens": [50865, 295, 512, 4846, 8310, 13, 50961], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 962, "seek": 297248, "start": 2984.48, "end": 2986.4, "text": " And in our case the space we're interested in is one dimensional", "tokens": [50965, 400, 294, 527, 1389, 264, 1901, 321, 434, 3102, 294, 307, 472, 18795, 51061], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 963, "seek": 297248, "start": 2986.48, "end": 2988.4, "text": " and we're interested in sliding these filters", "tokens": [51065, 293, 321, 434, 3102, 294, 21169, 613, 15995, 51161], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 964, "seek": 297248, "start": 2988.48, "end": 2990.4, "text": " over the input data.", "tokens": [51165, 670, 264, 4846, 1412, 13, 51261], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 965, "seek": 297248, "start": 2990.48, "end": 2992.4, "text": " So this diagram", "tokens": [51265, 407, 341, 10686, 51361], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 966, "seek": 297248, "start": 2992.48, "end": 2994.4, "text": " actually is fairly good as well.", "tokens": [51365, 767, 307, 6457, 665, 382, 731, 13, 51461], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 967, "seek": 297248, "start": 2994.48, "end": 2996.4, "text": " Basically what we've done is", "tokens": [51465, 8537, 437, 321, 600, 1096, 307, 51561], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 968, "seek": 297248, "start": 2996.48, "end": 2998.4, "text": " here they are highlighting in black", "tokens": [51565, 510, 436, 366, 26551, 294, 2211, 51661], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 969, "seek": 297248, "start": 2998.48, "end": 3000.4, "text": " one single sort of like tree", "tokens": [51665, 472, 2167, 1333, 295, 411, 4230, 51761], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 970, "seek": 297248, "start": 3000.48, "end": 3002.4, "text": " of this calculation.", "tokens": [51765, 295, 341, 17108, 13, 51861], "temperature": 0.0, "avg_logprob": -0.07962927967309952, "compression_ratio": 1.77007299270073, "no_speech_prob": 0.0004859231994487345}, {"id": 971, "seek": 300240, "start": 3002.4, "end": 3004.32, "text": " So just calculating the single output", "tokens": [50365, 407, 445, 28258, 264, 2167, 5598, 50461], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 972, "seek": 300240, "start": 3004.4, "end": 3006.32, "text": " example here.", "tokens": [50465, 1365, 510, 13, 50561], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 973, "seek": 300240, "start": 3006.4, "end": 3008.32, "text": " And so this is basically", "tokens": [50565, 400, 370, 341, 307, 1936, 50661], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 974, "seek": 300240, "start": 3008.4, "end": 3010.32, "text": " what we've implemented here.", "tokens": [50665, 437, 321, 600, 12270, 510, 13, 50761], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 975, "seek": 300240, "start": 3010.4, "end": 3012.32, "text": " We've implemented a single, this black structure", "tokens": [50765, 492, 600, 12270, 257, 2167, 11, 341, 2211, 3877, 50861], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 976, "seek": 300240, "start": 3012.4, "end": 3014.32, "text": " we've implemented that", "tokens": [50865, 321, 600, 12270, 300, 50961], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 977, "seek": 300240, "start": 3014.4, "end": 3016.32, "text": " and calculated a single output, like a single example.", "tokens": [50965, 293, 15598, 257, 2167, 5598, 11, 411, 257, 2167, 1365, 13, 51061], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 978, "seek": 300240, "start": 3016.4, "end": 3018.32, "text": " But what convolutions", "tokens": [51065, 583, 437, 3754, 15892, 51161], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 979, "seek": 300240, "start": 3018.4, "end": 3020.32, "text": " allow you to do is it allows you to take", "tokens": [51165, 2089, 291, 281, 360, 307, 309, 4045, 291, 281, 747, 51261], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 980, "seek": 300240, "start": 3020.4, "end": 3022.32, "text": " this black structure and", "tokens": [51265, 341, 2211, 3877, 293, 51361], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 981, "seek": 300240, "start": 3022.4, "end": 3024.32, "text": " kind of like slide it over the input sequence", "tokens": [51365, 733, 295, 411, 4137, 309, 670, 264, 4846, 8310, 51461], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 982, "seek": 300240, "start": 3024.4, "end": 3026.32, "text": " here and calculate", "tokens": [51465, 510, 293, 8873, 51561], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 983, "seek": 300240, "start": 3026.4, "end": 3028.32, "text": " all of these orange", "tokens": [51565, 439, 295, 613, 7671, 51661], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 984, "seek": 300240, "start": 3028.4, "end": 3030.32, "text": " outputs at the same time.", "tokens": [51665, 23930, 412, 264, 912, 565, 13, 51761], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 985, "seek": 300240, "start": 3030.4, "end": 3032.32, "text": " Or here that corresponds to calculating", "tokens": [51765, 1610, 510, 300, 23249, 281, 28258, 51861], "temperature": 0.0, "avg_logprob": -0.09618640336834017, "compression_ratio": 1.9462809917355373, "no_speech_prob": 6.070205563446507e-05}, {"id": 986, "seek": 303232, "start": 3032.32, "end": 3034.2400000000002, "text": " all of these outputs of", "tokens": [50365, 439, 295, 613, 23930, 295, 50461], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 987, "seek": 303232, "start": 3034.32, "end": 3036.2400000000002, "text": " at all the positions of", "tokens": [50465, 412, 439, 264, 8432, 295, 50561], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 988, "seek": 303232, "start": 3036.32, "end": 3038.2400000000002, "text": " deandre at the same time.", "tokens": [50565, 368, 474, 265, 412, 264, 912, 565, 13, 50661], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 989, "seek": 303232, "start": 3038.32, "end": 3040.2400000000002, "text": " And the reason that", "tokens": [50665, 400, 264, 1778, 300, 50761], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 990, "seek": 303232, "start": 3040.32, "end": 3042.2400000000002, "text": " this is much more efficient is because", "tokens": [50765, 341, 307, 709, 544, 7148, 307, 570, 50861], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 991, "seek": 303232, "start": 3042.32, "end": 3044.2400000000002, "text": " number one, as I mentioned, the for loop", "tokens": [50865, 1230, 472, 11, 382, 286, 2835, 11, 264, 337, 6367, 50961], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 992, "seek": 303232, "start": 3044.32, "end": 3046.2400000000002, "text": " is inside the CUDA kernels in the", "tokens": [50965, 307, 1854, 264, 29777, 7509, 23434, 1625, 294, 264, 51061], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 993, "seek": 303232, "start": 3046.32, "end": 3048.2400000000002, "text": " sliding. So that makes", "tokens": [51065, 21169, 13, 407, 300, 1669, 51161], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 994, "seek": 303232, "start": 3048.32, "end": 3050.2400000000002, "text": " it efficient. But number two, notice", "tokens": [51165, 309, 7148, 13, 583, 1230, 732, 11, 3449, 51261], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 995, "seek": 303232, "start": 3050.32, "end": 3052.2400000000002, "text": " the variable reuse here. For example", "tokens": [51265, 264, 7006, 26225, 510, 13, 1171, 1365, 51361], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 996, "seek": 303232, "start": 3052.32, "end": 3054.2400000000002, "text": " if we look at this circle, this node here,", "tokens": [51365, 498, 321, 574, 412, 341, 6329, 11, 341, 9984, 510, 11, 51461], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 997, "seek": 303232, "start": 3054.32, "end": 3056.2400000000002, "text": " this node here is the right child", "tokens": [51465, 341, 9984, 510, 307, 264, 558, 1440, 51561], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 998, "seek": 303232, "start": 3056.32, "end": 3058.2400000000002, "text": " of this node, but it's also", "tokens": [51565, 295, 341, 9984, 11, 457, 309, 311, 611, 51661], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 999, "seek": 303232, "start": 3058.32, "end": 3060.2400000000002, "text": " the left child of the node here.", "tokens": [51665, 264, 1411, 1440, 295, 264, 9984, 510, 13, 51761], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 1000, "seek": 303232, "start": 3060.32, "end": 3062.2400000000002, "text": " And so basically this", "tokens": [51765, 400, 370, 1936, 341, 51861], "temperature": 0.0, "avg_logprob": -0.10713391373122948, "compression_ratio": 1.7709923664122138, "no_speech_prob": 0.0003017145209014416}, {"id": 1001, "seek": 306224, "start": 3062.24, "end": 3064.16, "text": " node and its value is used", "tokens": [50365, 9984, 293, 1080, 2158, 307, 1143, 50461], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1002, "seek": 306224, "start": 3064.24, "end": 3066.16, "text": " twice. And so", "tokens": [50465, 6091, 13, 400, 370, 50561], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1003, "seek": 306224, "start": 3066.24, "end": 3068.16, "text": " right now, in this naive way,", "tokens": [50565, 558, 586, 11, 294, 341, 29052, 636, 11, 50661], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1004, "seek": 306224, "start": 3068.24, "end": 3070.16, "text": " we'd have to recalculate it.", "tokens": [50665, 321, 1116, 362, 281, 850, 304, 2444, 473, 309, 13, 50761], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1005, "seek": 306224, "start": 3070.24, "end": 3072.16, "text": " But here we are allowed to reuse it.", "tokens": [50765, 583, 510, 321, 366, 4350, 281, 26225, 309, 13, 50861], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1006, "seek": 306224, "start": 3072.24, "end": 3074.16, "text": " So in the convolutional neural network,", "tokens": [50865, 407, 294, 264, 45216, 304, 18161, 3209, 11, 50961], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1007, "seek": 306224, "start": 3074.24, "end": 3076.16, "text": " you think of these linear layers that we have", "tokens": [50965, 291, 519, 295, 613, 8213, 7914, 300, 321, 362, 51061], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1008, "seek": 306224, "start": 3076.24, "end": 3078.16, "text": " up above as filters.", "tokens": [51065, 493, 3673, 382, 15995, 13, 51161], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1009, "seek": 306224, "start": 3078.24, "end": 3080.16, "text": " And we take these filters", "tokens": [51165, 400, 321, 747, 613, 15995, 51261], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1010, "seek": 306224, "start": 3080.24, "end": 3082.16, "text": " and they're linear filters, and you slide them over", "tokens": [51265, 293, 436, 434, 8213, 15995, 11, 293, 291, 4137, 552, 670, 51361], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1011, "seek": 306224, "start": 3082.24, "end": 3084.16, "text": " input sequence, and we calculate", "tokens": [51365, 4846, 8310, 11, 293, 321, 8873, 51461], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1012, "seek": 306224, "start": 3084.24, "end": 3086.16, "text": " the first layer, and then the second layer,", "tokens": [51465, 264, 700, 4583, 11, 293, 550, 264, 1150, 4583, 11, 51561], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1013, "seek": 306224, "start": 3086.24, "end": 3088.16, "text": " and then the third layer, and then the output layer", "tokens": [51565, 293, 550, 264, 2636, 4583, 11, 293, 550, 264, 5598, 4583, 51661], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1014, "seek": 306224, "start": 3088.24, "end": 3090.16, "text": " of the sandwich, and it's all done very", "tokens": [51665, 295, 264, 11141, 11, 293, 309, 311, 439, 1096, 588, 51761], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1015, "seek": 306224, "start": 3090.24, "end": 3092.16, "text": " efficiently using these convolutions.", "tokens": [51765, 19621, 1228, 613, 3754, 15892, 13, 51861], "temperature": 0.0, "avg_logprob": -0.07772653054871015, "compression_ratio": 1.8992805755395683, "no_speech_prob": 0.0004425231891218573}, {"id": 1016, "seek": 309224, "start": 3092.24, "end": 3094.16, "text": " So we're going to cover that in a future video.", "tokens": [50365, 407, 321, 434, 516, 281, 2060, 300, 294, 257, 2027, 960, 13, 50461], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1017, "seek": 309224, "start": 3094.24, "end": 3096.16, "text": " The second thing I hope you took away from this video", "tokens": [50465, 440, 1150, 551, 286, 1454, 291, 1890, 1314, 490, 341, 960, 50561], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1018, "seek": 309224, "start": 3096.24, "end": 3098.16, "text": " is you've seen me basically implement", "tokens": [50565, 307, 291, 600, 1612, 385, 1936, 4445, 50661], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1019, "seek": 309224, "start": 3098.24, "end": 3100.16, "text": " all of these layer", "tokens": [50665, 439, 295, 613, 4583, 50761], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1020, "seek": 309224, "start": 3100.24, "end": 3102.16, "text": " Lego building blocks, or module", "tokens": [50765, 28761, 2390, 8474, 11, 420, 10088, 50861], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1021, "seek": 309224, "start": 3102.24, "end": 3104.16, "text": " building blocks. And I'm", "tokens": [50865, 2390, 8474, 13, 400, 286, 478, 50961], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1022, "seek": 309224, "start": 3104.24, "end": 3106.16, "text": " implementing them over here, and we've implemented", "tokens": [50965, 18114, 552, 670, 510, 11, 293, 321, 600, 12270, 51061], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1023, "seek": 309224, "start": 3106.24, "end": 3108.16, "text": " a number of layers together, and we're also", "tokens": [51065, 257, 1230, 295, 7914, 1214, 11, 293, 321, 434, 611, 51161], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1024, "seek": 309224, "start": 3108.24, "end": 3110.16, "text": " implementing these containers.", "tokens": [51165, 18114, 613, 17089, 13, 51261], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1025, "seek": 309224, "start": 3110.24, "end": 3112.16, "text": " And we've overall", "tokens": [51265, 400, 321, 600, 4787, 51361], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1026, "seek": 309224, "start": 3112.24, "end": 3114.16, "text": " PyTorchified our code quite a bit more.", "tokens": [51365, 9953, 51, 284, 339, 2587, 527, 3089, 1596, 257, 857, 544, 13, 51461], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1027, "seek": 309224, "start": 3114.24, "end": 3116.16, "text": " Now, basically what we're doing", "tokens": [51465, 823, 11, 1936, 437, 321, 434, 884, 51561], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1028, "seek": 309224, "start": 3116.24, "end": 3118.16, "text": " here is we're reimplementing Torch.nn,", "tokens": [51565, 510, 307, 321, 434, 33433, 43704, 278, 7160, 339, 13, 26384, 11, 51661], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1029, "seek": 309224, "start": 3118.24, "end": 3120.16, "text": " which is the neural network's", "tokens": [51665, 597, 307, 264, 18161, 3209, 311, 51761], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1030, "seek": 309224, "start": 3120.24, "end": 3122.16, "text": " library on top of", "tokens": [51765, 6405, 322, 1192, 295, 51861], "temperature": 0.0, "avg_logprob": -0.08520362801747779, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.001606339355930686}, {"id": 1031, "seek": 312216, "start": 3122.24, "end": 3124.08, "text": " Torch.tensor. And it looks very much", "tokens": [50369, 7160, 339, 13, 83, 23153, 13, 400, 309, 1542, 588, 709, 50461], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1032, "seek": 312216, "start": 3124.16, "end": 3126.08, "text": " like this, except it is much better", "tokens": [50465, 411, 341, 11, 3993, 309, 307, 709, 1101, 50561], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1033, "seek": 312216, "start": 3126.16, "end": 3128.08, "text": " because it's in PyTorch", "tokens": [50565, 570, 309, 311, 294, 9953, 51, 284, 339, 50661], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1034, "seek": 312216, "start": 3128.16, "end": 3130.08, "text": " instead of jinkling my Jupyter", "tokens": [50665, 2602, 295, 361, 45258, 452, 22125, 88, 391, 50761], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1035, "seek": 312216, "start": 3130.16, "end": 3132.08, "text": " notebook. So I think going forward", "tokens": [50765, 21060, 13, 407, 286, 519, 516, 2128, 50861], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1036, "seek": 312216, "start": 3132.16, "end": 3134.08, "text": " I will probably have considered us having", "tokens": [50865, 286, 486, 1391, 362, 4888, 505, 1419, 50961], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1037, "seek": 312216, "start": 3134.16, "end": 3136.08, "text": " unlocked Torch.nn.", "tokens": [50965, 30180, 7160, 339, 13, 26384, 13, 51061], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1038, "seek": 312216, "start": 3136.16, "end": 3138.08, "text": " We understand roughly what's in there,", "tokens": [51065, 492, 1223, 9810, 437, 311, 294, 456, 11, 51161], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1039, "seek": 312216, "start": 3138.16, "end": 3140.08, "text": " how these modules work, how they're nested,", "tokens": [51165, 577, 613, 16679, 589, 11, 577, 436, 434, 15646, 292, 11, 51261], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1040, "seek": 312216, "start": 3140.16, "end": 3142.08, "text": " and what they're doing on top of", "tokens": [51265, 293, 437, 436, 434, 884, 322, 1192, 295, 51361], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1041, "seek": 312216, "start": 3142.16, "end": 3144.08, "text": " Torch.tensor. So hopefully we'll just", "tokens": [51365, 7160, 339, 13, 83, 23153, 13, 407, 4696, 321, 603, 445, 51461], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1042, "seek": 312216, "start": 3144.16, "end": 3146.08, "text": " switch over and continue", "tokens": [51465, 3679, 670, 293, 2354, 51561], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1043, "seek": 312216, "start": 3146.16, "end": 3148.08, "text": " and start using Torch.nn directly.", "tokens": [51565, 293, 722, 1228, 7160, 339, 13, 26384, 3838, 13, 51661], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1044, "seek": 312216, "start": 3148.16, "end": 3150.08, "text": " The next thing I hope you got a bit of a sense of", "tokens": [51665, 440, 958, 551, 286, 1454, 291, 658, 257, 857, 295, 257, 2020, 295, 51761], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1045, "seek": 312216, "start": 3150.16, "end": 3152.08, "text": " is what the development process", "tokens": [51765, 307, 437, 264, 3250, 1399, 51861], "temperature": 0.0, "avg_logprob": -0.0655971429286859, "compression_ratio": 1.65814696485623, "no_speech_prob": 0.000770193466451019}, {"id": 1046, "seek": 315208, "start": 3152.08, "end": 3154.0, "text": " of building deep neural networks looks like.", "tokens": [50365, 295, 2390, 2452, 18161, 9590, 1542, 411, 13, 50461], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1047, "seek": 315208, "start": 3154.08, "end": 3156.0, "text": " Which I think was relatively representative", "tokens": [50465, 3013, 286, 519, 390, 7226, 12424, 50561], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1048, "seek": 315208, "start": 3156.08, "end": 3158.0, "text": " to some extent. So number one,", "tokens": [50565, 281, 512, 8396, 13, 407, 1230, 472, 11, 50661], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1049, "seek": 315208, "start": 3158.08, "end": 3160.0, "text": " we are spending a lot of time", "tokens": [50665, 321, 366, 6434, 257, 688, 295, 565, 50761], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1050, "seek": 315208, "start": 3160.08, "end": 3162.0, "text": " in the documentation page of PyTorch.", "tokens": [50765, 294, 264, 14333, 3028, 295, 9953, 51, 284, 339, 13, 50861], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1051, "seek": 315208, "start": 3162.08, "end": 3164.0, "text": " And we're reading through all the layers,", "tokens": [50865, 400, 321, 434, 3760, 807, 439, 264, 7914, 11, 50961], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1052, "seek": 315208, "start": 3164.08, "end": 3166.0, "text": " looking at documentations,", "tokens": [50965, 1237, 412, 4166, 763, 11, 51061], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1053, "seek": 315208, "start": 3166.08, "end": 3168.0, "text": " what are the shapes of the inputs,", "tokens": [51065, 437, 366, 264, 10854, 295, 264, 15743, 11, 51161], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1054, "seek": 315208, "start": 3168.08, "end": 3170.0, "text": " what can they be, what does the layer do,", "tokens": [51165, 437, 393, 436, 312, 11, 437, 775, 264, 4583, 360, 11, 51261], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1055, "seek": 315208, "start": 3170.08, "end": 3172.0, "text": " and so on. Unfortunately,", "tokens": [51265, 293, 370, 322, 13, 8590, 11, 51361], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1056, "seek": 315208, "start": 3172.08, "end": 3174.0, "text": " I have to say the PyTorch documentation", "tokens": [51365, 286, 362, 281, 584, 264, 9953, 51, 284, 339, 14333, 51461], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1057, "seek": 315208, "start": 3174.08, "end": 3176.0, "text": " is not very good.", "tokens": [51465, 307, 406, 588, 665, 13, 51561], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1058, "seek": 315208, "start": 3176.08, "end": 3178.0, "text": " They spend a ton of time on hardcore", "tokens": [51565, 814, 3496, 257, 2952, 295, 565, 322, 28196, 51661], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1059, "seek": 315208, "start": 3178.08, "end": 3180.0, "text": " engineering of all kinds of distributed primitives,", "tokens": [51665, 7043, 295, 439, 3685, 295, 12631, 2886, 38970, 11, 51761], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1060, "seek": 315208, "start": 3180.08, "end": 3182.0, "text": " etc. But as far as I can tell,", "tokens": [51765, 5183, 13, 583, 382, 1400, 382, 286, 393, 980, 11, 51861], "temperature": 0.0, "avg_logprob": -0.08194179283945184, "compression_ratio": 1.7266881028938907, "no_speech_prob": 0.0004751525411847979}, {"id": 1061, "seek": 318200, "start": 3182.0, "end": 3183.92, "text": " no one is maintaining documentation.", "tokens": [50365, 572, 472, 307, 14916, 14333, 13, 50461], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1062, "seek": 318200, "start": 3184.0, "end": 3185.92, "text": " It will lie to you,", "tokens": [50465, 467, 486, 4544, 281, 291, 11, 50561], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1063, "seek": 318200, "start": 3186.0, "end": 3187.92, "text": " it will be wrong, it will be incomplete,", "tokens": [50565, 309, 486, 312, 2085, 11, 309, 486, 312, 31709, 11, 50661], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1064, "seek": 318200, "start": 3188.0, "end": 3189.92, "text": " it will be unclear.", "tokens": [50665, 309, 486, 312, 25636, 13, 50761], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1065, "seek": 318200, "start": 3190.0, "end": 3191.92, "text": " So unfortunately, it is what it is", "tokens": [50765, 407, 7015, 11, 309, 307, 437, 309, 307, 50861], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1066, "seek": 318200, "start": 3192.0, "end": 3193.92, "text": " and you just kind of do your best", "tokens": [50865, 293, 291, 445, 733, 295, 360, 428, 1151, 50961], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1067, "seek": 318200, "start": 3194.0, "end": 3195.92, "text": " with what they've", "tokens": [50965, 365, 437, 436, 600, 51061], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1068, "seek": 318200, "start": 3196.0, "end": 3197.92, "text": " given us.", "tokens": [51065, 2212, 505, 13, 51161], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1069, "seek": 318200, "start": 3198.0, "end": 3199.92, "text": " Number two,", "tokens": [51165, 5118, 732, 11, 51261], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1070, "seek": 318200, "start": 3200.0, "end": 3201.92, "text": " the other thing that I hope you got", "tokens": [51265, 264, 661, 551, 300, 286, 1454, 291, 658, 51361], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1071, "seek": 318200, "start": 3202.0, "end": 3203.92, "text": " a sense of is there's a ton of", "tokens": [51365, 257, 2020, 295, 307, 456, 311, 257, 2952, 295, 51461], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1072, "seek": 318200, "start": 3204.0, "end": 3205.92, "text": " trying to make the shapes work.", "tokens": [51465, 1382, 281, 652, 264, 10854, 589, 13, 51561], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1073, "seek": 318200, "start": 3206.0, "end": 3207.92, "text": " And there's a lot of gymnastics around these multi-dimensional", "tokens": [51565, 400, 456, 311, 257, 688, 295, 48461, 926, 613, 4825, 12, 18759, 51661], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1074, "seek": 318200, "start": 3208.0, "end": 3209.92, "text": " arrays. And are they two-dimensional,", "tokens": [51665, 41011, 13, 400, 366, 436, 732, 12, 18759, 11, 51761], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1075, "seek": 318200, "start": 3210.0, "end": 3211.92, "text": " three-dimensional, four-dimensional?", "tokens": [51765, 1045, 12, 18759, 11, 1451, 12, 18759, 30, 51861], "temperature": 0.0, "avg_logprob": -0.0858060864434726, "compression_ratio": 1.7807692307692307, "no_speech_prob": 0.00046242994721978903}, {"id": 1076, "seek": 321192, "start": 3211.92, "end": 3213.84, "text": " Do the layers take what shapes?", "tokens": [50365, 1144, 264, 7914, 747, 437, 10854, 30, 50461], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1077, "seek": 321192, "start": 3213.92, "end": 3215.84, "text": " Is it NCL or NLC?", "tokens": [50465, 1119, 309, 20786, 43, 420, 426, 14766, 30, 50561], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1078, "seek": 321192, "start": 3215.92, "end": 3217.84, "text": " And you're permuting and viewing,", "tokens": [50565, 400, 291, 434, 4784, 10861, 293, 17480, 11, 50661], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1079, "seek": 321192, "start": 3217.92, "end": 3219.84, "text": " and it just gets pretty messy.", "tokens": [50665, 293, 309, 445, 2170, 1238, 16191, 13, 50761], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1080, "seek": 321192, "start": 3219.92, "end": 3221.84, "text": " And so that brings me to number three.", "tokens": [50765, 400, 370, 300, 5607, 385, 281, 1230, 1045, 13, 50861], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1081, "seek": 321192, "start": 3221.92, "end": 3223.84, "text": " I very often prototype these layers", "tokens": [50865, 286, 588, 2049, 19475, 613, 7914, 50961], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1082, "seek": 321192, "start": 3223.92, "end": 3225.84, "text": " and implementations in Jupyter Notebooks", "tokens": [50965, 293, 4445, 763, 294, 22125, 88, 391, 11633, 15170, 51061], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1083, "seek": 321192, "start": 3225.92, "end": 3227.84, "text": " and make sure that all the shapes work out.", "tokens": [51065, 293, 652, 988, 300, 439, 264, 10854, 589, 484, 13, 51161], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1084, "seek": 321192, "start": 3227.92, "end": 3229.84, "text": " And I'm spending a lot of time basically", "tokens": [51165, 400, 286, 478, 6434, 257, 688, 295, 565, 1936, 51261], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1085, "seek": 321192, "start": 3229.92, "end": 3231.84, "text": " babysitting the shapes and making sure", "tokens": [51265, 39764, 2414, 264, 10854, 293, 1455, 988, 51361], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1086, "seek": 321192, "start": 3231.92, "end": 3233.84, "text": " everything is correct. And then once I'm", "tokens": [51365, 1203, 307, 3006, 13, 400, 550, 1564, 286, 478, 51461], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1087, "seek": 321192, "start": 3233.92, "end": 3235.84, "text": " satisfied with the functionality in a Jupyter Notebook,", "tokens": [51465, 11239, 365, 264, 14980, 294, 257, 22125, 88, 391, 11633, 2939, 11, 51561], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1088, "seek": 321192, "start": 3235.92, "end": 3237.84, "text": " I will take that code and copy-paste it into", "tokens": [51565, 286, 486, 747, 300, 3089, 293, 5055, 12, 79, 9079, 309, 666, 51661], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1089, "seek": 321192, "start": 3237.92, "end": 3239.84, "text": " my repository of actual code", "tokens": [51665, 452, 25841, 295, 3539, 3089, 51761], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1090, "seek": 321192, "start": 3239.92, "end": 3241.84, "text": " that I'm training with. And so", "tokens": [51765, 300, 286, 478, 3097, 365, 13, 400, 370, 51861], "temperature": 0.0, "avg_logprob": -0.0889888896217829, "compression_ratio": 1.7375, "no_speech_prob": 0.0006952821859158576}, {"id": 1091, "seek": 324184, "start": 3241.84, "end": 3243.76, "text": " then I'm working with VS Code on the side.", "tokens": [50365, 550, 286, 478, 1364, 365, 25091, 15549, 322, 264, 1252, 13, 50461], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1092, "seek": 324184, "start": 3243.84, "end": 3245.76, "text": " So I usually have Jupyter Notebook and VS Code.", "tokens": [50465, 407, 286, 2673, 362, 22125, 88, 391, 11633, 2939, 293, 25091, 15549, 13, 50561], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1093, "seek": 324184, "start": 3245.84, "end": 3247.76, "text": " I develop in Jupyter Notebook, I paste", "tokens": [50565, 286, 1499, 294, 22125, 88, 391, 11633, 2939, 11, 286, 9163, 50661], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1094, "seek": 324184, "start": 3247.84, "end": 3249.76, "text": " into VS Code, and then I kick off experiments", "tokens": [50665, 666, 25091, 15549, 11, 293, 550, 286, 4437, 766, 12050, 50761], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1095, "seek": 324184, "start": 3249.84, "end": 3251.76, "text": " from the repo, of course,", "tokens": [50765, 490, 264, 49040, 11, 295, 1164, 11, 50861], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1096, "seek": 324184, "start": 3251.84, "end": 3253.76, "text": " from the code repository.", "tokens": [50865, 490, 264, 3089, 25841, 13, 50961], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1097, "seek": 324184, "start": 3253.84, "end": 3255.76, "text": " So that's roughly some notes on the", "tokens": [50965, 407, 300, 311, 9810, 512, 5570, 322, 264, 51061], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1098, "seek": 324184, "start": 3255.84, "end": 3257.76, "text": " development process of working with neural nets.", "tokens": [51065, 3250, 1399, 295, 1364, 365, 18161, 36170, 13, 51161], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1099, "seek": 324184, "start": 3257.84, "end": 3259.76, "text": " Lastly, I think this lecture unlocks a lot", "tokens": [51165, 18072, 11, 286, 519, 341, 7991, 517, 34896, 257, 688, 51261], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1100, "seek": 324184, "start": 3259.84, "end": 3261.76, "text": " of potential further lectures", "tokens": [51265, 295, 3995, 3052, 16564, 51361], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1101, "seek": 324184, "start": 3261.84, "end": 3263.76, "text": " because, number one, we have to convert our", "tokens": [51365, 570, 11, 1230, 472, 11, 321, 362, 281, 7620, 527, 51461], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1102, "seek": 324184, "start": 3263.84, "end": 3265.76, "text": " neural network to actually use these dilated", "tokens": [51465, 18161, 3209, 281, 767, 764, 613, 11504, 770, 51561], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1103, "seek": 324184, "start": 3265.84, "end": 3267.76, "text": " causal convolutional layers,", "tokens": [51565, 38755, 45216, 304, 7914, 11, 51661], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1104, "seek": 324184, "start": 3267.84, "end": 3269.76, "text": " so implementing the comnet.", "tokens": [51665, 370, 18114, 264, 395, 7129, 13, 51761], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1105, "seek": 324184, "start": 3269.84, "end": 3271.76, "text": " Number two, I potentially start", "tokens": [51765, 5118, 732, 11, 286, 7263, 722, 51861], "temperature": 0.0, "avg_logprob": -0.09115896596537008, "compression_ratio": 1.7648902821316614, "no_speech_prob": 0.00036068554618395865}, {"id": 1106, "seek": 327176, "start": 3271.76, "end": 3273.6800000000003, "text": " to get into what this means,", "tokens": [50365, 281, 483, 666, 437, 341, 1355, 11, 50461], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1107, "seek": 327176, "start": 3273.76, "end": 3275.6800000000003, "text": " where are residual connections and", "tokens": [50465, 689, 366, 27980, 9271, 293, 50561], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1108, "seek": 327176, "start": 3275.76, "end": 3277.6800000000003, "text": " skip connections and why are they useful.", "tokens": [50565, 10023, 9271, 293, 983, 366, 436, 4420, 13, 50661], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1109, "seek": 327176, "start": 3277.76, "end": 3279.6800000000003, "text": " Number three,", "tokens": [50665, 5118, 1045, 11, 50761], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1110, "seek": 327176, "start": 3279.76, "end": 3281.6800000000003, "text": " as I mentioned, we don't have any experimental harness.", "tokens": [50765, 382, 286, 2835, 11, 321, 500, 380, 362, 604, 17069, 19700, 13, 50861], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1111, "seek": 327176, "start": 3281.76, "end": 3283.6800000000003, "text": " So right now I'm just guessing, checking", "tokens": [50865, 407, 558, 586, 286, 478, 445, 17939, 11, 8568, 50961], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1112, "seek": 327176, "start": 3283.76, "end": 3285.6800000000003, "text": " everything. This is not representative of", "tokens": [50965, 1203, 13, 639, 307, 406, 12424, 295, 51061], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1113, "seek": 327176, "start": 3285.76, "end": 3287.6800000000003, "text": " typical deep learning workflows. You have to", "tokens": [51065, 7476, 2452, 2539, 43461, 13, 509, 362, 281, 51161], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1114, "seek": 327176, "start": 3287.76, "end": 3289.6800000000003, "text": " set up your evaluation harness.", "tokens": [51165, 992, 493, 428, 13344, 19700, 13, 51261], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1115, "seek": 327176, "start": 3289.76, "end": 3291.6800000000003, "text": " You can kick off experiments. You have lots of arguments", "tokens": [51265, 509, 393, 4437, 766, 12050, 13, 509, 362, 3195, 295, 12869, 51361], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1116, "seek": 327176, "start": 3291.76, "end": 3293.6800000000003, "text": " that your script can take.", "tokens": [51365, 300, 428, 5755, 393, 747, 13, 51461], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1117, "seek": 327176, "start": 3293.76, "end": 3295.6800000000003, "text": " You're kicking off a lot of experimentation.", "tokens": [51465, 509, 434, 19137, 766, 257, 688, 295, 37142, 13, 51561], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1118, "seek": 327176, "start": 3295.76, "end": 3297.6800000000003, "text": " You're looking at a lot of plots of training and validation", "tokens": [51565, 509, 434, 1237, 412, 257, 688, 295, 28609, 295, 3097, 293, 24071, 51661], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1119, "seek": 327176, "start": 3297.76, "end": 3299.6800000000003, "text": " losses, and you're looking at what is working", "tokens": [51665, 15352, 11, 293, 291, 434, 1237, 412, 437, 307, 1364, 51761], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1120, "seek": 327176, "start": 3299.76, "end": 3301.6800000000003, "text": " and what is not working. And you're working on this", "tokens": [51765, 293, 437, 307, 406, 1364, 13, 400, 291, 434, 1364, 322, 341, 51861], "temperature": 0.0, "avg_logprob": -0.09044529374238032, "compression_ratio": 1.8791540785498488, "no_speech_prob": 0.002004887443035841}, {"id": 1121, "seek": 330168, "start": 3301.68, "end": 3303.6, "text": " like population level, and you're doing", "tokens": [50365, 411, 4415, 1496, 11, 293, 291, 434, 884, 50461], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1122, "seek": 330168, "start": 3303.68, "end": 3305.6, "text": " all these hyperparameter searches.", "tokens": [50465, 439, 613, 9848, 2181, 335, 2398, 26701, 13, 50561], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1123, "seek": 330168, "start": 3305.68, "end": 3307.6, "text": " And so we've done none of that so far.", "tokens": [50565, 400, 370, 321, 600, 1096, 6022, 295, 300, 370, 1400, 13, 50661], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1124, "seek": 330168, "start": 3307.68, "end": 3309.6, "text": " So how to set that up", "tokens": [50665, 407, 577, 281, 992, 300, 493, 50761], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1125, "seek": 330168, "start": 3309.68, "end": 3311.6, "text": " and how to make it good, I think", "tokens": [50765, 293, 577, 281, 652, 309, 665, 11, 286, 519, 50861], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1126, "seek": 330168, "start": 3311.68, "end": 3313.6, "text": " is a whole another topic.", "tokens": [50865, 307, 257, 1379, 1071, 4829, 13, 50961], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1127, "seek": 330168, "start": 3313.68, "end": 3315.6, "text": " And number three, we should probably cover", "tokens": [50965, 400, 1230, 1045, 11, 321, 820, 1391, 2060, 51061], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1128, "seek": 330168, "start": 3315.68, "end": 3317.6, "text": " recurring neural networks. RNNs, LSTMs,", "tokens": [51065, 32279, 18161, 9590, 13, 45702, 45, 82, 11, 441, 6840, 26386, 11, 51161], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1129, "seek": 330168, "start": 3317.68, "end": 3319.6, "text": " Grooves, and of course Transformers.", "tokens": [51165, 12981, 78, 977, 11, 293, 295, 1164, 27938, 433, 13, 51261], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1130, "seek": 330168, "start": 3319.68, "end": 3321.6, "text": " So many", "tokens": [51265, 407, 867, 51361], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1131, "seek": 330168, "start": 3321.68, "end": 3323.6, "text": " places to go,", "tokens": [51365, 3190, 281, 352, 11, 51461], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1132, "seek": 330168, "start": 3323.68, "end": 3325.6, "text": " and we'll cover that in the future.", "tokens": [51465, 293, 321, 603, 2060, 300, 294, 264, 2027, 13, 51561], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1133, "seek": 330168, "start": 3325.68, "end": 3327.6, "text": " For now, bye. Sorry, I forgot to say that", "tokens": [51565, 1171, 586, 11, 6543, 13, 4919, 11, 286, 5298, 281, 584, 300, 51661], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1134, "seek": 330168, "start": 3327.68, "end": 3329.6, "text": " if you are interested, I think", "tokens": [51665, 498, 291, 366, 3102, 11, 286, 519, 51761], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1135, "seek": 330168, "start": 3329.68, "end": 3331.6, "text": " it is kind of interesting to try to beat this number", "tokens": [51765, 309, 307, 733, 295, 1880, 281, 853, 281, 4224, 341, 1230, 51861], "temperature": 0.0, "avg_logprob": -0.10905084917622228, "compression_ratio": 1.6435643564356435, "no_speech_prob": 0.0008944153669290245}, {"id": 1136, "seek": 333160, "start": 3331.6, "end": 3333.52, "text": " 1.993, because", "tokens": [50365, 502, 13, 8494, 18, 11, 570, 50461], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1137, "seek": 333160, "start": 3333.6, "end": 3335.52, "text": " I really haven't tried a lot of experimentation", "tokens": [50465, 286, 534, 2378, 380, 3031, 257, 688, 295, 37142, 50561], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1138, "seek": 333160, "start": 3335.6, "end": 3337.52, "text": " here, and there's quite a bit of longing for it potentially,", "tokens": [50565, 510, 11, 293, 456, 311, 1596, 257, 857, 295, 35050, 337, 309, 7263, 11, 50661], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1139, "seek": 333160, "start": 3337.6, "end": 3339.52, "text": " to still push this further.", "tokens": [50665, 281, 920, 2944, 341, 3052, 13, 50761], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1140, "seek": 333160, "start": 3339.6, "end": 3341.52, "text": " So I haven't tried any other", "tokens": [50765, 407, 286, 2378, 380, 3031, 604, 661, 50861], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1141, "seek": 333160, "start": 3341.6, "end": 3343.52, "text": " ways of allocating these channels in this", "tokens": [50865, 2098, 295, 12660, 990, 613, 9235, 294, 341, 50961], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1142, "seek": 333160, "start": 3343.6, "end": 3345.52, "text": " neural net. Maybe the number of", "tokens": [50965, 18161, 2533, 13, 2704, 264, 1230, 295, 51061], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1143, "seek": 333160, "start": 3345.6, "end": 3347.52, "text": " dimensions for the embedding is all", "tokens": [51065, 12819, 337, 264, 12240, 3584, 307, 439, 51161], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1144, "seek": 333160, "start": 3347.6, "end": 3349.52, "text": " wrong. Maybe it's possible to actually", "tokens": [51165, 2085, 13, 2704, 309, 311, 1944, 281, 767, 51261], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1145, "seek": 333160, "start": 3349.6, "end": 3351.52, "text": " take the original network with just one hidden layer", "tokens": [51265, 747, 264, 3380, 3209, 365, 445, 472, 7633, 4583, 51361], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1146, "seek": 333160, "start": 3351.6, "end": 3353.52, "text": " and make it big enough and actually", "tokens": [51365, 293, 652, 309, 955, 1547, 293, 767, 51461], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1147, "seek": 333160, "start": 3353.6, "end": 3355.52, "text": " beat my fancy hierarchical", "tokens": [51465, 4224, 452, 10247, 35250, 804, 51561], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1148, "seek": 333160, "start": 3355.6, "end": 3357.52, "text": " network. It's not obvious.", "tokens": [51565, 3209, 13, 467, 311, 406, 6322, 13, 51661], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1149, "seek": 333160, "start": 3357.6, "end": 3359.52, "text": " That would be kind of embarrassing if this", "tokens": [51665, 663, 576, 312, 733, 295, 17299, 498, 341, 51761], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1150, "seek": 333160, "start": 3359.6, "end": 3361.52, "text": " did not do better, even once you torture", "tokens": [51765, 630, 406, 360, 1101, 11, 754, 1564, 291, 20711, 51861], "temperature": 0.0, "avg_logprob": -0.08633013859691235, "compression_ratio": 1.6646706586826348, "no_speech_prob": 0.0007364965276792645}, {"id": 1151, "seek": 336152, "start": 3361.52, "end": 3363.44, "text": " it a little bit. Maybe you can read the", "tokens": [50365, 309, 257, 707, 857, 13, 2704, 291, 393, 1401, 264, 50461], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1152, "seek": 336152, "start": 3363.52, "end": 3365.44, "text": " WaveNet paper and try to figure out how some of these", "tokens": [50465, 28530, 31890, 3035, 293, 853, 281, 2573, 484, 577, 512, 295, 613, 50561], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1153, "seek": 336152, "start": 3365.52, "end": 3367.44, "text": " layers work and implement them yourselves using", "tokens": [50565, 7914, 589, 293, 4445, 552, 14791, 1228, 50661], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1154, "seek": 336152, "start": 3367.52, "end": 3369.44, "text": " what we have. And of course", "tokens": [50665, 437, 321, 362, 13, 400, 295, 1164, 50761], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1155, "seek": 336152, "start": 3369.52, "end": 3371.44, "text": " you can always tune some of the initialization", "tokens": [50765, 291, 393, 1009, 10864, 512, 295, 264, 5883, 2144, 50861], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1156, "seek": 336152, "start": 3371.52, "end": 3373.44, "text": " or some of the optimization", "tokens": [50865, 420, 512, 295, 264, 19618, 50961], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1157, "seek": 336152, "start": 3373.52, "end": 3375.44, "text": " and see if you can improve it that way.", "tokens": [50965, 293, 536, 498, 291, 393, 3470, 309, 300, 636, 13, 51061], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1158, "seek": 336152, "start": 3375.52, "end": 3377.44, "text": " So I'd be curious if people can come up with some", "tokens": [51065, 407, 286, 1116, 312, 6369, 498, 561, 393, 808, 493, 365, 512, 51161], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1159, "seek": 336152, "start": 3377.52, "end": 3379.44, "text": " ways to beat this.", "tokens": [51165, 2098, 281, 4224, 341, 13, 51261], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}, {"id": 1160, "seek": 336152, "start": 3379.52, "end": 3381.44, "text": " And yeah, that's it for now. Bye.", "tokens": [51265, 400, 1338, 11, 300, 311, 309, 337, 586, 13, 4621, 13, 51361], "temperature": 0.0, "avg_logprob": -0.04232500233781447, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0005530297639779747}], "language": "en"}